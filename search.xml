<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[前端知识体系]]></title>
    <url>%2F2021%2F08%2F09%2Fui-%E5%89%8D%E7%AB%AF%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[前端三要素 HTML（结构）：超文本标记语言（Hyper Text Markup Language），决定网页的结构和内容 CSS（表现）：层叠样式表（Cascading Style Sheets），设定网页的表现样式 JavaScript（行为）：是一种弱类型脚本语言，其源代码不需经过编译，而是由浏览器解释运行，用于控制网页的行为 结构层（HTML）略 表现层（CSS）CSS 层叠样式表是一门标记语言，并不是编程语言，因此不可以自定义变量，不可以引用等，换句话说就是不具备任何语法支持，它主要缺陷如下： 语法不够强大，比如无法嵌套书写，导致模块化开发中需要书写很多重复的选择器； 没有变量和合理的样式复用机制，使得逻辑上相关的属性值必须以字面量的形式重复输出，导致难以维护； 这就导致了我们在工作中无端增加了许多工作量。为了解决这个问题，前端开发人员会使用一种称之为 【CSS 预处理器】 的工具，提供 CSS 缺失的样式层复用机制、减少冗余代码，提高样式代码的可维护性。大大提高了前端在样式上的开发效率。 什么是 CSS 预处理器CSS 预处理器定义了一种新的语言，其基本思想是，用一种专门的编程语言，为 CSS 增加了一些编程的特性，将 CSS 作为目标生成文件，然后开发者就只要使用这种语言进行 CSS 的编码工作。转化成通俗易懂的话来说就是“用一种专门的编程语言，进行 Web 页面样式设计，再通过编译器转化为正常的 CSS 文件，以供项目使用”。 常用的 CSS 预处理器有哪些 SASS：基于 Ruby，通过服务端处理，功能强大。解析效率高。需要学习 Ruby 语言，上手难度高于 LESS。 LESS：基于 NodeJS，通过客户端处理，使用简单。功能比 SASS 简单，解析效率也低于 SASS，但在实际开发中足够了，所以我们后台人员如果需要的话，建议使用 LESS。 行为层（JavaScript）JavaScript 一门弱类型脚本语言，其源代码在发往客户端运行之前不需经过编译，而是将文本格式的字符代码发送给浏览器由浏览器解释运行。 Native 原生 JS 开发原生 JS 开发，也就是让我们按照 【ECMAScript】 标准的开发方式，简称是 ES，特点是所有浏览器都支持。截止到当前博客发布时间（2018 年 12 月 04 日），ES 标准已发布如下版本： ES3 ES4（内部，未正式发布） ES5（全浏览器支持） ES6（常用，当前主流版本） ES7 ES8 ES9（草案阶段） 区别就是逐步增加新特性。 TypeScript 微软的标准TypeScript 是一种由微软开发的自由和开源的编程语言。它是 JavaScript 的一个超集，而且本质上向这个语言添加了可选的静态类型和基于类的面向对象编程。由安德斯·海尔斯伯格（C#、Delphi、TypeScript 之父；.NET 创立者）主导。 该语言的特点就是除了具备 ES 的特性之外还纳入了许多不在标准范围内的新特性，所以会导致很多浏览器不能直接支持 TypeScript 语法，需要编译后（编译成 JS）才能被浏览器正确执行。 JavaScript 框架 jQuery：大家熟知的 JavaScript 框架，优点是简化了 DOM 操作，缺点是 DOM 操作太频繁，影响前端性能；在前端眼里使用它仅仅是为了兼容 IE6、7、8； Angular：Google 收购的前端框架，由一群 Java 程序员开发，其特点是将后台的 MVC 模式搬到了前端并增加了模块化开发的理念，与微软合作，采用 TypeScript 语法开发；对后台程序员友好，对前端程序员不太友好；最大的缺点是版本迭代不合理（如：1代 -&gt; 2代，除了名字，基本就是两个东西；截止发表博客时已推出了 Angular6） React：Facebook 出品，一款高性能的 JS 前端框架；特点是提出了新概念 【虚拟 DOM】 用于减少真实 DOM 操作，在内存中模拟 DOM 操作，有效的提升了前端渲染效率；缺点是使用复杂，因为需要额外学习一门 【JSX】 语言； Vue：一款渐进式 JavaScript 框架，所谓渐进式就是逐步实现新特性的意思，如实现模块化开发、路由、状态管理等新特性。其特点是综合了 Angular（模块化） 和 React（虚拟 DOM） 的优点； Axios：前端通信框架；因为 Vue 的边界很明确，就是为了处理 DOM，所以并不具备通信能力，此时就需要额外使用一个通信框架与服务器交互；当然也可以直接选择使用 jQuery 提供的 AJAX 通信功能； UI 框架 Ant-Design：阿里巴巴出品，基于 React 的 UI 框架 ElementUI：饿了么出品，基于 Vue 的 UI 框架 Bootstrap：Twitter 推出的一个用于前端开发的开源工具包 AmazeUI：又叫“妹子 UI”，一款 HTML5 跨屏前端框架 JavaScript 构建工具 Babel：JS 编译工具，主要用于浏览器不支持的 ES 新特性，比如用于编译 TypeScript WebPack：模块打包器，主要作用是打包、压缩、合并及按序加载 注：以上知识点已将 WebApp 开发所需技能全部梳理完毕 三端统一混合开发（Hybrid App）主要目的是实现一套代码三端统一（PC、Android、iOS）并能够调用到设备底层硬件（如：传感器、GPS、摄像头等），打包方式主要有以下两种： 云打包：HBuild -&gt; HBuildX，DCloud 出品；API Cloud 本地打包： Cordova（前身是 PhoneGap） 微信小程序详见微信官网，这里就是介绍一个方便微信小程序 UI 开发的框架：WeUI 后端技术前端人员为了方便开发也需要掌握一定的后端技术，但我们 Java 后台人员知道后台知识体系极其庞大复杂，所以为了方便前端人员开发后台应用，就出现了 NodeJS 这样的技术。 NodeJS 的作者已经声称放弃 NodeJS（说是架构做的不好再加上笨重的 node_modules，可能让作者不爽了吧），开始开发全新架构的 Deno 既然是后台技术，那肯定也需要框架和项目管理工具，NodeJS 框架及项目管理工具如下： Express：NodeJS 框架 Koa：Express 简化版 NPM：项目综合管理工具，类似于 Maven YARN：NPM 的替代方案，类似于 Maven 和 Gradle 的关系 附：当前主流前端框架Vue.jsiViewiview 是一个强大的基于 Vue 的 UI 库，有很多实用的基础组件比 elementui 的组件更丰富，主要服务于 PC 界面的中后台产品。使用单文件的 Vue 组件化开发模式 基于 npm + webpack + babel 开发，支持 ES2015 高质量、功能丰富 友好的 API ，自由灵活地使用空间。 官网地址 Github iview-admin 备注：属于前端主流框架，选型时可考虑使用，主要特点是移动端支持较多 ElementUIElement 是饿了么前端开源维护的 Vue UI 组件库，组件齐全，基本涵盖后台所需的所有组件，文档讲解详细，例子也很丰富。主要用于开发 PC 端的页面，是一个质量比较高的 Vue UI 组件库。 官网地址 Github vue-element-admin 备注：属于前端主流框架，选型时可考虑使用，主要特点是桌面端支持较多 ICE飞冰是阿里巴巴团队基于 React/Angular/Vue 的中后台应用解决方案，在阿里巴巴内部，已经有 270 多个来自几乎所有 BU 的项目在使用。飞冰包含了一条从设计端到开发端的完整链路，帮助用户快速搭建属于自己的中后台应用。 官网地址 Github 备注：主要组件还是以 React 为主，截止 2019 年 02 月 17 日更新博客前对 Vue 的支持还不太完善，目前尚处于观望阶段 VantUIVant UI 是有赞前端团队基于有赞统一的规范实现的 Vue 组件库，提供了一整套 UI 基础组件和业务组件。通过 Vant，可以快速搭建出风格统一的页面，提升开发效率。 官网地址 Github AtUIat-ui 是一款基于 Vue 2.x 的前端 UI 组件库，主要用于快速开发 PC 网站产品。 它提供了一套 npm + webpack + babel 前端开发工作流程，CSS 样式独立，即使采用不同的框架实现都能保持统一的 UI 风格。 官网地址 Github CubeUIcube-ui 是滴滴团队开发的基于 Vue.js 实现的精致移动端组件库。支持按需引入和后编译，轻量灵活；扩展性强，可以方便地基于现有组件实现二次开发。 官网地址 Github 混合开发FlutterFlutter 是谷歌的移动端 UI 框架，可在极短的时间内构建 Android 和 iOS 上高质量的原生级应用。Flutter 可与现有代码一起工作, 它被世界各地的开发者和组织使用, 并且 Flutter 是免费和开源的。 官网地址 Github 备注：Google 出品，主要特点是快速构建原生 APP 应用程序，如做混合应用该框架为必选框架 IonicIonic 既是一个 CSS 框架也是一个 Javascript UI 库，Ionic 是目前最有潜力的一款 HTML5 手机应用开发框架。通过 SASS 构建应用程序，它提供了很多 UI 组件来帮助开发者开发强大的应用。它使用 JavaScript MVVM 框架和 AngularJS/Vue 来增强应用。提供数据的双向绑定，使用它成为 Web 和移动开发者的共同选择。 官网地址 官网文档 Github 微信小程序mpvuempvue 是美团开发的一个使用 Vue.js 开发小程序的前端框架，目前支持 微信小程序、百度智能小程序，头条小程序 和 支付宝小程序。 框架基于 Vue.js，修改了的运行时框架 runtime 和代码编译器 compiler 实现，使其可运行在小程序环境中，从而为小程序开发引入了 Vue.js 开发体验。 官网地址 Github 备注：完备的 Vue 开发体验，并且支持多平台的小程序开发，推荐使用 WeUIWeUI 是一套同微信原生视觉体验一致的基础样式库，由微信官方设计团队为微信内网页和微信小程序量身设计，令用户的使用感知更加统一。包含 button、cell、dialog、toast、article、icon 等各式元素。 官网地址 Github]]></content>
  </entry>
  <entry>
    <title><![CDATA[开发必备工具]]></title>
    <url>%2F2021%2F08%2F09%2Fmgr-%E5%85%A8%E6%A0%88%E5%BC%80%E5%8F%91%E7%A5%9E%E5%85%B5%E5%88%A9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[工欲善其事，必先利其器。 一、团队协作团队协作 Teambition：团队协作工具创导者 有道云协作：企业知识管理与协作平台 tower：深受用户喜爱的团队协作工具 远程 TeamViewer：安全远程访问和支持TeamViewer 基于最广泛的平台和技术，连接全世界的人、地区和事物。 向日葵：简单好用的远程控制软件 mstsc：运行 win+r，输入 mstsc。不要忽略 windows 自带的强大远程桌面连接工具 注意：真的不要再用 QQ 远程了，真的很卡！ 笔记备忘 印象笔记：工作必备效率应用 有道云笔记：网易出品，获得 5000 万用户青睐的笔记软件。提供了 PC 端、移动端、网页端等多端应用，用户可以随时随地对线上资料进行编辑、分享以及协同。 日事清：怕工作进度延误 就用日事清 滴答清单：一个帮你高效完成任务和规划时间的应用 二、图形与设计思维导图与原型设计 XMind：思维导图，框架图等等，非常推荐。收费软件，部分功能可用 MindManager：让思考、计划和沟通变得更容易 百度脑图：在线免费脑图，推荐 Mockplus：更快、更简单的原型设计 Axure RP：是一款专业的快速原型设计工具 绘图工具 Visio：微软绘图工具，以直观的方式工作，轻松绘制图表 亿图：国产综合图形图表设计软件。类似 visio 的绘图工具（完美破解版） ProcessOn：支持流程图、思维导图、原型图、UML、网络拓扑图、组织结构图等 draw.io：free online diagram software for making flowcharts, process diagrams, org charts, UML, ER and network diagrams StarUML：A sophisticated software modeler for agile and concise modeling（总之 UML 绘图神器） 平面与视频设计只会写代码，设计都不会？本人从事过多年平面设计和视频相关的工作，这里也给大家推荐一些平时做设计的时的一些软件。 Adobe Photoshop：图像编辑和合成，这个就不用我介绍了吧 Adobe Premiere Pro：视频制作和编辑（业余爱好者可使用绘声绘影） After Effects：电影视觉效果和动态图形 After Illustrator：矢量图形和插图 Corel DRAW：和 AI 齐名的矢量图制作工具 三、版本控制SVN Subversion (SVN) 是一个开源的版本控制系統, 也就是说 Subversion 管理着随时间改变的数据。 这些数据放置在一个中央资料档案库 (repository) 中。 这个档案库很像一个普通的文件服务器, 不过它会记住每一次文件的变动。 这样你就可以把档案恢复到旧的版本, 或是浏览文件的变动历史 工具下载：tortoiseSVN 学习资源 文档：菜鸟教程 SVN教程 视频：版本管理工具介绍—SVN篇 Git Git 是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目 工具下载： SourceTree（推荐★★★） tortoiseGit GitHub Desktop 学习资源 文档：菜鸟教程 Git教程 视频：版本管理工具介绍—Git篇 Git 工作流 集中式工作流，功能分支工作流， GitFlow 工作流，Forking 工作流，Pull Requests Git 托管平台 Github：全球最大的程序员社交网站（同性交友网站） 码云：国内比较大的 Git 托管平台。码云专为开发者提供稳定、高效、安全的云端软件开发协作平台。无论是个人、团队、或是企业，都能够用码云实现代码托管、项目管理、协作开发 CODING：国内 Git 托管平台，Coding，让开发更简单 自主搭建代码托管平台 GitLab：可以使用 GitLab 官方的服务，也提供了开源社区版供团队搭建使用。（推荐使用 Docker 可实现一键自动化搭建） Gogs：一款极易搭建的自助 Git 服务，通过 Go 语言写的，适合在 Linux 服务器上搭建 VisualSVN：VisualSVN Server allows you to easily install and manage a fully-functional Subversion server on the Windows platform. iF.SVNAdmin：The iF.SVNAdmin application is a web based GUI to your Subversion authorization file. It is based on PHP 5.3 and requires a web server (Apache) to be installed. （通过 PHP 在 Linux 上搭建 SVN 平台，并且有 web 管理页面） 四、全栈开发数据库管理（以Mysql为例） Navicat Premium：可以连接所有数据库，配套Navicat也针对不同的数据库有不同的版本，请点击进入官网自行查看，收费软件，需要百度自行破解。 SQLyog：Administrate MySQL Databases With Ease Using a Graphical Interface，免费 SSH 连接工具 MobaXterm（超级推荐，太极客了！而且是免费的） Xshell 5 SecureCRT 6.6 GitKraken 推荐：以上三款工具我都使用过，目前已经弃用 Xshell 和 SecureCRT，推荐使用 MobaXterm SHELL Oh My Zsh - a delightful &amp; open source framework for Z-Shell 接口调试工具 抓包工具1 | Fiddler：The free web debugging proxy（很优秀的抓包工具，目前似乎只支持 windows 用户） 抓包工具2 | charles：Charles is an HTTP proxy / HTTP monitor / Reverse Proxy that enables a developer to view all of the HTTP and SSL / HTTPS traffic between their machine and the Internet.（MacOS的必备抓包工具） 接口调试 | postman：Developers use Postman to buildmodern software for the API-first world. 轻量级开发工具 Sublime Text：A sophisticated text editor for code, markup and prose VS Code：Free. Open source. Runs everywhere.（非常推荐，后起之秀，有丰富的社区插件，超级推荐使用，推荐安装 One Dark Pro Theme） Atom：A hackable text editor for the 21st Century brackets：A modern, open source text editor that understands web design.（前端神奇） 三者比较请移步知乎：Atom、Sublime Text、VSCode 三者比较，各有哪些优势和弱势？ 容器化技术 Docker：秒级启动虚拟机容器技术。真正一次编写，到处运行。（一定要学！） 五、文档技术在团队协作中必须会涉及到文档交互部分，这里推荐以下几个文档平台和开源项目 文档平台 看云：专注于文档在线创作、协作和托管（极力推荐，每个文档只有50Mb的免费空间，超过需要收费） 自动文档生成工具 ApiDoc：Inline Documentation for RESTful web APIs，可以通过命令行将代码中的注释生成在线可调试的文档，开发者的福音啊 Swagger：The Best APIs are Built with Swagger Tools，在 Java web 项目中用的比较多 开源框架 ShowDoc：一个非常适合IT团队的在线 API 文档、技术文档工具。使用 PHP 开发的文档框架 MinDoc：MinDoc 是一款针对IT团队开发的简单好用的文档管理系统 vuepress：vue 官方团队的文档解决方案，适合于静态博客或是文档 docsify：类似 gitbook 和 vuepress 的文档解决方案 hexo：markdown 编写，自动生成静态博客]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello hexo]]></title>
    <url>%2F2021%2F08%2F09%2Fhello-hexo%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. 环境搭建 下载node，也可以采用绿色版本的C:\app\node-v14.15.1-win-x64 node版本过高会导致hexo命令错误，将node版本换为v12.14.0版本后，部署就成功了！ 123456789101112131415161718node -vnpm -v# 配置node镜像npm config set registry https://registry.npm.taobao.orgnpm config set disturl https://npm.taobao.org/distnpm config set electron_mirror https://npm.taobao.org/mirrors/electron/npm config set sass_binary_site https://npm.taobao.org/mirrors/node-sass/npm config set phantomjs_cdnurl https://npm.taobao.org/mirrors/phantomjs/# 进入hexo-home仓库，根据package.json安装依赖模块npm i# 下面的也必须安装，否则无法全局识别hexo命令npm install -g hexo-cli # 安装在C:\app\node-v14.15.1-win-x64\node_modules或者npm install hexohexo -v 下载git Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server123$ hexo server$ hexo s$ hexo s -p 8000 -o More info: Server Generate static files123$ hexo generate$ hexo g$ hexo deploy -g More info: Generating Deploy to remote sites1$ hexo deploy 参数 描述 默认值 layout 布局 title 标题 date 建立日期 文件建立日期 updated 更新日期 文件更新日期 comments 开启文章的评论功能 true tags 标签（不适用于分页） categories 分类（不适用于分页） permalink 覆盖文章网址]]></content>
  </entry>
  <entry>
    <title><![CDATA[居家隔离]]></title>
    <url>%2F2021%2F08%2F09%2Fdiary-2021-08-09-%E5%B1%85%E5%AE%B6%E9%9A%94%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[居家隔离不断成长]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>hello</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker常用命令]]></title>
    <url>%2F2020%2F09%2F17%2Fos-linux-docker-base%2F</url>
    <content type="text"><![CDATA[Author: gzxu@vip.qq.com Date: July 25 2020 1. Install and Config Docker1.1 Before Install12345678910111213141516uname -r # Docker require CentOS core above 3.10# with ubuntu：sudo apt# install dependies package:yum-util offer yum-config-manager，another two is dependies of devicemapperyum install -y yum-utils device-mapper-persistent-data lvm2# set official yum sourceyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # set aliyun yum sourceyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # view config is successvim /etc/yum.repos.d/docker-ce.repo# uninstall old versionyum remove docker docker-common docker-selinux docker-engineyum update # ensure yum package is up to date. a long time... 1.2 Install123456# list all docker version, choose offical versionyum list docker-ce --showduplicates | sort -r # 由于repo中默认只开启stable仓库，故这里安装的是最新稳定版17.12.0yum install docker-ce -y # verify install is success, client and service should updocker version 1.3 start docker123systemctl start docker systemctl enable dockersystemctl status docker 1.4 config docker image source1234567891011121314151617181920212223# 配置镜像地址,注意地址是自己申请的，有可能失效，需要更新确认。# 阿里云-控制台-https://cr.console.aliyun.com/cn-hangzhou/instances/repositories# 其他镜像"registry-mirrors": ["http://hub-mirror.c.163.com"]sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'&#123; "registry-mirrors": ["https://8q8njuiz.mirror.aliyuncs.com"]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker# 解释：tee 命令相当于管道的一个T型接头。它将从 STDIN 过来的数据同时发往两处。一处是STDOUT ，另一处是 tee 命令行所指定的文件名&#123; "registry-mirrors": [ "https://mirror.ccs.tencentyun.com" ]&#125;docker info查看Registry Mirrors 2 Edit and Inspect Docker2.1 change port1234567# centos下容器的配置文件路径,其中的hashofthecontainer是docker镜像的hash值，可以通过docker ps或者docker inspect containername查看。（CONTAINER ID就可以看出来）vim /var/lib/docker/containers/[hash_of_the_container]/hostconfig.json# 文件中其中有一项是PortBindings，其中8080/tcp对应的是容器内部的8080端口，HostPort对应的是映射到宿主机的端口9190。systemctl restart docker 2.2 stats docker Mem and CPU12# 查看所有docker容器占用内存、CPU情况docker stats $(docker ps --format=&#123;&#123;.Names&#125;&#125;) 2.3 Docker logs12345678docker logs --helpdocker logs -f -t --since="2017-05-31" --tail=10 container_name# --since : 此参数指定了输出日志开始日期，或者 --since 34m。# -f : 查看实时日志# -t : 查看日志产生的日期# -tail=10 : 查看最后的10条日志。# container_name : 容器名称 2.4 into Docker container1234docker exec -it ub bash# ub is container name# exit to exit container# docker default do not support vim 2.5 docker install vim12apt-get updateapt-get install vim 2.6 centos下新用户使用docker123456789解决方案：sudo gpasswd -a $USER docker #将当前用户添加至docker用户组newgrp docker #更新docker用户组# 错误：Got permission denied while trying to connect to the Docker daemon socket# 自己理解：docker默认对docker用户组给与权限# 本质是修改了/etc/group 下的docker:x:992:pc,mac,nodesudo groupadd docker # 添加docker用户组sudo gpasswd -a $frank docker # 检测当前用户frank是否已经在docker用户组中 2.7 docker与本机传文件1234567# docker container to host# docker cp tc:docker容器内的文件全路径 本机保存文件的全路径docker cp tc:/data/configure.txt E:\PHP\configure.txt# host to container# docker cp 本机保存文件的全路径 container_id:docker容器内的文件全路径docker cp E:\PHP\configure.txt tc:/data1/configure.txt]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-Base]]></title>
    <url>%2F2020%2F08%2F20%2Futils-docker-docker-base%2F</url>
    <content type="text"><![CDATA[Author: gzxu@vip.qq.com Date: July 25 2020 1. Install and Config Docker1.1 Before Install12345678910111213141516uname -r # Docker require CentOS core above 3.10# with ubuntu：sudo apt# install dependies package:yum-util offer yum-config-manager，another two is dependies of devicemapperyum install -y yum-utils device-mapper-persistent-data lvm2# set official yum sourceyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # set aliyun yum sourceyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # view config is successvim /etc/yum.repos.d/docker-ce.repo# uninstall old versionyum remove docker docker-common docker-selinux docker-engineyum update # ensure yum package is up to date. a long time... 1.2 Install123456# list all docker version, choose offical versionyum list docker-ce --showduplicates | sort -r # 由于repo中默认只开启stable仓库，故这里安装的是最新稳定版17.12.0yum install docker-ce -y # verify install is success, client and service should updocker version 1.3 start docker123systemctl start docker systemctl enable dockersystemctl status docker 1.4 config docker image source1234567891011121314151617181920212223# 配置镜像地址,注意地址是自己申请的，有可能失效，需要更新确认。# 阿里云-控制台-https://cr.console.aliyun.com/cn-hangzhou/instances/repositories# 其他镜像"registry-mirrors": ["http://hub-mirror.c.163.com"]sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'&#123; "registry-mirrors": ["https://8q8njuiz.mirror.aliyuncs.com"]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker# 解释：tee 命令相当于管道的一个T型接头。它将从 STDIN 过来的数据同时发往两处。一处是STDOUT ，另一处是 tee 命令行所指定的文件名&#123; "registry-mirrors": [ "https://mirror.ccs.tencentyun.com" ]&#125;docker info查看Registry Mirrors 2 Edit and Inspect Docker2.1 change port1234567# centos下容器的配置文件路径,其中的hashofthecontainer是docker镜像的hash值，可以通过docker ps或者docker inspect containername查看。（CONTAINER ID就可以看出来）vim /var/lib/docker/containers/[hash_of_the_container]/hostconfig.json# 文件中其中有一项是PortBindings，其中8080/tcp对应的是容器内部的8080端口，HostPort对应的是映射到宿主机的端口9190。systemctl restart docker 2.2 stats docker Mem and CPU12# 查看所有docker容器占用内存、CPU情况docker stats $(docker ps --format=&#123;&#123;.Names&#125;&#125;) 2.3 Docker logs12345678docker logs --helpdocker logs -f -t --since="2017-05-31" --tail=10 container_name# --since : 此参数指定了输出日志开始日期，或者 --since 34m。# -f : 查看实时日志# -t : 查看日志产生的日期# -tail=10 : 查看最后的10条日志。# container_name : 容器名称 2.4 into Docker container1234docker exec -it ub bash# ub is container name# exit to exit container# docker default do not support vim 2.5 docker install vim12apt-get updateapt-get install vim 2.6 centos下新用户使用docker123456789解决方案：sudo gpasswd -a $USER docker #将当前用户添加至docker用户组newgrp docker #更新docker用户组# 错误：Got permission denied while trying to connect to the Docker daemon socket# 自己理解：docker默认对docker用户组给与权限# 本质是修改了/etc/group 下的docker:x:992:pc,mac,nodesudo groupadd docker # 添加docker用户组sudo gpasswd -a $frank docker # 检测当前用户frank是否已经在docker用户组中 2.7 docker与本机传文件1234567# docker container to host# docker cp tc:docker容器内的文件全路径 本机保存文件的全路径docker cp tc:/data/configure.txt E:\PHP\configure.txt# host to container# docker cp 本机保存文件的全路径 container_id:docker容器内的文件全路径docker cp E:\PHP\configure.txt tc:/data1/configure.txt]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-Example]]></title>
    <url>%2F2020%2F08%2F20%2Futils-docker-docker-example%2F</url>
    <content type="text"><![CDATA[1. 数据库部分1.1 docker pull mysql:5.712docker run -d -p 3306:3306 -v /data/docker/mysql/datadir:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=Gz,.9918xu --name mysql mysql:5.7# 不需要提前创建/data/docker/mysql目录 12345# 设置支持utf-8解决方案（已测试有效）sudo docker exec -it mysql bashecho "character-set-server=utf8" &gt;&gt; /etc/mysql/mysql.conf.d/mysqld.cnfexitsudo docker restart mysql 1234567docker exec -it mysql bashmysql -uroot -p# 给root设置密码ALTER USER 'root' IDENTIFIED WITH mysql_native_password BY '123456';# 添加远程登录用户CREATE USER 'admin'@'%' IDENTIFIED WITH mysql_native_password BY '123456';GRANT ALL PRIVILEGES ON *.* TO 'admin'@'%'; 1234docker ps -a --no-trunc # ps列出container -a列出全部, --no-trunc不截断docker stop mysqldocker restart mysqldocker rm mysql 1.2 docker pull redis1234docker run -d -p 6379:6379 --name redis redis --requirepass &quot;Gzxu2012&quot;# 上面的第二个redis是repository或者image id# 查看redis版本docker exec -it redis redis-server -v 1.3 docker pull mongo1234567docker run -d -p 27017:27017 -v /data/docker/mongo/db:/data/db --name mongo mongo --auth注意：前面27017是虚拟机的端口，后面27017是docker内的端口注意：重新创建的时候记得删除/data/docker/mongo/db下的数据# 还原数据docker cp leanote_install_data mongo:/leanote_install_datamongorestore -h 127.0.0.1:27017 --authenticationDatabase admin -u root -p Gzxu2012 -d leanote /leanote_install_data --drop 12# 用户及权限管理docker exec -it mongo mongo admin 1.数据库用户角色：read、readWrite;2.数据库管理角色：dbAdmin、dbOwner、userAdmin；3.集群管理角色：clusterAdmin、clusterManager、clusterMonitor、hostManager；4.备份恢复角色：backup、restore5.所有数据库角色：readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase6.超级用户角色：root 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354db.createUser( &#123; user:&quot;root&quot;, pwd:&quot;Gzxu2012&quot;, roles:[&quot;root&quot;] &#125;);db.auth(&quot;root&quot;,&quot;Gzxu2012&quot;);show dbs;show tables;show users;db.system.users.find();# 删除用户db.dropUser(&apos;frank&apos;)注意此时添加的用户都只用于admin数据库，而非你存储业务数据的数据库db.createUser(&#123; user: &apos;admin&apos;, pwd: &apos;Gzxu2012&apos;, roles: [&#123; role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; &#125; ] &#125;); # 必须先通过此处登录一下，然后切换其他db才能创建用户。# 用户创建在哪个db下就说明登录的时候需要验证哪个dbuse admin;db.auth(&quot;admin&quot;,&quot;Gzxu2012&quot;);db.createUser(&#123; user: &apos;rw&apos;, pwd: &apos;Gzxu2012&apos;, roles: [&apos;readWriteAnyDatabase&apos;] &#125;);# 此用户需要验证admin数据库，但可以读写所有数据库。db.createUser(&#123; user: &apos;frank&apos;, pwd: &apos;Gzxu2012&apos;, roles: [ &#123; role: &quot;readWrite&quot;, db: &quot;app&quot; &#125; ] &#125;);# 此用户需要验证admin数据库，但可以读写app数据库。use app;db.createUser(&#123; user: &apos;app&apos;, pwd: &apos;Gzxu2012&apos;, roles: [ &#123; role: &quot;readWrite&quot;, db: &quot;app&quot; &#125; ] &#125;);# 此用户需要验证app数据库，但可以读写app数据库。# 退出重新登录，写入测试数据use app;db.auth(&quot;app&quot;,&quot;Gzxu2012&quot;);db.test.save(&#123;name:&quot;gzxu@vip.qq.com&quot;&#125;);# 需要切换到use对应的db上db.runCommand( &#123; updateUser:&quot;frank&quot;, roles:[&#123;role:&quot;readWrite&quot;, db:&quot;hi&quot;&#125;,&#123;role:&quot;readWrite&quot;, db:&quot;app&quot;&#125;] &#125;);db.test.save(&#123;name:&quot;frank&quot;&#125;);db.test.insert() --直接插入不更新# 只更新找到的第一条，并且会替换该字段db.test.update(&#123;name:&quot;frank&quot;&#125;,&#123;age:16&#125;);# 只更新找到的第1条数据，新增字段db.test.update(&#123;name:&quot;frank&quot;&#125;,&#123;$set:&#123;age:123&#125;&#125;);# 更新所有找到匹配的数据db.test.update(&#123;name:&quot;frank&quot;&#125;,&#123;$set:&#123;age:30&#125;&#125;, &#123;multi: true&#125;);db.test.find(); 1.3 docker pull postgres:9.61234docker run -d -p 5432:5432 -v /data/docker/postgres/pgdata:/var/lib/postgresql/data -e POSTGRES_PASSWORD=Gz,.9918xu --name pg postgres:9.6# 注意：postgres镜像默认的用户名为postgres， 登陆口令为创建容器是指定的值。# 执行下列sql语句看版本select version(); 1.4 docker pull elasticsearch123456789101112docker run -d -p 9200:9200 -p 9300:9300 -e ES_JAVA_OPTS=&quot;-Xms256m -Xmx256m&quot; --name es elasticsearch# 访问http://docker:9200sysctl -w vm.max_map_count=262144docker pull kibanadocker run --name kibana -p 5601:5601 \--link es:es \-e &quot;elasticsearch.host=http://es:9200&quot; \-d kibana 1.5 docker pull couchbase1234567891011docker run -d --name cb -p 8091-8094:8091-8094 -p 11210:11210 couchbasehttp://docker:8091docker exec -it cb bashcd /opt/couchbase/bin./cbq -u admin -p Gzxu2012 -engine=http://127.0.0.1:8091/cbq&gt; SELECT callsign FROM `travel-sample` LIMIT 5;# 镜像的导入导出# 容器的导入导出docker export f2582758af13 | gzip &gt; ubuntu-web.tar.gzzcat ubuntu-web.gz | docker import - ubuntu-web 2. 容器2.1 docker pull tomcat123456docker run -d -p 8899:8080 -v /data/docker/tomcat/webapps:/usr/local/tomcat/webapps --name tc tomcat:8.5docker exec -it tc bashcp webapps.dist/* webapps/不需要重启实际上只需要在本地的webapps下创建ROOT目录即可，不需要重命名或者拷贝文件到webapps下 123456789可以利用manager进行容器内的管理，容器内修改：conf/tomcat-user.xml下，分别对mamager和host-manager进行管理&lt;role rolename=&quot;manager-gui&quot;/&gt; &lt;role rolename=&quot;admin-gui&quot;/&gt; &lt;user username=&quot;tomcat&quot; password=&quot;nokia3&quot; roles=&quot;manager-gui&quot;/&gt; &lt;user username=&quot;admin&quot; password=&quot;nokia3&quot; roles=&quot;admin-gui&quot;/&gt;容器外修改webapps/manager/META-INF/context.xml替换allow=&quot;^.*$&quot;内容&lt;Context antiResourceLocking=&quot;false&quot; privileged=&quot;true&quot; &gt; &lt;Valve className=&quot;org.apache.catalina.valves.RemoteAddrValve&quot; allow=&quot;^.*$&quot; /&gt; &lt;/Context&gt; 12解决tomcat访问Oracle时区的问题ln -snf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo Asia/Shanghai &gt; /etc/timezone 2.2 docker pull nginx123docker run -d -p 9901:80 -v /data/docker/nginxs/nginx9901/www:/usr/share/nginx/html -v /data/docker/nginxs/nginx9901/conf/conf.d:/etc/nginx/conf.d -v /data/docker/nginxs/nginx9901/logs:/var/log/nginx --name ng9901 nginxdocker run -d -p 88:80 -v /data/docker/nginx/www:/usr/share/nginx/html -v /data/docker/nginx/conf/conf.d:/etc/nginx/conf.d -v /data/docker/nginx/logs:/var/log/nginx --name ng nginx 1234567891011121314151617181920212223242526vim /data/docker/nginx/conf/conf.d/default.confserver &#123; listen 80; server_name localhost; location / &#123; root /usr/share/nginx/html; index index.html index.htm; &#125; # 匹配url中带有api的，并转发 location /api &#123; # 利用正则进行匹配#去掉api前缀，$1是正则中的第一串,这样后端的接口也不需要带api了 rewrite ^/api/(.*)$ /$1 break; # 转发的参数设定 proxy_pass http://192.168.3.66:8899; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125;&#125; 123456789101112vim /data/docker/nginx/www/index.html&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;title&gt;Frank’s Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;我的第一个标题&lt;/h1&gt; &lt;p&gt;我的第一个段落。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 2.3 docker pull httpd:2.412345678docker run -dit --name apache2 -p 99:80 -v /data/docker/apache2/www/:/usr/local/apache2/htdocs/ httpd:2.4docker exec -it apache2 bashmkdir –p /data/docker/apache2/conf/docker cp apache2:/usr/local/apache2/conf/httpd.conf /data/docker/apache2/conf/httpd.confls /data/docker/apache2/confdocker stop apache2;docker rm apache2docker run -dit --name apache2 -p 99:80 -v /data/docker/apache2/www/:/usr/local/apache2/htdocs/ -v /data/docker/apache2/conf/httpd.conf:/usr/local/apache2/conf/httpd.conf httpd:2.4 12345678910111213141516171819202122232425262728sudo vim /data/docker/apache2/conf/httpd.conf按G键（大写的），切到vi的最后一行，添加如下代码:IndexOptions Charset=UTF-8添加完成后输入:wq!，保存并退出vi。docker restart apache21)找到 LoadModule 对应的位置，加入以下代码：LoadModule autoindex_module modules/mod_autoindex.so Include conf/extra/httpd-autoindex.conf2) 找到以下代码：&lt;Directory /&gt; AllowOverride none Require all deny&lt;/Directory&gt;修改deny为 granted找到&lt;Directory &quot;/usr/local/apache2/htdocs&quot;&gt;Options Indexes FollowSymLinks AllowOverride None # Allow open access: Require all granted allow from all IndexOptions FancyIndexing FoldersFirst NameWidth=* DescriptionWidth=* SuppressHTMLPreamble HTMLTable IndexOptions Charset=utf-8 IconHeight=16 IconWidth=16 SuppressRules IndexIgnore web header.html footer.html actions defects HeaderName /web/header.html ReadmeName /web/footer.html IndexOrderDefault Ascending DateServerSignature Off3）apache2\www\web增加header.html，footer.html 2.4 docker pull fauria/vsftpd12345678910docker run -d --name ftp -p 21:21 -p 20:20 -p 21100-21110:21100-21110 -v /root/docker/ftp:/home/vsftpd -e FTP_USER=ftpuser -e FTP_PASS=ftpuser -e PASV_ADDRESS=192.168.72.111 -e PASV_MIN_PORT=21100 -e PASV_MAX_PORT=21110 fauria/vsftpd#docker exec -i -t ftp bash#vi /etc/vsftpd/virtual_users.txt#mkdir /home/vsftpd/ftpuser#/usr/bin/db_load -T -t hash -f /etc/vsftpd/virtual_users.txt /etc/vsftpd/virtual_users.db#docker restart ftp# 一定要确保端口通过安全组ftp://ftpuser:ftpuser@192.168.72.111/ 3. 其他/未分类3.1 docker pull rabbitmq:3.7-management123docker run -d -p 5672:5672 -p 15672:15672 --name mq rabbitmq:3.7-managementhttp://cvm:15672guest,guest 3.2 docker pull wordpress1docker run -d -p 8081:80 -e WORDPRESS_DB_HOST=192.168.72.111:3306 -e WORDPRESS_DB_USER=root -e WORDPRESS_DB_PASSWORD=password --name wp wordpress 3.3 docker pull jenkins12docker run -d -p 86:8080 -p 50000:50000 -v /etc/localtime:/etc/localtime --name jk jenkinsdocker exec jk tail /var/jenkins_home/secrets/initialAdminPassword 3.4 docker pull redmin123# 先启动postgresdocker run -d -p 3000:3000 -v /root/docker/redmine/datadir:/usr/src/redmine/files --link pg:postgres --name red redminehttp://docker:3000 3.5 docker pull owncloud12docker run -d -p 89:80 -v /data/docker/owncloud:/var/www/html --link mysql:mysql --name oc owncloud--link my-mysql:mysql ：将 owncloud容器(客户) 链接到mysql容器(服务)，链接别名(可省略)：mysql。此处的意思是在owncloud开始配置的时候，选择mysql/mariadb，输入数据库的用户名和密码以及新建数据库的名字如own_cloud，重要的是最下方localhost修改为别名mysql。 3.6 docker pull portainer/portainer1docker run -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock --restart=always --name ui portainer/portainer 3.7 docker pull gitlab/gitlab-ce12345678910111213#比较大，需要的资源比较多，未成功# gitlab-ce为稳定版本，后面不填写版本则默认pull最新latest版本docker run -d -p 443:443 -p 8899:8899 -p 222:22 -v /root/docker/gitlab/config:/etc/gitlab -v /root/docker/gitlab/logs:/var/log/gitlab -v /root/docker/gitlab/data:/var/opt/gitlab --name gl gitlab/gitlab-cevim /root/docker/gitlab/config/gitlab.rb# 配置http协议所使用的访问地址,不加端口号默认为80external_url &apos;http://192.168.72.111&apos;# 配置ssh协议所使用的访问地址和端口gitlab_rails[&apos;gitlab_ssh_host&apos;] = &apos;192.168.72.111&apos;gitlab_rails[&apos;gitlab_shell_ssh_port&apos;] = 222 # 此端口run时22映射的222端口docker restart gl 3.8 docker pull ubuntu:15.10123456789101112docker run --name ub ubuntu:15.10 /bin/echo &quot;Hello world&quot;docker logs ubdocker run -i -t --name ubi ubuntu:15.10 /bin/bash# -t:在新容器内指定一个伪终端或终端。# -i:允许你对容器内的标准输入 (STDIN) 进行交互。docker run -d --name ubd ubuntu:15.10 /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;docker logs ubd# 日志随容器的删除而删除# 再次进入交互环境docker exec -it ub bash docker exec -it ub sh 3.9 docker pull python:2.71docker run -it --rm --name py2 -v &quot;$PWD&quot;:/usr/src/myapp -w /usr/src/myapp python:2.7 python py.py 3.10 docker pull python:3.7.712docker run -it --rm --name py3 -v &quot;$PWD&quot;:/usr/src/myapp -w /usr/src/myapp python:3.7.7 python first.py# 经过测试，这类docker都会运行一下直接退出，不保存container 3.11 docker pull netdata12345678910111213docker run -d --name=netdata \ -p 3008:19999 \ -v netdatalib:/var/lib/netdata \ -v netdatacache:/var/cache/netdata \ -v /etc/passwd:/host/etc/passwd:ro \ -v /etc/group:/host/etc/group:ro \ -v /proc:/host/proc:ro \ -v /sys:/host/sys:ro \ -v /etc/os-release:/host/etc/os-release:ro \ --restart unless-stopped \ --cap-add SYS_PTRACE \ --security-opt apparmor=unconfined \ netdata/netdata]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win2linux]]></title>
    <url>%2F2020%2F08%2F17%2Futils-win2linux%2F</url>
    <content type="text"><![CDATA[windows的话，你需要装一下git bash软件，然后把bin目录添加到系统环境变量PATH中，你会发现很多linux命令都在windows上可用了。]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git忽略对已经纳入版本管理的文件]]></title>
    <url>%2F2020%2F08%2F17%2Futils-git-git-ignore-file-change%2F</url>
    <content type="text"><![CDATA[项目开发过程中，会遇到本地配置文件每个开发人员不同的情况，但如果遇到类似数据库配置这种最终需要加入 git 版本控制的配置，则会陷入两难境地。要么不跟踪，要么有人提交后其他人同步下来必须手动修改，非常麻烦。其实，对于已被纳入版本管理的文件，git 也提供了很好的解决办法。 12345678# 告诉git忽略对已经纳入版本管理的文件 .classpath 的修改，git 会一直忽略此文件直到重新告诉 git 可以再次跟踪此文件$ git update-index --assume-unchanged .classpath# 告诉 git 恢复跟踪 $ git update-index --no-assume-unchanged .classpath# 查看当前被忽略的、已经纳入版本库管理的文件：$ git ls-files -v | grep -e "^[hsmrck]"]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-Base]]></title>
    <url>%2F2020%2F08%2F17%2Futils-git-git-base%2F</url>
    <content type="text"><![CDATA[配置git环境git config –global user.email “dirsky@126.com“git config –global user.name “frank”git config –global push.default simple把RSA.public填写到git远程服务器上 查看配置git config –global -l 测试ssh -T git@gitee.comssh -T git@github.com git initgit remote add origin git@github.com:dirsky/sublime-lab.gitgit pull origin master 删除git remote rm origin 添加git add .git commit -m “init commit”git remote add origin git@github.com:dirsky/express-test.git 建立本地与远程的关联git push -u origin master git config –global alias.acp ‘!f() { git add -A &amp;&amp; git commit -m “$@” &amp;&amp; git push; }; f’使用时只需要 git acp “内容” find . -name ‘*.DS_Store’ -type f -delete删除项目下的 ds_Store]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GIt-hosts]]></title>
    <url>%2F2020%2F08%2F17%2Futils-git-github-hosts%2F</url>
    <content type="text"><![CDATA[access github slow 140.82.113.4 github.com199.232.69.194 github.global.ssl.fastly.net]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git日志回显设置中文]]></title>
    <url>%2F2020%2F08%2F17%2Futils-git-git-log-err-code%2F</url>
    <content type="text"><![CDATA[Git Bash command window insert below code: 123git config --global i18n.commitencoding utf-8git config --global i18n.logoutputencoding utf-8export LESSCHARSET=utf-8]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile]]></title>
    <url>%2F2020%2F08%2F11%2Futils-docker-docker-compose%2F</url>
    <content type="text"><![CDATA[docker-compose.yml1234567891011version: &apos;3&apos;services: test-boot: build: context: ./ dockerfile: ./Dockerfile image: gzxu/test-boot container_name: gzxu-tb restart: always ports: - 8080:8080]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile]]></title>
    <url>%2F2020%2F08%2F11%2Futils-docker-%E7%BC%96%E5%86%99Dockerfile%2F</url>
    <content type="text"><![CDATA[下面是Dockerfile12345678910111213141516171819202122232425262728293031323334353637383940# VERSION 0.0.1FROM java:8MAINTAINER guozhong.xu@qq.com# VOLUME 指定了临时文件目录为/tmp。# 其效果是在主机 /var/lib/docker 目录下创建了一个临时文件，并链接到容器的/tmVOLUME /tmp# 将jar包添加到容器中并更名为app.jarCOPY test-0331-boot-0.0.1-SNAPSHOT.jar app.jarRUN bash -c &quot;touch /app.jar&quot;EXPOSE 8080# 为了缩短 Tomcat 的启动时间，添加java.security.egd的系统属性指向/dev/urandomENTRYPOINT [&quot;java&quot;,&quot;-Djava.security.egd=file:/dev/./urandom&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;]docker build -t gzxu/test-boot .docker run -d -p 8080:8080 --name tb gzxu/test-bootdocker rmi -f id# 保存包含依赖的镜像docker save &gt; test-boot.tar gzxu/test-boot # -o：指定保存的镜像的名字；test-boot.tar：保存到本地的镜像名称；gzxu/test-boot。# 载入镜像docker load &lt; test-boot.taryum -y install openssl openssl-devel ncurses-devel.x86_64 bzip2-devel sqlite-devel python-devel zlibyum install -y epel-release &amp;&amp; yum install -y python-pip &amp;&amp; pip install --upgrade pip &amp;&amp; pip install docker-composedocker-compose versiondocker-compose up -d]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[npm和yarn的区别及增加国内镜像（淘宝镜像）]]></title>
    <url>%2F2019%2F12%2F16%2Fui-npm%E5%92%8Cyarn%E7%9A%84%E5%8C%BA%E5%88%AB%E5%8F%8A%E5%A2%9E%E5%8A%A0%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F%EF%BC%88%E6%B7%98%E5%AE%9D%E9%95%9C%E5%83%8F%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Yarn和npm命令对比 npm yarn 说明 npm install, npm i yarn 安装所有依赖包 npm install react –save, -S yarn add react 添加依赖包 npm uninstall react –save yarn remove react 移除依赖包 npm install react –save-dev, -D yarn add react –dev 将依赖项添加到不同依赖项类别中 npm update –save yarn upgrade 升级依赖包 npm uninstall 模块：删除模块，但不删除模块留在package.json中的对应信息 npm uninstall 模块 –save 删除模块，同时删除模块留在package.json中dependencies下的对应信息 npm uninstall 模块 –save-dev删除模块，同时删除模块留在package.json中devDependencies下的对应信息 yarn和npm淘宝镜像12345npm config set registry https://registry.npm.taobao.orgnpm config set disturl https://npm.taobao.org/distnpm config set electron_mirror https://npm.taobao.org/mirrors/electron/npm config set sass_binary_site https://npm.taobao.org/mirrors/node-sass/npm config set phantomjs_cdnurl https://npm.taobao.org/mirrors/phantomjs/ 12345npm config set registry https://registry.npm.taobao.orgnpm config set disturl https://npm.taobao.org/distnpm config set electron_mirror https://npm.taobao.org/mirrors/electron/npm config set sass_binary_site https://npm.taobao.org/mirrors/node-sass/npm config set phantomjs_cdnurl https://npm.taobao.org/mirrors/phantomjs/ 持久化文件路径在 C:\Users\用户名 .npmrc和.yarnrc文件]]></content>
      <categories>
        <category>ui</category>
      </categories>
      <tags>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker容器 MySQL中文乱码解决方案]]></title>
    <url>%2F2019%2F09%2F14%2Futils-docker-Docker%E5%AE%B9%E5%99%A8-MySQL%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[解决方案docker exec进入容器1sudo docker exec -it mysql bash 注意：mysql是容器的别名 将 character-set-server=utf8 写入mysql配置文件1echo &quot;character-set-server=utf8&quot; &gt;&gt; /etc/mysql/mysql.conf.d/mysqld.cnf 重启mysql 容器使以上修改生效1sudo docker restart mysql 启发永久性处理方案是在创建容器的时候直接增加下列参数 1--character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简历中对了解、熟悉、掌握、精通的界定]]></title>
    <url>%2F2019%2F09%2F13%2Fmgr-%E4%BA%86%E8%A7%A3%E3%80%81%E7%86%9F%E6%82%89%E3%80%81%E6%8E%8C%E6%8F%A1%E3%80%81%E7%B2%BE%E9%80%9A%E7%95%8C%E5%AE%9A%2F</url>
    <content type="text"><![CDATA[“了解”，即能够对所需要的知识有所认识，但这种认识可能是局部或点状地，当谈及这些知识时，能意识到自己知道； “熟悉”，即对所需要的知识有系统性地认识，能够找到知识之间的联系,把点状地认识连成线，且可以运用相关知识解决部分实际问题； “掌握”，即对所需要的知识有全局性的认识，能在将各个知识点串成线的基础之上，认清整个知识网络，且可以在实际工作中自由运用； “精通”，即对所需要的知识能够做到融会贯通，不仅对知识网络有清晰的认识，而且能够将其与其他相关领域的知识相融合，能够在灵活运用知识的同时不断创新。]]></content>
      <categories>
        <category>mgr</category>
      </categories>
      <tags>
        <tag>规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql配置文件]]></title>
    <url>%2F2019%2F08%2F12%2Fdb-mysql-mysql-config%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728[mysqld]basedir = path # 使用给定目录作为根目录(安装目录)。datadir = path # 从给定目录读取数据库文件。pid-file = filename # 为mysqld程序指定一个存放进程ID的文件(仅适用于UNIX/Linux系统);socket = /tmp/mysql.sock # 为MySQL客户程序与服务器之间的本地通信指定一个套接字文件(Linux下默认是/var/lib/mysql/mysql.sock文件)port = 3306 # 指定MsSQL侦听的端口key_buffer = 384M # key_buffer是用于索引块的缓冲区大小，增加它可得到更好处理的索引(对所有读和多重写)。 索引块是缓冲的并且被所有的线程共享，key_buffer的大小视内存大小而定。table_cache = 512 # 为所有线程打开表的数量。增加该值能增加mysqld要求的文件描述符的数量。可以避免频繁的打开数据表产生的开销sort_buffer_size = 2M # 每个需要进行排序的线程分配该大小的一个缓冲区。增加这值加速ORDER BY或GROUP BY操作。 注意：该参数对应的分配内存是每连接独占！如果有100个连接，那么实际分配的总共排序缓冲区大小为100×6=600MBread_buffer_size = 2M # 读查询操作所能使用的缓冲区大小。和sort_buffer_size一样，该参数对应的分配内存也是每连接独享。query_cache_size = 32M # 指定MySQL查询结果缓冲区的大小read_rnd_buffer_size = 8M # 改参数在使用行指针排序之后，随机读用的。myisam_sort_buffer_size =64M # MyISAM表发生变化时重新排序所需的缓冲thread_concurrency = 8 # 最大并发线程数，取值为服务器逻辑CPU数量×2，如果CPU支持H.T超线程，再×2thread_cache = 8 # #缓存可重用的线程数skip-locking # 避免MySQL的外部锁定，减少出错几率增强稳定性。[mysqldump]max_allowed_packet =16M # 服务器和客户端之间最大能发送的可能信息包[myisamchk]key_buffer = 256Msort_buffer = 256Mread_buffer = 2Mwrite_buffer = 2M]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件版本英文缩写]]></title>
    <url>%2F2019%2F08%2F06%2Fmgr-%E8%BD%AF%E4%BB%B6%E7%89%88%E6%9C%AC%E8%8B%B1%E6%96%87%E7%BC%A9%E5%86%99%2F</url>
    <content type="text"><![CDATA[分类一1.Alpha版(内部测试版)：一般只在软件开发公司内部运行，不对外公开。主要是开发者自己对产品进行测试，检查产品是否存在缺陷、错误，验证产品功能与说明书、用户手册是否一致。Alpha版本的产品仍然需要完整的功能测试，而其功能亦未完善，但是可以满足一般需求。因为它是整个软件释出周期中的第一个主要阶段，所以它的名称是“Alpha”，希腊字母中的第一个字母。Alpha版本通常会送交到开发软件的组织或社群中的各个软件测试者，用作内部测试。在市场上，越来越多公司会邀请外部的客户或合作伙伴参与其软件的Alpha测试阶段。这令软件在此阶段有更大的可用性测试。在测试的第一个阶段中，开发者通常会进行白盒测试。其他测试会在稍后时间由其他测试团体以黑盒或灰盒技术进行，不过有时会同时进行。 2.Beta版(外部测试版)：软件开发公司为对外宣传，将非正式产品免费发送给具有典型性的用户，让用户测试该软件的不足之处及存在问题，以便在正式发行前进一步改进和完善。一般可通过Internet免费下载，也可以向软件公司索取。Beta版本是第一个对外公开的软件版本，是由公众参与的测试阶段。一般来说，Beta包含所有功能，但可能有一些已知问题和较轻微的臭虫(Bug)。Beta版本的测试者通常是开发软件的组织的客户，他们会以免费或优惠价钱得到软件，但会成为组织的免费测试者。Beta版本主要测试产品的支援和市场反应（在邀请Beta用户时）等。 3.Demo版(演示版)：主要是演示正式软件的部分功能，用户可以从中得知软件的基本操作，为正式产品的发售扩大影响。如果是游戏的话，则只有一两个关卡可以玩。该版本也可以从Internet上免费下载。 4.Enhanced版(增强版或加强版)：如果是一般软件，一般称作“增强版”，会加入一些实用的新功能。如果是游戏，一般称作“加强版”，会加入一些新的游戏场景和游戏情节等。这是正式发售的版本。 5.Free版(自由版)：这一般是个人或自由软件联盟组织的成员制作的软件，希望免费给大家使用，没有版权，一般也是通过Internet免费下载。 6.Full Version版(完全版)：也就是正式版，是最终正式发售的版本。 7.Shareware版(共享版)：有些公司为了吸引客户，对于他们制作的某些软件，可以让用户通过Internet免费下载的方式获取。不过，此版本软件多会带有一些使用时间或次数的限制，但可以利用在线注册或电子注册成为正式版用户。 8.Release版(发行版)：不是正式版，带有时间限制，也是为扩大影响所做的宣传策略之一。比如Windows Me的发行版就限制了只能使用几个月，可从Internet上免费下载或由公司免费奉送。Release Candidate（简称RC）指可能成为最终产品的版本，如果没有再出现问题则可释出正式版本。在此阶段，产品包含所有功能亦不会出现严重问题。通常此阶段的产品是接近完整的。微软公司很多时会使用此名称。在1990年代，苹果电脑把在这阶段的产品称为“Golden Master”，而最后的Golden Master为正式版本。这阶段亦称Gamma（更后期的称为Delta，及其后的希腊字母）。 9.Uprgade版(升级版)：当你有某个软件以前的正式版本时，可以购买升级版，将你的软件升级为最新版。升级后的软件与正式版在功能上相同，但价格会低些，这主要是为了给原有的正版用户提供优惠。 10.Retail版 （零售版）一般只针对个人的功能不是很全的版本，价格比较低，升级时间也有限制。 11.Cardware版属共享软件的一种，只要给作者回复一封电邮或明信片即可。（有的作者并由此提供注册码等），目前这种形式已不多见。 12.Plus版（增强版）不过这种大部分是在程序界面及多媒体功能上增强。 13.Preview版（预览版）软件商为了满足那些对新版本很关注的人，发布的可以看到大部分功能的测试软件。 14.Corporation &amp; Enterprise版（企业版）只针对企业发布的全功能版本，价格比较昂贵，服务非常齐全。 15.Standard版（标准版）软件商推荐大家使用的版本，这种版本一般比较稳定，BUG少。 16.Mini版迷你版也叫精简版只有最基本的功能，为那些想节省硬盘空间或者不追求华丽的人准备的。 分类二Premium – 超值版Professional – 专业版Express – 特别版Deluxe – 豪华版Regged – 已注册版CN – 简版CHT – 繁版EN – 英版Multilanguage – 多语言版Rip 是指从原版文件（一般是指光盘或光盘镜像文件）直接将有用的内容（核心内容）分离出来，剔除无用的文档，例如PDF说明文件啊，视频演示啊之类的东西，也可以算做是精简版吧…但主要内容功能是一点也不能缺少的！另：DVDrip是指将视频和音频直接从DVD光盘里以文件方式分离出来。trial 试用版（含有某些限制，如时间、功能，注册后也有可能变为正式版）RC 版 是 Release Candidate 的缩写，意思是发布倒计时，该版本已经完成全部功能并清除大部分的BUG。到了这个阶段只会除BUG，不会对软件做任何大的更改。RTM 版 这基本就是最终的版本，英文是 Release To Manufactur，意思是发布到生产商。 版本号V（Version）：即版本，通常用数字表示版本号。(如:EVEREST Ultimate v4.20.1188 Beta )Build：用数字或日期标示版本号的一种方式。(如:VeryCD eMule v0.48a Build 071112)SP：Service Pack，升级包。(如:Windows XP SP 2/Vista SP 1/Windows7 SP1)授权和功能划分：Trial：试用版，通常都有时间限制，有些试用版软件还在功能上做了一定的限制。可注册或购买成为正式版Unregistered：未注册版，通常没有时间限制，在功能上相对于正式版做了一定的限制。可注册或购买成为正式版。Demo：演示版，仅仅集成了正式版中的几个功能，不能升级成正式版。Lite：精简版。Full version：完整版，属于正式版。语言划分SC： Simplified Chinese简体中文版。CN： 简体中文版GBK： 简体中文汉字内码扩展规范版。TC： Traditional Chinese繁体中文版。CHT： 繁体中文版BIG5： 繁体中文大五码版。EN： 英文版Multilanguage： 多语言版UTF8： Unicode Transformation Format 8 bit，对现有的中文系统不是好的解决方案。开发阶段划分α（Alpha）版：内测版，内部交流或者专业测试人员测试用。Bug较多，普通用户最好不要安装。β（Beta）版：公测版，专业爱好者大规模测试用，存在一些缺陷，该版本也不适合一般用户安装。γ（Gamma）版：相当成熟的测试版，与即将发行的正式版相差无几。RC版：Release Candidate。RC 版。是 Release Candidate 的缩写，意思是发布倒计时，候选版本，处于Gamma阶段，该版本已经完成全部功能并清除大部分的BUG。到了这个阶段只会除BUG，不会对软件做任何大的更改。从Alpha到Beta再到Gamma是改进的先后关系，但RC1、RC2往往是取舍关系。SR版：修正版或更新版，修正了正式版推出后发现的Bug。Final版：正式版。 其他版本Enhance： 增强版或者加强版属于正式版Free： 自由版Release： 发行版 有时间限制Upgrade： 升级版Retail： 零售版Cardware：属共享软件的一种，只要给作者回复一封电邮或明信片即可。（有的作者并由此提供注册码等），目前这种形式已不多见。/ SPlus： 属增强版，不过这种大部分是在程序界面及多媒体功能上增强。Preview： 预览版Corporation &amp; Enterprise：企业版Standard： 标准版Mini： 迷你版也叫精简版只有最基本的功能Premium： 贵价版Professional：专业版Express： 特别版Deluxe： 豪华版Regged： 已注册版Rip：是指从原版文件（一般是指光盘或光盘镜像文件）直接将有用的内容（核心内容）分离出来，剔除无用的文档，例如PDF说明文件啊，视频演示啊之类的东西，也可以算做是精简版吧…但主要内容功能是一点也不能缺少的！另：DVDrip是指将视频和音频直接从DVD光盘里以文件方式分离出来。RTM： 这基本就是最终的版本，英文是 Release To Manufactur，意思是发布到生产商。OEM： Original Equipment Manufacturer 是给电脑生产厂的版本，无需多说。FPP： Full Packaged Product (FPP)/Retail 就是零售版（盒装软件），这种产品的光盘的卷标都带有”FPP”字样，比如英文WXP Pro的FPP版本的光盘卷标就是WXPFPP_EN，其中WX表示是Windows XP，P是Professional（H是Home），FPP表明是零售版本，EN是表明是英语。获得途径除了在商店购买之外，某些MSDN用户也可以得到。VLO： Volume Licensing for Organizations (VLO) 团体批量许可证（大量采购授权合约），这是为团体购买而制定的一种优惠方式。这种产品的光盘的卷标都带有”VOL”字样，取”Volume”前3个字母，以表明是批量，比如英文WXP Pro的VOL版本的光盘卷标就是WXPVOL_EN，其中WX表示是Windows XP，P是Professional（VOL没有Home版本），VOL表明是团体批量许可证版本，EN是表明是英语。获得途径主要是集团购买，某些MSDN用户也可以得到。]]></content>
      <categories>
        <category>mgr</category>
      </categories>
      <tags>
        <tag>规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[8 张图读懂大型网站技术架构]]></title>
    <url>%2F2019%2F04%2F23%2Fsubject-architecture-8-%E5%BC%A0%E5%9B%BE%E8%AF%BB%E6%87%82%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[1. 大型网站架构演化 2. 大型架构模式 3. 大型网站核心架构要素 4. 瞬时响应:网站的高性能架构 5. 万无一失:网站的高可用架构 6. 永无止境:网站的伸缩性架构 7. 随机应变:网站的可扩展性架构 8. 固若金汤:网站的安全机构]]></content>
      <categories>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>网站架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于大型网站系统架构你不得不懂的10个问题]]></title>
    <url>%2F2019%2F04%2F23%2Fsubject-architecture-%E5%85%B3%E4%BA%8E%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%BD%A0%E4%B8%8D%E5%BE%97%E4%B8%8D%E6%87%82%E7%9A%8410%E4%B8%AA%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1. 你使用过哪些组件或者方法来提升网站性能,可用性以及并发量 提高硬件能力、增加系统服务器。（当服务器增加到某个程度的时候系统所能提供的并发访问量几乎不变，所以不能根本解决问题） 使用缓存（本地缓存：本地可以使用JDK自带的 Map、Guava Cache.分布式缓存：Redis、Memcache.本地缓存不适用于提高系统并发量，一般是用处用在程序中。比如Spring是如何实现单例的呢？大家如果看过源码的话，应该知道，S把已经初始过的变量放在一个Map中，下次再要使用这个变量的时候，先判断Map中有没有，这也就是系统中常见的单例模式的实现。） 消息队列 （解耦+削峰+异步） 采用分布式开发 （不同的服务部署在不同的机器节点上，并且一个服务也可以部署在多台机器上，然后利用 Nginx 负载均衡访问。这样就解决了单点部署(All In)的缺点，大大提高的系统并发量） 数据库分库（读写分离）、分表（水平分表、垂直分表） 采用集群 （多台机器提供相同的服务） CDN 加速 (将一些静态资源比如图片、视频等等缓存到离用户最近的网络节点) 浏览器缓存 使用合适的连接池（数据库连接池、线程池等等） 适当使用多线程进行开发。 2. 设计高可用系统的常用手段 降级： 服务降级是当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行。降级往往会指定不同的级别，面临不同的异常等级执行不同的处理。根据服务方式：可以拒接服务，可以延迟服务，也有时候可以随机服务。根据服务范围：可以砍掉某个功能，也可以砍掉某些模块。总之服务降级需要根据不同的业务需求采用不同的降级策略。主要的目的就是服务虽然有损但是总比没有好； 限流： 防止恶意请求流量、恶意攻击，或者防止流量超出系统峰值； 缓存： 避免大量请求直接落到数据库，将数据库击垮； 超时和重试机制： 避免请求堆积造成雪崩； 回滚机制： 快速修复错误版本。 3. 现代互联网应用系统通常具有哪些特点? 高并发，大流量； 高可用：系统7×24小时不间断服务； 海量数据：需要存储、管理海量数据，需要使用大量服务器； 用户分布广泛，网络情况复杂：许多大型互联网都是为全球用户提供服务的，用户分布范围广，各地网络情况千差万别； 安全环境恶劣：由于互联网的开放性，使得互联网更容易受到攻击，大型网站几乎每天都会被黑客攻击； 需求快速变更，发布频繁：和传统软件的版本发布频率不同，互联网产品为快速适应市场，满足用户需求，其产品发布频率是极高的； 渐进式发展：与传统软件产品或企业应用系统一开始就规划好全部的功能和非功能需求不同，几乎所有的大型互联网网站都是从一个小网站开始，渐进地发展起来。 4. 谈谈你对微服务领域的了解和认识现在大公司都在用并且未来的趋势都是 Spring Cloud，而阿里开源的 Spring Cloud Alibaba 也是 Spring Cloud 规范的实现 。 我们通常把 Spring Cloud 理解为一系列开源组件的集合，但是 Spring Cloud并不是等同于 Spring Cloud Netflix 的 Ribbon、Feign、Eureka（停止更新）、Hystrix 这一套组件，而是抽象了一套通用的开发模式。它的目的是通过抽象出这套通用的模式，让开发者更快更好地开发业务。但是这套开发模式运行时的实际载体，还是依赖于 RPC、网关、服务发现、配置管理、限流熔断、分布式链路跟踪等组件的具体实现。 Spring Cloud Alibaba 是官方认证的新一套 Spring Cloud 规范的实现,Spring Cloud Alibaba 是一套国产开源产品集合，后续还会有中文 reference 和一些原理分析文章，所以，这对于国内的开发者是非常棒的一件事。阿里的这一举动势必会推动国内微服务技术的发展，因为在没有 Spring Cloud Alibaba 之前，我们的第一选择是 Spring Cloud Netflix，但是它们的文档都是英文的，出问题后排查也比较困难， 在国内并不是有特别多的人精通。Spring Cloud Alibaba 由阿里开源组件和阿里云产品组件两部分组成，其致力于提供微服务一站式解决方案，方便开发者通过 Spring Cloud 编程模型轻松开发微服务应用。 另外，Apache Dubbo Ecosystem 是围绕 Apache Dubbo 打造的微服务生态，是经过生产验证的微服务的最佳实践组合。在阿里巴巴的微服务解决方案中，Dubbo、Nacos 和 Sentinel，以及后续将开源的微服务组件，都是 Dubbo EcoSystem 的一部分。阿里后续也会将 Dubbo EcoSystem 集成到 Spring Cloud 的生态中。 5. 谈谈你对 Dubbo 和 Spring Cloud 的认识(两者关系)具体可以看公众号-阿里巴巴中间件的这篇文章:独家解读：Dubbo Ecosystem - 从微服务框架到微服务生态 Dubbo 与 Spring Cloud 并不是竞争关系，Dubbo 作为成熟的 RPC 框架，其易用性、扩展性和健壮性已得到业界的认可。未来 Dubbo 将会作为 Spring Cloud Alibaba 的 RPC 组件，并与 Spring Cloud 原生的 Feign 以及 RestTemplate 进行无缝整合，实现“零”成本迁移。 在阿里巴巴的微服务解决方案中，Dubbo、Nacos 和 Sentinel，以及后续将开源的微服务组件，都是 Dubbo EcoSystem 的一部分。我们后续也会将 Dubbo EcoSystem 集成到 Spring Cloud 的生态中。 6. 性能测试了解吗?说说你知道的性能测试工具?性能测试指通过自动化的测试工具模拟多种正常、峰值以及异常负载条件来对系统的各项性能指标进行测试。性能测试是总称，通常细分为： 基准测试： 在给系统施加较低压力时，查看系统的运行状况并记录相关数做为基础参考 负载测试： 是指对系统不断地增加压力或增加一定压力下的持续时间，直到系统的某项或多项性能指标达到安全临界值，例如某种资源已经达到饱和状态等 。此时继续加压，系统处理能力会下降。 压力测试： 超过安全负载情况下，不断施加压力（增加并发请求），直到系统崩溃或无法处理任何请求，依此获得系统最大压力承受能力。 稳定性测试： 被测试系统在特定硬件、软件、网络环境下，加载一定业务压力（模拟生产环境不同时间点、不均匀请求，呈波浪特性）运行一段较长时间，以此检测系统是否稳定。 后端程序员或者测试平常比较常用的测试工具是 JMeter（官网：https://jmeter.apache.org/）。Apache JMeter 是一款基于Java的压力测试工具(100％纯Java应用程序)，旨在加载测试功能行为和测量性能。它最初被设计用于 Web 应用测试但后来扩展到其他测试领域。 7. 对于一个单体应用系统,随着产品使用的用户越来越多,网站的流量会增加,最终单台服务器无法处理那么大的流量怎么办?这个时候就要考虑扩容了。《亿级流量网站架构核心技术》这本书上面介绍到我们可以考虑下面几步来解决这个问题： 第一步，可以考虑简单的扩容来解决问题。比如增加系统的服务器，提高硬件能力等等。 第二步，如果简单扩容搞不定，就需要水平拆分和垂直拆分数据／应用来提升系统的伸缩性，即通过扩容提升系统负载能力。 第三步，如果通过水平拆分／垂直拆分还是搞不定，那就需要根据现有系统特性，架构层面进行重构甚至是重新设计，即推倒重来。 对于系统设计，理想的情况下应支持线性扩容和弹性扩容，即在系统瓶颈时，只需要增加机器就可以解决系统瓶颈，如降低延迟提升吞吐量，从而实现扩容需求。 如果你想扩容，则支持水平/垂直伸缩是前提。在进行拆分时，一定要清楚知道自己的目的是什么，拆分后带来的问题如何解决，拆分后如果没有得到任何收益就不要为了拆而拆，即不要过度拆分，要适合自己的业务。 8. 大表优化的常见手段 当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下： 限定数据的范围： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内； 读/写分离： 经典的数据库拆分方案，主库负责写，从库负责读； 垂直分区： 根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。 如下图所示，这样来说大家应该就更容易理解了。垂直拆分的优点： 可以使得行数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂； 水平分区： 保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。 水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。水平拆分可以支持非常大的数据量。需要注意的一点是:分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 水平拆分最好分库 。水平拆分能够 支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨界点Join性能较差，逻辑复杂。《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。 下面补充一下数据库分片的两种常见方案： 客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。 中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。 9. 在系统中使用消息队列能带来什么好处?《大型网站技术架构》第四章和第七章均有提到消息队列对应用性能及扩展性的提升。 1) 通过异步处理提高系统性能如上图，在不使用消息队列服务器的时候，用户的请求数据直接写入数据库，在高并发的情况下数据库压力剧增，使得响应速度变慢。但是在使用消息队列之后，用户的请求数据发送给消息队列之后立即 返回，再由消息队列的消费者进程从消息队列中获取数据，异步写入数据库。由于消息队列服务器处理速度快于数据库（消息队列也比数据库有更好的伸缩性），因此响应速度得到大幅改善。 通过以上分析我们可以得出消息队列具有很好的削峰作用的功能——即通过异步处理，将短时间高并发产生的事务消息存储在消息队列中，从而削平高峰期的并发事务。 举例：在电子商务一些秒杀、促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。如下图所示：因为用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败。因此使用消息队列进行异步处理之后，需要适当修改业务流程进行配合，比如用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功，以免交易纠纷。这就类似我们平时手机订火车票和电影票。 2) 降低系统耦合性我们知道模块分布式部署以后聚合方式通常有两种：1.分布式消息队列和2.分布式服务。 先来简单说一下分布式服务： 目前使用比较多的用来构建SOA（Service Oriented Architecture面向服务体系结构）的分布式服务框架是阿里巴巴开源的Dubbo.如果想深入了解Dubbo的可以看我写的关于Dubbo的这一篇文章：《高性能优秀的服务框架-dubbo介绍》：https://juejin.im/post/5acadeb1f265da2375072f9c 再来谈我们的分布式消息队列： 我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。 我们最常见的事件驱动架构类似生产者消费者模式，在大型网站中通常用利用消息队列实现事件驱动结构。如下图所示：消息队列使利用发布-订阅模式工作，消息发送者（生产者）发布消息，一个或多个消息接受者（消费者）订阅消息。 从上图可以看到消息发送者（生产者）和消息接受者（消费者）之间没有直接耦合，消息发送者将消息发送至分布式消息队列即结束对消息的处理，消息接受者从分布式消息队列获取该消息后进行后续处理，并不需要知道该消息从何而来。对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计。 消息接受者对消息进行过滤、处理、包装后，构造成一个新的消息类型，将消息继续发送出去，等待其他消息接受者订阅该消息。因此基于事件（消息对象）驱动的业务架构可以是一系列流程。 另外为了避免消息队列服务器宕机造成消息丢失，会将成功发送到消息队列的消息存储在消息生产者服务器上，等消息真正被消费者服务器处理后才删除消息。在消息队列服务器宕机后，生产者服务器会选择分布式消息队列服务器集群中的其他服务器发布消息。 备注： 不要认为消息队列只能利用发布-订阅模式工作，只不过在解耦这个特定业务环境下是使用发布-订阅模式的，比如在我们的ActiveMQ消息队列中还有点对点工作模式，具体的会在后面的文章给大家详细介绍，这一篇文章主要还是让大家对消息队列有一个更透彻的了解。 这个问题一般会在上一个问题问完之后，紧接着被问到。“使用消息队列会带来什么问题？”这个问题要引起重视，一般我们都会考虑使用消息队列会带来的好处而忽略它带来的问题！ 10. 说说自己对 CAP 定理,BASE 理论的了解CAP 定理在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer’s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点： 一致性（Consistence） :所有节点访问同一份最新的数据副本 可用性（Availability）:每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据 分区容错性（Partition tolerance） : 分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。 CAP仅适用于原子读写的NOSQL场景中，并不适合数据库系统。现在的分布式系统具有更多特性比如扩展性、可用性等等，在进行系统设计和开发时，我们不应该仅仅局限在CAP问题上。 注意：不是所谓的3选2（不要被网上大多数文章误导了）: 大部分人解释这一定律时，常常简单的表述为：“一致性、可用性、分区容忍性三者你只能同时达到其中两个，不可能同时达到”。实际上这是一个非常具有误导性质的说法，而且在CAP理论诞生12年之后，CAP之父也在2012年重写了之前的论文。 当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能2选1。也就是说当网络分区之后P是前提，决定了P之后才有C和A的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。 我在网上找了很多文章想看一下有没有文章提到这个不是所谓的3选2，用百度半天没找到了一篇，用谷歌搜索找到一篇比较不错的，如果想深入学习一下CAP就看这篇文章把，我这里就不多BB了：《分布式系统之CAP理论》 ： http://www.cnblogs.com/hxsyl/p/4381980.html BASE 理论BASE 是 Basically Available（基本可用） 、Soft-state（软状态） 和 Eventually Consistent（最终一致性） 三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，它大大降低了我们对系统的要求。 BASE理论的核心思想： 即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。 BASE理论三要素： 基本可用： 基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。 比如： ①响应时间上的损失:正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了1~2秒；②系统功能上的损失：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面； 软状态： 软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时； 最终一致性： 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 参考 《大型网站技术架构》 《亿级流量网站架构核心技术》 《Java工程师修炼之道》 https://www.cnblogs.com/puresoul/p/5456855.html]]></content>
      <categories>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>网站架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL高性能优化规范建议]]></title>
    <url>%2F2019%2F04%2F05%2Fdb-mysql-MySQL%E9%AB%98%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E8%A7%84%E8%8C%83%E5%BB%BA%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[数据库命令规范 所有数据库对象名称必须使用小写字母并用下划线分割 所有数据库对象名称禁止使用 MySQL 保留关键字（如果表名中包含关键字查询时，需要将其用单引号括起来） 数据库对象的命名要能做到见名识意，并且最后不要超过 32 个字符 临时库表必须以 tmp_为前缀并以日期为后缀，备份表必须以 bak_为前缀并以日期 (时间戳) 为后缀 所有存储相同数据的列名和列类型必须一致（一般作为关联列，如果查询时关联列类型不一致会自动进行数据类型隐式转换，会造成列上的索引失效，导致查询效率降低） 数据库基本设计规范1. 所有表必须使用 Innodb 存储引擎没有特殊要求（即 Innodb 无法满足的功能如：列存储，存储空间数据等）的情况下，所有表必须使用 Innodb 存储引擎（MySQL5.5 之前默认使用 Myisam，5.6 以后默认的为 Innodb）。 Innodb 支持事务，支持行级锁，更好的恢复性，高并发下性能更好。 2. 数据库和表的字符集统一使用 UTF8兼容性更好，统一字符集可以避免由于字符集转换产生的乱码，不同的字符集进行比较前需要进行转换会造成索引失效，如果数据库中有存储 emoji 表情的需要，字符集需要采用 utf8mb4 字符集。 3. 所有表和字段都需要添加注释使用 comment 从句添加表和列的备注，从一开始就进行数据字典的维护 4. 尽量控制单表数据量的大小,建议控制在 500 万以内。500 万并不是 MySQL 数据库的限制，过大会造成修改表结构，备份，恢复都会有很大的问题。 可以用历史数据归档（应用于日志数据），分库分表（应用于业务数据）等手段来控制数据量大小 5. 谨慎使用 MySQL 分区表分区表在物理上表现为多个文件，在逻辑上表现为一个表； 谨慎选择分区键，跨分区查询效率可能更低； 建议采用物理分表的方式管理大数据。 6.尽量做到冷热数据分离,减小表的宽度 MySQL 限制每个表最多存储 4096 列，并且每一行数据的大小不能超过 65535 字节。 减少磁盘 IO,保证热数据的内存缓存命中率（表越宽，把表装载进内存缓冲池时所占用的内存也就越大,也会消耗更多的 IO）； 更有效的利用缓存，避免读入无用的冷数据； 经常一起使用的列放到一个表中（避免更多的关联操作）。 7. 禁止在表中建立预留字段预留字段的命名很难做到见名识义。 预留字段无法确认存储的数据类型，所以无法选择合适的类型。 对预留字段类型的修改，会对表进行锁定。 8. 禁止在数据库中存储图片,文件等大的二进制数据通常文件很大，会短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机 IO 操作，文件很大时，IO 操作很耗时。 通常存储于文件服务器，数据库只存储文件地址信息 9. 禁止在线上做数据库压力测试10. 禁止从开发环境,测试环境直接连接生产环境数据库 数据库字段设计规范1. 优先选择符合存储需要的最小的数据类型原因： 列的字段越大，建立索引时所需要的空间也就越大，这样一页中所能存储的索引节点的数量也就越少也越少，在遍历时所需要的 IO 次数也就越多，索引的性能也就越差。 方法： a.将字符串转换成数字类型存储,如:将 IP 地址转换成整形数据 MySQL 提供了两个方法来处理 ip 地址 inet_aton 把 ip 转为无符号整型 (4-8 位) inet_ntoa 把整型的 ip 转为地址 插入数据前，先用 inet_aton 把 ip 地址转为整型，可以节省空间，显示数据时，使用 inet_ntoa 把整型的 ip 地址转为地址显示即可。 b.对于非负型的数据 (如自增 ID,整型 IP) 来说,要优先使用无符号整型来存储 原因： 无符号相对于有符号可以多出一倍的存储空间 12SIGNED INT -2147483648~2147483647UNSIGNED INT 0~4294967295 VARCHAR(N) 中的 N 代表的是字符数，而不是字节数，使用 UTF8 存储 255 个汉字 Varchar(255)=765 个字节。过大的长度会消耗更多的内存。 2. 避免使用 TEXT,BLOB 数据类型，最常见的 TEXT 类型可以存储 64k 的数据a. 建议把 BLOB 或是 TEXT 列分离到单独的扩展表中 MySQL 内存临时表不支持 TEXT、BLOB 这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行。而且对于这种数据，MySQL 还是要进行二次查询，会使 sql 性能变得很差，但是不是说一定不能使用这样的数据类型。 如果一定要使用，建议把 BLOB 或是 TEXT 列分离到单独的扩展表中，查询时一定不要使用 select * 而只需要取出必要的列，不需要 TEXT 列的数据时不要对该列进行查询。 2、TEXT 或 BLOB 类型只能使用前缀索引 因为MySQL 对索引字段长度是有限制的，所以 TEXT 类型只能使用前缀索引，并且 TEXT 列上是不能有默认值的 3. 避免使用 ENUM 类型修改 ENUM 值需要使用 ALTER 语句 ENUM 类型的 ORDER BY 操作效率低，需要额外操作 禁止使用数值作为 ENUM 的枚举值 4. 尽可能把所有列定义为 NOT NULL原因： 索引 NULL 列需要额外的空间来保存，所以要占用更多的空间 进行比较和计算时要对 NULL 值做特别的处理 5. 使用 TIMESTAMP(4 个字节) 或 DATETIME 类型 (8 个字节) 存储时间TIMESTAMP 存储的时间范围 1970-01-01 00:00:01 ~ 2038-01-19-03:14:07 TIMESTAMP 占用 4 字节和 INT 相同，但比 INT 可读性高 超出 TIMESTAMP 取值范围的使用 DATETIME 类型存储 经常会有人用字符串存储日期型的数据（不正确的做法） 缺点 1：无法用日期函数进行计算和比较 缺点 2：用字符串存储日期要占用更多的空间 6. 同财务相关的金额类数据必须使用 decimal 类型 非精准浮点：float,double 精准浮点：decimal Decimal 类型为精准浮点数，在计算时不会丢失精度 占用空间由定义的宽度决定，每 4 个字节可以存储 9 位数字，并且小数点要占用一个字节 可用于存储比 bigint 更大的整型数据 索引设计规范1. 限制每张表上的索引数量,建议单张表索引不超过 5 个索引并不是越多越好！索引可以提高效率同样可以降低效率。 索引可以增加查询效率，但同样也会降低插入和更新的效率，甚至有些情况下会降低查询效率。 因为 MySQL 优化器在选择如何优化查询时，会根据统一信息，对每一个可以用到的索引来进行评估，以生成出一个最好的执行计划，如果同时有很多个索引都可以用于查询，就会增加 MySQL 优化器生成执行计划的时间，同样会降低查询性能。 2. 禁止给表中的每一列都建立单独的索引5.6 版本之前，一个 sql 只能使用到一个表中的一个索引，5.6 以后，虽然有了合并索引的优化方式，但是还是远远没有使用一个联合索引的查询方式好。 3. 每个 Innodb 表必须有个主键Innodb 是一种索引组织表：数据的存储的逻辑顺序和索引的顺序是相同的。每个表都可以有多个索引，但是表的存储顺序只能有一种。 Innodb 是按照主键索引的顺序来组织表的 不要使用更新频繁的列作为主键，不适用多列主键（相当于联合索引） 不要使用 UUID,MD5,HASH,字符串列作为主键（无法保证数据的顺序增长） 主键建议使用自增 ID 值 4. 常见索引列建议 出现在 SELECT、UPDATE、DELETE 语句的 WHERE 从句中的列 包含在 ORDER BY、GROUP BY、DISTINCT 中的字段 并不要将符合 1 和 2 中的字段的列都建立一个索引， 通常将 1、2 中的字段建立联合索引效果更好 多表 join 的关联列 5.如何选择索引列的顺序建立索引的目的是：希望通过索引进行数据查找，减少随机 IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。 区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数） 尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO 性能也就越好） 使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引） 6. 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间） 重复索引示例：primary key(id)、index(id)、unique index(id) 冗余索引示例：index(a,b,c)、index(a,b)、index(a) 7. 对于频繁的查询优先考虑使用覆盖索引 覆盖索引：就是包含了所有查询字段 (where,select,ordery by,group by 包含的字段) 的索引 覆盖索引的好处： 避免 Innodb 表进行索引的二次查询: Innodb 是以聚集索引的顺序来存储的，对于 Innodb 来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了 IO 操作，提升了查询效率。 可以把随机 IO 变成顺序 IO 加快查询效率: 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。 8.索引 SET 规范尽量避免使用外键约束 不建议使用外键约束（foreign key），但一定要在表与表之间的关联键上建立索引 外键可用于保证数据的参照完整性，但建议在业务端实现 外键会影响父表和子表的写操作从而降低性能 数据库 SQL 开发规范1. 建议使用预编译语句进行数据库操作预编译语句可以重复使用这些计划，减少 SQL 编译所需要的时间，还可以解决动态 SQL 所带来的 SQL 注入的问题。 只传参数，比传递 SQL 语句更高效。 相同语句可以一次解析，多次使用，提高处理效率。 2. 避免数据类型的隐式转换隐式转换会导致索引失效如: 1select name,phone from customer where id = &apos;111&apos;; 3. 充分利用表上已经存在的索引避免使用双%号的查询条件。如：a like &#39;%123%&#39;，（如果无前置%,只有后置%，是可以用到列上的索引的） 一个 SQL 只能利用到复合索引中的一列进行范围查询。如：有 a,b,c 列的联合索引，在查询条件中有 a 列的范围查询，则在 b,c 列上的索引将不会被用到。 在定义联合索引时，如果 a 列要用到范围查找的话，就要把 a 列放到联合索引的右侧，使用 left join 或 not exists 来优化 not in 操作，因为 not in 也通常会使用索引失效。 4. 数据库设计时，应该要对以后扩展进行考虑5. 程序连接不同的数据库使用不同的账号，进制跨库查询 为数据库迁移和分库分表留出余地 降低业务耦合度 避免权限过大而产生的安全风险 6. 禁止使用 SELECT * 必须使用 SELECT &lt;字段列表&gt; 查询原因： 消耗更多的 CPU 和 IO 以网络带宽资源 无法使用覆盖索引 可减少表结构变更带来的影响 7. 禁止使用不含字段列表的 INSERT 语句如： 1insert into values (&apos;a&apos;,&apos;b&apos;,&apos;c&apos;); 应使用： 1insert into t(c1,c2,c3) values (&apos;a&apos;,&apos;b&apos;,&apos;c&apos;); 8. 避免使用子查询，可以把子查询优化为 join 操作通常子查询在 in 子句中，且子查询中为简单 SQL(不包含 union、group by、order by、limit 从句) 时,才可以把子查询转化为关联查询进行优化。 子查询性能差的原因： 子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大。 由于子查询会产生大量的临时表也没有索引，所以会消耗过多的 CPU 和 IO 资源，产生大量的慢查询。 9. 避免使用 JOIN 关联太多的表对于 MySQL 来说，是存在关联缓存的，缓存的大小可以由 join_buffer_size 参数进行设置。 在 MySQL 中，对于同一个 SQL 多关联（join）一个表，就会多分配一个关联缓存，如果在一个 SQL 中关联的表越多，所占用的内存也就越大。 如果程序中大量的使用了多表关联的操作，同时 join_buffer_size 设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性。 同时对于关联操作来说，会产生临时表操作，影响查询效率，MySQL 最多允许关联 61 个表，建议不超过 5 个。 10. 减少同数据库的交互次数数据库更适合处理批量操作，合并多个相同的操作到一起，可以提高处理效率。 11. 对应同一列进行 or 判断时，使用 in 代替 orin 的值不要超过 500 个，in 操作可以更有效的利用索引，or 大多数情况下很少能利用到索引。 12. 禁止使用 order by rand() 进行随机排序order by rand() 会把表中所有符合条件的数据装载到内存中，然后在内存中对所有数据根据随机生成的值进行排序，并且可能会对每一行都生成一个随机值，如果满足条件的数据集非常大，就会消耗大量的 CPU 和 IO 及内存资源。 推荐在程序中获取一个随机值，然后从数据库中获取数据的方式。 13. WHERE 从句中禁止对列进行函数转换和计算对列进行函数转换或计算时会导致无法使用索引 不推荐： 1where date(create_time)=&apos;20190101&apos; 推荐： 1where create_time &gt;= &apos;20190101&apos; and create_time &lt; &apos;20190102&apos; 14. 在明显不会有重复值时使用 UNION ALL 而不是 UNION UNION 会把两个结果集的所有数据放到临时表中后再进行去重操作 UNION ALL 不会再对结果集进行去重操作 15. 拆分复杂的大 SQL 为多个小 SQL 大 SQL 逻辑上比较复杂，需要占用大量 CPU 进行计算的 SQL MySQL 中，一个 SQL 只能使用一个 CPU 进行计算 SQL 拆分后可以通过并行执行来提高处理效率 数据库操作行为规范1. 超 100 万行的批量写 (UPDATE,DELETE,INSERT) 操作,要分批多次进行操作大批量操作可能会造成严重的主从延迟 主从环境中,大批量操作可能会造成严重的主从延迟，大批量的写操作一般都需要执行一定长的时间，而只有当主库上执行完成后，才会在其他从库上执行，所以会造成主库与从库长时间的延迟情况 binlog 日志为 row 格式时会产生大量的日志 大批量写操作会产生大量日志，特别是对于 row 格式二进制数据而言，由于在 row 格式中会记录每一行数据的修改，我们一次修改的数据越多，产生的日志量也就会越多，日志的传输和恢复所需要的时间也就越长，这也是造成主从延迟的一个原因 避免产生大事务操作 大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对 MySQL 的性能产生非常大的影响。 特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批 2. 对于大表使用 pt-online-schema-change 修改表结构 避免大表修改产生的主从延迟 避免在对表字段进行修改时进行锁表 对大表数据结构的修改一定要谨慎，会造成严重的锁表操作，尤其是生产环境，是不能容忍的。 pt-online-schema-change 它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器。把原表中新增的数据也复制到新表中，在行所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉。把原来一个 DDL 操作，分解成多个小的批次进行。 3. 禁止为程序使用的账号赋予 super 权限 当达到最大连接数限制时，还运行 1 个有 super 权限的用户连接 super 权限只能留给 DBA 处理问题的账号使用 4. 对于程序连接数据库账号,遵循权限最小原则 程序使用数据库账号只能在一个 DB 下使用，不准跨库 程序使用的账号原则上不准有 drop 权限]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[new-position]]></title>
    <url>%2F2019%2F01%2F25%2Fdiary-2019-01-25-new-position%2F</url>
    <content type="text"><![CDATA[new-position2018年01月25日公司安排了新的title给我。]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>hello</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何解决消息队列的延时以及过期失效问题？]]></title>
    <url>%2F2018%2F12%2F03%2Fsubject-high-concurrency-mq-time-delay-and-expired-failure%2F</url>
    <content type="text"><![CDATA[剖析关于这个事儿，我们一个一个来梳理吧，先假设一个场景，我们现在消费端出故障了，然后大量消息在 mq 里积压，现在出事故了，慌了。 大量消息在 mq 里积压了几个小时了还没解决几千万条数据在 MQ 里积压了七八个小时，从下午 4 点多，积压到了晚上 11 点多。这个是我们真实遇到过的一个场景，确实是线上故障了，这个时候要不然就是修复 consumer 的问题，让它恢复消费速度，然后傻傻的等待几个小时消费完毕。这个肯定不能在面试的时候说吧。 一个消费者一秒是 1000 条，一秒 3 个消费者是 3000 条，一分钟就是 18 万条。所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概 1 小时的时间才能恢复过来。 一般这个时候，只能临时紧急扩容了，具体操作步骤和思路如下： 先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。 新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。 然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的 10 倍数量的 queue。 接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。 等快速消费完积压数据之后，得恢复原先部署的架构，重新用原先的 consumer 机器来消费消息。 mq 中的消息过期失效了假设你用的是 RabbitMQ，RabbtiMQ 是可以设置过期时间的，也就是 TTL。如果消息在 queue 中积压超过一定的时间就会被 RabbitMQ 给清理掉，这个数据就没了。那这就是第二个坑了。这就不是说数据会大量积压在 mq 里，而是大量的数据会直接搞丢。 这个情况下，就不是说要增加 consumer 消费积压的消息，因为实际上没啥积压，而是丢了大量的消息。我们可以采取一个方案，就是批量重导，这个我们之前线上也有类似的场景干过。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上12点以后，用户都睡觉了。这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入 mq 里面去，把白天丢的数据给他补回来。也只能是这样了。 假设 1 万个订单积压在 mq 里面，没有处理，其中 1000 个订单都丢了，你只能手动写程序把那 1000 个订单给查出来，手动发到 mq 里去再补一次。 mq 都快写满了如果消息积压在 mq 里，你很长时间都没有处理掉，此时导致 mq 都快写满了，咋办？这个还有别的办法吗？没有，谁让你第一个方案执行的太慢了，你临时写程序，接入数据来消费，消费一个丢弃一个，都不要了，快速消费掉所有的消息。然后走第二个方案，到了晚上再补数据吧。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列如何进行架构设计？]]></title>
    <url>%2F2018%2F12%2F03%2Fsubject-high-concurrency-mq-design%2F</url>
    <content type="text"><![CDATA[面试题如果让你写一个消息队列，该如何进行架构设计？说一下你的思路。 面试官心理分析其实聊到这个问题，一般面试官要考察两块： 你有没有对某一个消息队列做过较为深入的原理的了解，或者从整体了解把握住一个消息队列的架构原理。 看看你的设计能力，给你一个常见的系统，就是消息队列系统，看看你能不能从全局把握一下整体架构设计，给出一些关键点出来。 说实话，问类似问题的时候，大部分人基本都会蒙，因为平时从来没有思考过类似的问题，大多数人就是平时埋头用，从来不去思考背后的一些东西。类似的问题，比如，如果让你来设计一个 Spring 框架你会怎么做？如果让你来设计一个 Dubbo 框架你会怎么做？如果让你来设计一个 MyBatis 框架你会怎么做？ 面试题剖析其实回答这类问题，说白了，不求你看过那技术的源码，起码你要大概知道那个技术的基本原理、核心组成部分、基本架构构成，然后参照一些开源的技术把一个系统设计出来的思路说一下就好。 比如说这个消息队列系统，我们从以下几个角度来考虑一下： 首先这个 mq 得支持可伸缩性吧，就是需要的时候快速扩容，就可以增加吞吐量和容量，那怎么搞？设计个分布式的系统呗，参照一下 kafka 的设计理念，broker -&gt; topic -&gt; partition，每个 partition 放一个机器，就存一部分数据。如果现在资源不够了，简单啊，给 topic 增加 partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了？ 其次你得考虑一下这个 mq 的数据要不要落地磁盘吧？那肯定要了，落磁盘才能保证别进程挂了数据就丢了。那落磁盘的时候怎么落啊？顺序写，这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的，这就是 kafka 的思路。 其次你考虑一下你的 mq 的可用性啊？这个事儿，具体参考之前可用性那个环节讲解的 kafka 的高可用保障机制。多副本 -&gt; leader &amp; follower -&gt; broker 挂了重新选举 leader 即可对外服务。 能不能支持数据 0 丢失啊？可以的，参考我们之前说的那个 kafka 数据零丢失方案。 mq 肯定是很复杂的，面试官问你这个问题，其实是个开放题，他就是看看你有没有从架构角度整体构思和设计的思维以及能力。确实这个问题可以刷掉一大批人，因为大部分人平时不思考这些东西。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[annotation-in-spring]]></title>
    <url>%2F2018%2F11%2F26%2Flang-java-spring-annotation-in-spring%2F</url>
    <content type="text"><![CDATA[@Configuration把一个类作为一个IoC容器，它的某个方法头上如果注册了@Bean，就会作为这个Spring容器中的Bean。 @Scope注解 作用域 @Lazy(true) 表示延迟初始化 @Service用于标注业务层组件、 @Controller用于标注控制层组件@Repository用于标注数据访问组件，即DAO组件。 @Component泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。 @Scope用于指定scope作用域的（用在类上） @PostConstruct用于指定初始化方法（用在方法上） @PreDestory用于指定销毁方法（用在方法上） @DependsOn：定义Bean初始化及销毁时的顺序 @Primary：自动装配时当出现多个Bean候选者时，被注解为@Primary的Bean将作为首选者，否则将抛出异常 @Autowired 默认按类型装配，如果我们想使用按名称装配，可以结合@Qualifier注解一起使用。如下： @Autowired @Qualifier(“personDaoBean”) 存在多个实例配合使用 @Resource默认按名称装配，当找不到与名称匹配的bean才会按类型装配。 @PostConstruct 初始化注解 @PreDestroy 摧毁注解 默认 单例 启动就加载 Spring中的这几个注解有什么区别：@Component 、@Repository、@Service、@Controller @Component指的是组件， @Controller，@Repository和@Service 注解都被@Component修饰，用于代码中区分表现层，持久层和业务层的组件，代码中组件不好归类时可以使用@Component来标注 当前版本只有区分的作用，未来版本可能会添加更丰富的功能]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[国内互联网公司的开源项目及Github地址汇总]]></title>
    <url>%2F2018%2F11%2F09%2Fmgr-Github-Oraganizations%2F</url>
    <content type="text"><![CDATA[Alibaba 阿里巴巴 Ant Design Ant Design Ant DataVis Team 蚂蚁金服 - 体验技术部 - 数据图形组 NG-ZORRO Angular Developers aliceui 支付宝 issyteam kissyteam Taobao, Inc. 淘宝 天猫前端 天猫 seajs Web的模块加载程序 kissyteam 强大的模块集合 egg Node.js的Web框架框架 Huawei 华为 Tencent 腾讯 Github Tencent 腾讯 csdn 腾讯 AlloyTeam 腾讯 Baidu 百度 Baidu FEX team 百度 Baidu EFE team 百度 Baidu BEFE 百度企业产品前端研发团队 eleme 饿了么 饿了么前端 饿了么 NetEase 网易 SOHUDBA 搜狐 Qihoo360 奇虎360 360EntSecGroup-Skylar 360企业安全集团终端安全子公司 唯品会 唯品会 Douban Inc. 豆瓣 大众点评网 大众点评网 美团点评 美团点评 Xiaomi Open Source. 小米 meituan.com 美团 蘑菇街 蘑菇街 CodisLabs 豌豆荚 当当 当当 有赞 有赞 TeamStuQ TeamStuQ 稀土 稀土 伯乐在线 伯乐在线 深度 Wuhan Deepin Technology Co.,Ltd. DNSPod DNSPod Weibo R&amp;D Open Source Projects 新浪]]></content>
      <categories>
        <category>mgr</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何保证消息的顺序性？]]></title>
    <url>%2F2018%2F11%2F03%2Fsubject-high-concurrency-how-to-ensure-the-order-of-messages%2F</url>
    <content type="text"><![CDATA[剖析我举个例子，我们以前做过一个 mysql binlog 同步的系统，压力还是非常大的，日同步数据要达到上亿，就是说数据从一个 mysql 库原封不动地同步到另一个 mysql 库里面去（mysql -&gt; mysql）。常见的一点在于说比如大数据 team，就需要同步一个 mysql 库过来，对公司的业务系统的数据做各种复杂的操作。 你在 mysql 里增删改一条数据，对应出来了增删改 3 条 binlog 日志，接着这三条 binlog 发送到 MQ 里面，再消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你楞是换了顺序给执行成删除、修改、增加，不全错了么。 本来这个数据同步过来，应该最后这个数据被删除了；结果你搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。 先看看顺序会错乱的俩场景： RabbitMQ：一个 queue，多个 consumer。比如，生产者向 RabbitMQ 里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ 中消费这三条数据中的一条，结果消费者2先执行完操作，把 data2 存入数据库，然后是 data1/data3。这不明显乱了。 Kafka：比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。接着，我们在消费者里可能会搞多个线程来并发处理消息。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。 解决方案RabbitMQ拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。 Kafka 一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何保证消息的可靠性传输？]]></title>
    <url>%2F2018%2F11%2F03%2Fsubject-high-concurrency-how-to-ensure-the-reliable-transmission-of-messages%2F</url>
    <content type="text"><![CDATA[剖析数据的丢失问题，可能出现在生产者、MQ、消费者中，咱们从 RabbitMQ 和 Kafka 分别来分析一下吧。 RabbitMQ 生产者弄丢了数据生产者将数据发送到 RabbitMQ 的时候，可能数据就在半路给搞丢了，因为网络问题啥的，都有可能。 此时可以选择用 RabbitMQ 提供的事务功能，就是生产者发送数据之前开启 RabbitMQ 事务channel.txSelect，然后发送消息，如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务channel.txRollback，然后重试发送消息；如果收到了消息，那么可以提交事务channel.txCommit。 123456789101112// 开启事务channel.txSelecttry &#123; // 这里发送消息&#125; catch (Exception e) &#123; channel.txRollback // 这里再次重发这条消息&#125;// 提交事务channel.txCommit 但是问题是，RabbitMQ 事务机制（同步）一搞，基本上吞吐量会下来，因为太耗性能。 所以一般来说，如果你要确保说写 RabbitMQ 的消息别丢，可以开启 confirm 模式，在生产者那里设置开启 confirm 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个 ack 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 nack 接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。 事务机制和 confirm 机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是 confirm 机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息 RabbitMQ 接收了之后会异步回调你的一个接口通知你这个消息接收到了。 所以一般在生产者这块避免数据丢失，都是用 confirm 机制的。 RabbitMQ 弄丢了数据就是 RabbitMQ 自己弄丢了数据，这个你必须开启 RabbitMQ 的持久化，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，可能导致少量数据丢失，但是这个概率较小。 设置持久化有两个步骤： 创建 queue 的时候将其设置为持久化这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是它是不会持久化 queue 里的数据的。 第二个是发送消息的时候将消息的 deliveryMode 设置为 2就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。 必须要同时设置这两个持久化才行，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。 注意，哪怕是你给 RabbitMQ 开启了持久化机制，也有一种可能，就是这个消息写到了 RabbitMQ 中，但是还没来得及持久化到磁盘上，结果不巧，此时 RabbitMQ 挂了，就会导致内存里的一点点数据丢失。 所以，持久化可以跟生产者那边的 confirm 机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者 ack 了，所以哪怕是在持久化到磁盘之前，RabbitMQ 挂了，数据丢了，生产者收不到 ack，你也是可以自己重发的。 消费端弄丢了数据RabbitMQ 如果丢失了数据，主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，RabbitMQ 认为你都消费了，这数据就丢了。 这个时候得用 RabbitMQ 提供的 ack 机制，简单来说，就是你必须关闭 RabbitMQ 的自动 ack，可以通过一个 api 来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里 ack 一把。这样的话，如果你还没处理完，不就没有 ack 了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。 Kafka消费端弄丢了数据唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边自动提交了 offset，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。 这不是跟 RabbitMQ 差不多吗，大家都知道 Kafka 会自动提交 offset，那么只要关闭自动提交 offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是可能会有重复消费，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。 生产环境碰到的一个问题，就是说我们的 Kafka 消费者消费到了数据之后是写到一个内存的 queue 里先缓冲一下，结果有的时候，你刚把消息写入内存 queue，然后消费者会自动提交 offset。然后此时我们重启了系统，就会导致内存 queue 里还没来得及处理的数据就丢失了。 Kafka 弄丢了数据这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。 生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。 所以此时一般是要求起码设置如下 4 个参数： 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。 在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。 在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。 我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。 生产者会不会弄丢数据？如果按照上述的思路设置了 acks=all，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何保证消息不被重复消费？]]></title>
    <url>%2F2018%2F11%2F03%2Fsubject-high-concurrency-how-to-ensure-that-messages-are-not-repeatedly-consumed%2F</url>
    <content type="text"><![CDATA[剖析回答这个问题，首先你别听到重复消息这个事儿，就一无所知吧，你先大概说一说可能会有哪些重复消费的问题。 首先，比如 RabbitMQ、RocketMQ、Kafka，都有可能会出现消息重复消费的问题，正常。因为这问题通常不是 MQ 自己保证的，是由我们开发来保证的。挑一个 Kafka 来举个例子，说说怎么重复消费吧。 Kafka 实际上有个 offset 的概念，就是每个消息写进去，都有一个 offset，代表消息的序号，然后 consumer 消费了数据之后，每隔一段时间（定时定期），会把自己消费过的消息的 offset 提交一下，表示“我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的 offset 来继续消费吧”。 但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接 kill 进程了，再重启。这会导致 consumer 有些消息处理了，但是没来得及提交 offset，尴尬了。重启之后，少数消息会再次消费一次。 举个栗子。 有这么个场景。数据 1/2/3 依次进入 kafka，kafka 会给这三条数据每条分配一个 offset，代表这条数据的序号，我们就假设分配的 offset 依次是 152/153/154。消费者从 kafka 去消费的时候，也是按照这个顺序去消费。假如当消费者消费了 offset=153 的这条数据，刚准备去提交 offset 到 zookeeper，此时消费者进程被重启了。那么此时消费过的数据 1/2 的 offset 并没有提交，kafka 也就不知道你已经消费了 offset=153 这条数据。那么重启之后，消费者会找 kafka 说，嘿，哥儿们，你给我接着把上次我消费到的那个地方后面的数据继续给我传递过来。由于之前的 offset 没有提交成功，那么数据 1/2 会再次传过来，如果此时消费者没有去重的话，那么就会导致重复消费。 如果消费者干的事儿是拿一条数据就往数据库里写一条，会导致说，你可能就把数据 1/2 在数据库里插入了 2 次，那么数据就错啦。 其实重复消费不可怕，可怕的是你没考虑到重复消费之后，怎么保证幂等性。 举个例子吧。假设你有个系统，消费一条消息就往数据库里插入一条数据，要是你一个消息重复两次，你不就插入了两条，这数据不就错了？但是你要是消费到第二次的时候，自己判断一下是否已经消费过了，若是就直接扔了，这样不就保留了一条数据，从而保证了数据的正确性。 一条数据重复出现两次，数据库里就只有一条数据，这就保证了系统的幂等性。 幂等性，通俗点说，就一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，不能出错。 所以第二个问题来了，怎么保证消息队列消费的幂等性？ 其实还是得结合业务来思考，我这里给几个思路： 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。 当然，如何保证 MQ 的消费是幂等性的，需要结合具体的业务来看。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何保证 redis 的高并发和高可用？]]></title>
    <url>%2F2018%2F11%2F03%2Fsubject-high-concurrency-how-to-ensure-high-concurrency-and-high-availability-of-redis%2F</url>
    <content type="text"><![CDATA[如何保证 redis 的高并发和高可用？redis 的主从复制原理能介绍一下么？redis 的哨兵原理能介绍一下么？ 剖析如果你用 redis 缓存技术的话，肯定要考虑如何用 redis 来加多台机器，保证 redis 是高并发的，还有就是如何让 redis 保证自己不是挂掉以后就直接死掉了，即 redis 高可用。 由于此节内容较多，因此，会分为两个小节进行讲解。 redis 主从架构 redis 基于哨兵实现高可用 redis 实现高并发主要依靠主从架构，一主多从，一般来说，很多项目其实就足够了，单主用来写入数据，单机几万 QPS，多从用来查询数据，多个从实例可以提供每秒 10w 的 QPS。 如果想要在实现高并发的同时，容纳大量的数据，那么就需要 redis 集群，使用 redis 集群之后，可以提供每秒几十万的读写并发。 redis 高可用，如果是做主从架构部署，那么加上哨兵就可以了，就可以实现，任何一个实例宕机，可以进行主备切换。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何保证消息队列的高可用？]]></title>
    <url>%2F2018%2F11%2F03%2Fsubject-high-concurrency-how-to-ensure-high-availability-of-message-queues%2F</url>
    <content type="text"><![CDATA[剖析这个问题这么问是很好的，因为不能问你 Kafka 的高可用性怎么保证？ActiveMQ 的高可用性怎么保证？一个面试官要是这么问就显得很没水平，人家可能用的就是 RabbitMQ，没用过 Kafka，你上来问人家 Kafka 干什么？这不是摆明了刁难人么。 所以有水平的面试官，问的是 MQ 的高可用性怎么保证？这样就是你用过哪个 MQ，你就说说你对那个 MQ 的高可用性的理解。 RabbitMQ 的高可用性RabbitMQ 是比较有代表性的，因为是基于主从（非分布式）做高可用性的，我们就以 RabbitMQ 为例子讲解第一种 MQ 的高可用性怎么实现。 RabbitMQ 有三种模式：单机模式、普通集群模式、镜像集群模式。 单机模式单机模式，就是 Demo 级别的，一般就是你本地启动了玩玩儿的😄，没人生产用单机模式。 普通集群模式（无高可用性）普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。你创建的 queue，只会放在一个 RabbitMQ 实例上，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。 这种方式确实很麻烦，也不怎么好，没做到所谓的分布式，就是个普通集群。因为这导致你要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个 queue 所在实例消费数据，前者有数据拉取的开销，后者导致单实例性能瓶颈。 而且如果那个放 queue 的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果你开启了消息持久化，让 RabbitMQ 落地存储消息的话，消息不一定会丢，得等这个实例恢复了，然后才可以继续从这个 queue 拉取数据。 所以这个事儿就比较尴尬了，这就没有什么所谓的高可用性，这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个 queue 的读写操作。 镜像集群模式（高可用性）这种模式，才是所谓的 RabbitMQ 的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会存在于多个实例上，就是说，每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把消息同步到多个实例的 queue 上。 那么如何开启这个镜像集群模式呢？其实很简单，RabbitMQ 有很好的管理控制台，就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建 queue 的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。 这样的话，好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！第二，这么玩儿，不是分布式的，就没有扩展性可言了，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并没有办法线性扩展你的 queue。你想，如果这个 queue 的数据量很大，大到这个机器上的容量无法容纳了，此时该怎么办呢？ Kafka 的高可用性Kafka 一个最基本的架构认识：由多个 broker 组成，每个 broker 是一个节点；你创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据。 这就是天然的分布式消息队列，就是说一个 topic 的数据，是分散放在多个机器上的，每个机器就放一部分数据。 实际上 RabbmitMQ 之类的，并不是分布式消息队列，它就是传统的消息队列，只不过提供了一些集群、HA(High Availability, 高可用性) 的机制而已，因为无论怎么玩儿，RabbitMQ 一个 queue 的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个 queue 的完整数据。 Kafka 0.8 以前，是没有 HA 机制的，就是任何一个 broker 宕机了，那个 broker 上的 partition 就废了，没法写也没法读，没有什么高可用性可言。 比如说，我们假设创建了一个 topic，指定其 partition 数量是 3 个，分别在三台机器上。但是，如果第二台机器宕机了，会导致这个 topic 的 1/3 的数据就丢了，因此这个是做不到高可用的。 Kafka 0.8 以后，提供了 HA 机制，就是 replica（复制品） 副本机制。每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower。写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。只能读写 leader？很简单，要是你可以随意读写每个 follower，那么就要 care 数据一致性的问题，系统复杂度太高，很容易出问题。Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性。 这么搞，就有所谓的高可用性了，因为如果某个 broker 宕机了，没事儿，那个 broker上面的 partition 在其他机器上都有副本的，如果这上面有某个 partition 的 leader，那么此时会从 follower 中重新选举一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了。 写数据的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader，leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为） 消费的时候，只会从 leader 去读，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。 看到这里，相信你大致明白了 Kafka 是如何保证高可用机制的了，对吧？不至于一无所知，现场还能给面试官画画图。要是遇上面试官确实是 Kafka 高手，深挖了问，那你只能说不好意思，太深入的你没研究过。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何设计一个高并发系统？]]></title>
    <url>%2F2018%2F11%2F03%2Fsubject-high-concurrency-high-concurrency-design%2F</url>
    <content type="text"><![CDATA[分析说实话，如果面试官问你这个题目，那么你必须要使出全身吃奶劲了。为啥？因为你没看到现在很多公司招聘的 JD 里都是说啥，有高并发就经验者优先。 如果你确实有真才实学，在互联网公司里干过高并发系统，那你确实拿 offer 基本如探囊取物，没啥问题。面试官也绝对不会这样来问你，否则他就是蠢。 假设你在某知名电商公司干过高并发系统，用户上亿，一天流量几十亿，高峰期并发量上万，甚至是十万。那么人家一定会仔细盘问你的系统架构，你们系统啥架构？怎么部署的？部署了多少台机器？缓存咋用的？MQ 咋用的？数据库咋用的？就是深挖你到底是如何扛住高并发的。 因为真正干过高并发的人一定知道，脱离了业务的系统架构都是在纸上谈兵，真正在复杂业务场景而且还高并发的时候，那系统架构一定不是那么简单的，用个 redis，用 mq 就能搞定？当然不是，真实的系统架构搭配上业务之后，会比这种简单的所谓“高并发架构”要复杂很多倍。 如果有面试官问你个问题说，如何设计一个高并发系统？那么不好意思，一定是因为你实际上没干过高并发系统。面试官看你简历就没啥出彩的，感觉就不咋地，所以就会问问你，如何设计一个高并发系统？其实说白了本质就是看看你有没有自己研究过，有没有一定的知识积累。 最好的当然是招聘个真正干过高并发的哥儿们咯，但是这种哥儿们人数稀缺，不好招。所以可能次一点的就是招一个自己研究过的哥儿们，总比招一个啥也不会的哥儿们好吧！ 所以这个时候你必须得做一把个人秀了，秀出你所有关于高并发的知识！ 面试题剖析其实所谓的高并发，如果你要理解这个问题呢，其实就得从高并发的根源出发，为啥会有高并发？为啥高并发就很牛逼？ 我说的浅显一点，很简单，就是因为刚开始系统都是连接数据库的，但是要知道数据库支撑到每秒并发两三千的时候，基本就快完了。所以才有说，很多公司，刚开始干的时候，技术比较 low，结果业务发展太快，有的时候系统扛不住压力就挂了。 当然会挂了，凭什么不挂？你数据库如果瞬间承载每秒 5000/8000，甚至上万的并发，一定会宕机，因为比如 mysql 就压根儿扛不住这么高的并发量。 所以为啥高并发牛逼？就是因为现在用互联网的人越来越多，很多 app、网站、系统承载的都是高并发请求，可能高峰期每秒并发量几千，很正常的。如果是什么双十一之类的，每秒并发几万几十万都有可能。 那么如此之高的并发量，加上原本就如此之复杂的业务，咋玩儿？真正厉害的，一定是在复杂业务系统里玩儿过高并发架构的人，但是你没有，那么我给你说一下你该怎么回答这个问题： 可以分为以下 6 点： 系统拆分 缓存 MQ 分库分表 读写分离 ElasticSearch 系统拆分将一个系统拆分为多个子系统，用 dubbo 来搞。然后每个系统连一个数据库，这样本来就一个库，现在多个数据库，不也可以扛高并发么。 缓存缓存，必须得用缓存。大部分的高并发场景，都是读多写少，那你完全可以在数据库和缓存里都写一份，然后读的时候大量走缓存不就得了。毕竟人家 redis 轻轻松松单机几万的并发。所以你可以考虑考虑你的项目里，那些承载主要请求的读场景，怎么用缓存来抗高并发。 MQMQ，必须得用 MQ。可能你还是会出现高并发写的场景，比如说一个业务操作里要频繁搞数据库几十次，增删改增删改，疯了。那高并发绝对搞挂你的系统，你要是用 redis 来承载写那肯定不行，人家是缓存，数据随时就被 LRU 了，数据格式还无比简单，没有事务支持。所以该用 mysql 还得用 mysql 啊。那你咋办？用 MQ 吧，大量的写请求灌入 MQ 里，排队慢慢玩儿，后边系统消费后慢慢写，控制在 mysql 承载范围之内。所以你得考虑考虑你的项目里，那些承载复杂写业务逻辑的场景里，如何用 MQ 来异步写，提升并发性。MQ 单机抗几万并发也是 ok 的，这个之前还特意说过。 分库分表分库分表，可能到了最后数据库层面还是免不了抗高并发的要求，好吧，那么就将一个数据库拆分为多个库，多个库来扛更高的并发；然后将一个表拆分为多个表，每个表的数据量保持少一点，提高 sql 跑的性能。 读写分离读写分离，这个就是说大部分时候数据库可能也是读多写少，没必要所有请求都集中在一个库上吧，可以搞个主从架构，主库写入，从库读取，搞一个读写分离。读流量太多的时候，还可以加更多的从库。 ElasticSearchElasticsearch，简称 es。es 是分布式的，可以随便扩容，分布式天然就可以支撑高并发，因为动不动就可以扩容加机器来扛更高的并发。那么一些比较简单的查询、统计类的操作，可以考虑用 es 来承载，还有一些全文搜索类的操作，也可以考虑用 es 来承载。 上面的 6 点，基本就是高并发系统肯定要干的一些事儿，大家可以仔细结合之前讲过的知识考虑一下，到时候你可以系统的把这块阐述一下，然后每个部分要注意哪些问题，之前都讲过了，你都可以阐述阐述，表明你对这块是有点积累的。 说句实话，毕竟你真正厉害的一点，不是在于弄明白一些技术，或者大概知道一个高并发系统应该长什么样？其实实际上在真正的复杂的业务系统里，做高并发要远远比上面提到的点要复杂几十倍到上百倍。你需要考虑：哪些需要分库分表，哪些不需要分库分表，单库单表跟分库分表如何 join，哪些数据要放到缓存里去，放哪些数据才可以扛住高并发的请求，你需要完成对一个复杂业务系统的分析之后，然后逐步逐步的加入高并发的系统架构的改造，这个过程是无比复杂的，一旦做过一次，并且做好了，你在这个市场上就会非常的吃香。 其实大部分公司，真正看重的，不是说你掌握高并发相关的一些基本的架构知识，架构中的一些技术，RocketMQ、Kafka、Redis、Elasticsearch，高并发这一块，你了解了，也只能是次一等的人才。对一个有几十万行代码的复杂的分布式系统，一步一步架构、设计以及实践过高并发架构的人，这个经验是难能可贵的。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>design</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C# 2 JAVA]]></title>
    <url>%2F2018%2F10%2F16%2Fdiary-2018-10-16-java%2F</url>
    <content type="text"><![CDATA[整理上从C# 全部转向 JAVA2018年10月16日 17:25:55]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>hello</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[58到家数据库30条军规解读]]></title>
    <url>%2F2018%2F09%2F27%2Fdb-mysql-58%E5%88%B0%E5%AE%B6MySQL%E5%86%9B%E8%A7%84%E5%8D%87%E7%BA%A7%E7%89%88%2F</url>
    <content type="text"><![CDATA[一、基础规范 表存储引擎必须使用InnoDB 表字符集默认使用utf8，必要时候使用utf8mb4 解读：（1）通用，无乱码风险，汉字3字节，英文1字节（2）utf8mb4是utf8的超集，有存储4字节例如表情符号时，使用它 禁止使用存储过程，视图，触发器，Event 解读：（1）对数据库性能影响较大，互联网业务，能让站点层和服务层干的事情，不要交到数据库层（2）调试，排错，迁移都比较困难，扩展性较差 禁止在数据库中存储大文件，例如照片，可以将大文件存储在对象存储系统，数据库中存储路径 禁止在线上环境做数据库压力测试 测试，开发，线上数据库环境必须隔离 二、命名规范 库名，表名，列名必须用小写，采用下划线分隔 解读：abc，Abc，ABC都是给自己埋坑 库名，表名，列名必须见名知义，长度不要超过32字符 解读：tmp，wushan谁TM知道这些库是干嘛的 库备份必须以bak为前缀，以日期为后缀 从库必须以-s为后缀 备库必须以-ss为后缀 三、表设计规范 单实例表个数必须控制在2000个以内 单表分表个数必须控制在1024个以内 表必须有主键，推荐使用UNSIGNED整数为主键 潜在坑：删除无主键的表，如果是row模式的主从架构，从库会挂住 禁止使用外键，如果要保证完整性，应由应用程式实现 解读：外键使得表之间相互耦合，影响update/delete等SQL性能，有可能造成死锁，高并发情况下容易成为数据库瓶颈 建议将大字段，访问频度低的字段拆分到单独的表中存储，分离冷热数据 解读：具体参加《如何实施数据库垂直拆分》 四、列设计规范 根据业务区分使用tinyint/int/bigint，分别会占用1/4/8字节 根据业务区分使用char/varchar 解读： （1）字段长度固定，或者长度近似的业务场景，适合使用char，能够减少碎片，查询性能高 （2）字段长度相差较大，或者更新较少的业务场景，适合使用varchar，能够减少空间 根据业务区分使用datetime/timestamp 解读：前者占用5个字节，后者占用4个字节，存储年使用YEAR，存储日期使用DATE，存储时间使用datetime 必须把字段定义为NOT NULL并设默认值 解读：（1）NULL的列使用索引，索引统计，值都更加复杂，MySQL更难优化（2）NULL需要更多的存储空间（3）NULL只能采用IS NULL或者IS NOT NULL，而在=/!=/in/not in时有大坑 使用INT UNSIGNED存储IPv4，不要用char(15) 使用varchar(20)存储手机号，不要使用整数 解读：（1）牵扯到国家代号，可能出现+/-/()等字符，例如+86（2）手机号不会用来做数学运算（3）varchar可以模糊查询，例如like ‘138%’ 使用TINYINT来代替ENUM 解读：ENUM增加新值要进行DDL操作 五、索引规范 唯一索引使用uniq_[字段名]来命名 非唯一索引使用idx_[字段名]来命名 单张表索引数量建议控制在5个以内 解读：（1）互联网高并发业务，太多索引会影响写性能（2）生成执行计划时，如果索引太多，会降低性能，并可能导致MySQL选择不到最优索引（3）异常复杂的查询需求，可以选择ES等更为适合的方式存储 组合索引字段数不建议超过5个 解读：如果5个字段还不能极大缩小row范围，八成是设计有问题 不建议在频繁更新的字段上建立索引 非必要不要进行JOIN查询，如果要进行JOIN查询，被JOIN的字段必须类型相同，并建立索引 解读：踩过因为JOIN字段类型不一致，而导致全表扫描的坑么？ 理解组合索引最左前缀原则，避免重复建设索引，如果建立了(a,b,c)，相当于建立了(a), (a,b), (a,b,c) 六、SQL规范 禁止使用select *，只获取必要字段 解读：（1）select *会增加cpu/io/内存/带宽的消耗（2）指定字段能有效利用索引覆盖（3）指定字段查询，在表结构变更时，能保证对应用程序无影响 insert必须指定字段，禁止使用insert into T values() 解读：指定字段插入，在表结构变更时，能保证对应用程序无影响 隐式类型转换会使索引失效，导致全表扫描 禁止在where条件列使用函数或者表达式 解读：导致不能命中索引，全表扫描 禁止负向查询以及%开头的模糊查询 解读：导致不能命中索引，全表扫描 禁止大表JOIN和子查询 同一个字段上的OR必须改写问IN，IN的值必须少于50个 应用程序必须捕获SQL异常 解读：方便定位线上问题 说明：本军规适用于并发量大，数据量大的典型互联网业务，可直接带走参考，不谢。]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[58到家数据库30条军规解读]]></title>
    <url>%2F2018%2F09%2F27%2Fdb-mysql-58%E5%88%B0%E5%AE%B6%E6%95%B0%E6%8D%AE%E5%BA%9330%E6%9D%A1%E5%86%9B%E8%A7%84%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[军规适用场景：并发量大、数据量大的互联网业务军规：介绍内容解读：讲解原因，解读比军规更重要 一、基础规范（1）必须使用InnoDB存储引擎解读：支持事务、行级锁、并发性能更好、CPU及内存缓存页优化使得资源利用率更高 （2）必须使用UTF8字符集解读：万国码，无需转码，无乱码风险，节省空间 （3）数据表、数据字段必须加入中文注释解读：N年后谁tm知道这个r1,r2,r3字段是干嘛的 （4）禁止使用存储过程、视图、触发器、Event解读：高并发大数据的互联网业务，架构设计思路是“解放数据库CPU，将计算转移到服务层”，并发量大的情况下，这些功能很可能将数据库拖死，业务逻辑放到服务层具备更好的扩展性，能够轻易实现“增机器就加性能”。数据库擅长存储与索引，CPU计算还是上移吧 （5）禁止存储大文件或者大照片解读：为何要让数据库做它不擅长的事情？大文件和照片存储在文件系统，数据库里存URI多好 二、命名规范（6）只允许使用内网域名，而不是ip连接数据库 （7）线上环境、开发环境、测试环境数据库内网域名遵循命名规范业务名称：xxx线上环境：dj.xxx.db开发环境：dj.xxx.rdb测试环境：dj.xxx.tdb从库在名称后加-s标识，备库在名称后加-ss标识线上从库：dj.xxx-s.db线上备库：dj.xxx-sss.db （8）库名、表名、字段名：小写，下划线风格，不超过32个字符，必须见名知意，禁止拼音英文混用 （9）表名t_xxx，非唯一索引名idx_xxx，唯一索引名uniq_xxx 三、表设计规范（10）单实例表数目必须小于500 （11）单表列数目必须小于30 （12）表必须有主键，例如自增主键解读：a）主键递增，数据行写入可以提高插入性能，可以避免page分裂，减少表碎片提升空间和内存的使用b）主键要选择较短的数据类型， Innodb引擎普通索引都会保存主键的值，较短的数据类型可以有效的减少索引的磁盘空间，提高索引的缓存效率c） 无主键的表删除，在row模式的主从架构，会导致备库夯住 （13）禁止使用外键，如果有外键完整性约束，需要应用程序控制解读：外键会导致表与表之间耦合，update与delete操作都会涉及相关联的表，十分影响sql 的性能，甚至会造成死锁。高并发情况下容易造成数据库性能，大数据高并发业务场景数据库使用以性能优先 四、字段设计规范（14）必须把字段定义为NOT NULL并且提供默认值解读：a）null的列使索引/索引统计/值比较都更加复杂，对MySQL来说更难优化b）null 这种类型MySQL内部需要进行特殊处理，增加数据库处理记录的复杂性；同等条件下，表中有较多空字段的时候，数据库的处理性能会降低很多c）null值需要更多的存储空，无论是表还是索引中每行中的null的列都需要额外的空间来标识d）对null 的处理时候，只能采用is null或is not null，而不能采用=、in、&lt;、&lt;&gt;、!=、not in这些操作符号。如：where name!=’shenjian’，如果存在name为null值的记录，查询结果就不会包含name为null值的记录 （15）禁止使用TEXT、BLOB类型解读：会浪费更多的磁盘和内存空间，非必要的大量的大字段查询会淘汰掉热数据，导致内存命中率急剧降低，影响数据库性能 （16）禁止使用小数存储货币解读：使用整数吧，小数容易导致钱对不上 （17）必须使用varchar(20)存储手机号解读：a）涉及到区号或者国家代号，可能出现+-()b）手机号会去做数学运算么？c）varchar可以支持模糊查询，例如：like“138%” （18）禁止使用ENUM，可使用TINYINT代替解读：a）增加新的ENUM值要做DDL操作b）ENUM的内部实际存储就是整数，你以为自己定义的是字符串？ 五、索引设计规范（19）单表索引建议控制在5个以内 （20）单索引字段数不允许超过5个解读：字段超过5个时，实际已经起不到有效过滤数据的作用了 （21）禁止在更新十分频繁、区分度不高的属性上建立索引解读：a）更新会变更B+树，更新频繁的字段建立索引会大大降低数据库性能b）“性别”这种区分度不大的属性，建立索引是没有什么意义的，不能有效过滤数据，性能与全表扫描类似 （22）建立组合索引，必须把区分度高的字段放在前面解读：能够更加有效的过滤数据 六、SQL使用规范（23）禁止使用SELECT *，只获取必要的字段，需要显示说明列属性解读：a）读取不需要的列会增加CPU、IO、NET消耗b）不能有效的利用覆盖索引c）使用SELECT *容易在增加或者删除字段后出现程序BUG （24）禁止使用INSERT INTO t_xxx VALUES(xxx)，必须显示指定插入的列属性解读：容易在增加或者删除字段后出现程序BUG （25）禁止使用属性隐式转换解读：SELECT uid FROM t_user WHERE phone=13812345678 会导致全表扫描，而不能命中phone索引，猜猜为什么？（这个线上问题不止出现过一次） （26）禁止在WHERE条件的属性上使用函数或者表达式解读：SELECT uid FROM t_user WHERE from_unixtime(day)&gt;=’2017-02-15’ 会导致全表扫描正确的写法是：SELECT uid FROM t_user WHERE day&gt;= unix_timestamp(‘2017-02-15 00:00:00’) （27）禁止负向查询，以及%开头的模糊查询解读：a）负向查询条件：NOT、!=、&lt;&gt;、!&lt;、!&gt;、NOT IN、NOT LIKE等，会导致全表扫描b）%开头的模糊查询，会导致全表扫描 （28）禁止大表使用JOIN查询，禁止大表使用子查询解读：会产生临时表，消耗较多内存与CPU，极大影响数据库性能 （29）禁止使用OR条件，必须改为IN查询解读：旧版本Mysql的OR查询是不能命中索引的，即使能命中索引，为何要让数据库耗费更多的CPU帮助实施查询优化呢？ （30）应用程序必须捕获SQL异常，并有相应处理 总结：大数据量高并发的互联网业务，极大影响数据库性能的都不让用，不让用哟。==【完】==]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[再议数据库军规]]></title>
    <url>%2F2018%2F09%2F27%2Fdb-mysql-%E5%86%8D%E8%AE%AE%E6%95%B0%E6%8D%AE%E5%BA%93%E5%86%9B%E8%A7%84%2F</url>
    <content type="text"><![CDATA[军规：必须使用UTF8字符集和DBA负责人确认后，纠正为“新库默认使用utf8mb4字符集”。这点感谢网友的提醒，utf8mb4是utf8的超集，emoji表情以及部分不常见汉字在utf8下会表现为乱码，故需要升级至utf8mb4。默认使用这个字符集的原因是：“标准，万国码，无需转码，无乱码风险”，并不“节省空间”。 一个潜在坑：阿里云上RDS服务如果要从utf8升级为utf8mb4，需要重启实例，所以58到家并没有把所有的数据库升级成这个字符集，而是“新库默认使用utf8mb4字符集”。 自搭的Mysql可以完成在线转换，而不需要重启数据库实例。 军规：数据表、数据字段必须加入中文注释这一点应该没有疑问。不过也有朋友提出，加入注释会方便黑客，建议“注释写在文档里，文档和数据库同步更新”。这个建议根据经验来说是不太靠谱的：（1）不能怕bug就不写代码，怕黑客就不写注释，对吧？（2）文档同步更新也不太现实，还是把注释写好，代码可读性做好更可行，互联网公司的文档管理？呆过互联网公司的同学估计都清楚。 军规：禁止使用存储过程、视图、触发器、Event 军规：禁止使用外键，如果有外键完整性约束，需要应用程序控制 军规：禁止大表使用JOIN查询，禁止大表使用子查询很多网友提出，这些军规不合理，完全做到不可能。 如原文所述，58到家数据库30条军规的背景是“并发量大、数据量大的互联网业务”，这类业务架构设计的重点往往是吞吐量，性能优先（和钱相关的少部分业务是一致性优先），对数据库性能影响较大的数据库特性较少使用。这类场景的架构方向是“解放数据库CPU，把复杂逻辑计算放到服务层”，服务层具备更好的扩展性，容易实现“增机器就扩充性能”，数据库擅长存储与索引，勿让数据库背负过重的任务。 关于这个点，再有较真的柳岩小编就不回复了哈，任何事情都没有百分之百，但58到家的数据库使用确实没有存储过程、视图、触发器、外键、用户自定义函数，针对业务特性设计架构，等单库吞吐量到了几千上万，就明白这些军规的重要性啦。 军规：只允许使用内网域名，而不是ip连接数据库这一点应该也没有疑问。不只是数据库，缓存（memcache、redis）的连接，服务（service）的连接都必须使用内网域名，机器迁移/平滑升级/运维管理…太多太多的好处，如果朋友你还是采用ip直连的，赶紧升级到内网域名吧。 军规：禁止使用小数存储国币有朋友问存储前乘以100，取出后除以100是否可行，个人建议“尽量少的使用除法”。 曾经踩过这样的坑，100元分3天摊销，每天摊销100/3元，结果得到3个33.33。后来实施对账系统，始终有几分钱对不齐，郁闷了很久（不是几分钱的事，是业务方质疑的眼神让研发很不爽），最后发现是除法惹的祸。解决方案：使用“分”作为单位，这样数据库里就是整数了。 案例：SELECT uid FROM t_user WHERE phone=13812345678 会导致全表扫描，而不能命中phone索引这个坑大家没踩过么？phone是varchar类型，SQL语句带入的是整形，故不会命中索引，加个引号就好了：SELECT uid FROM t_user WHERE phone=’13812345678’ 军规：禁止使用负向查询NOT、!=、&lt;&gt;、!&lt;、!&gt;、NOT IN、NOT LIKE等，会导致全表扫描此军规争议比较大，部分网友反馈不这么做很多业务实现不了，稍微解释一下：一般来说，WHERE过滤条件不会只带这么一个“负向查询条件”，还会有其他过滤条件，举个例子：查询沈剑已完成订单之外的订单（好拗口）：SELECT oid FROM t_order WHERE uid=123 AND status != 1; 订单表5000w数据，但uid=123就会迅速的将数据量过滤到很少的级别（uid建立了索引），此时再接上一个负向的查询条件就无所谓了，扫描的行数本身就会很少。 但如果要查询所有已完成订单之外的订单： SELECT oid FROM t_order WHERE status != 1; 这就挂了，立马CPU100%，status索引会失效，负向查询导致全表扫描。 末了，除了《58到家数据库30条军规解读》中提到的基础规范、命名规范、表设计规范、字段设计规范、索引设计规范、SQL使用规范，还有一个行为规范的军规：（31）禁止使用应用程序配置文件内的帐号手工访问线上数据库（32）禁止非DBA对线上数据库进行写操作，修改线上数据需要提交工单，由DBA执行，提交的SQL语句必须经过测试（33）分配非DBA以只读帐号，必须通过VPN+跳板机访问授权的从库（34）开发、测试、线上环境隔离 为什么要制定行为规范的军规呢，大伙的公司是不是有这样的情况：任何研发、测试都有连接线上数据库的帐号？是不是经常有这类误操作？（1）本来只想update一条记录，where条件搞错，update了全部的记录（2）本来只想delete几行记录，结果删多了，四下无人，再insert回去（3）以为drop的是测试库，结果把线上库drop掉了（4）以为操作的是分库x，结果SecureCRT开窗口太多，操作成了分库y（5）写错配置文件，压力测试压到线上库了，生成了N多脏数据…无数的事情，结果就是打电话给DBA，让他们帮忙擦屁股。…所谓的“业务灵活性”都是扯淡，为什么要有行为规范？不让你带刀，不是限制你，而是保护你的安全。要相信DBA是专业的，让专业的人干专业的事情。别把DBA看做你的对立面，多和他们沟通业务场景，沟通请求读写比，沟通访问模式，他们真的能帮助到你，这是我带DBA团队的一些感触。 谁都可能删除全库，能找回数据的，真的只有DBA。==【完】==]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[赶集mysql军规]]></title>
    <url>%2F2018%2F09%2F27%2Fdb-mysql-%E8%B5%B6%E9%9B%86mysql%E5%86%9B%E8%A7%84%2F</url>
    <content type="text"><![CDATA[一，核心军规 不在数据库做计算，cpu计算务必移至业务层 控制单表数据量，单表记录控制在千万级 控制列数量，字段数控制在20以内 平衡范式与冗余，为提高效率可以牺牲范式设计，冗余数据 拒绝3B(big)，大sql，大事务，大批量 二，字段类军规 用好数值类型 tinyint(1Byte)smallint(2Byte)mediumint(3Byte)int(4Byte)bigint(8Byte)bad case：int(1)/int(11) 有些字符转化为数字用int而不是char(15)存储ip 优先使用enum或set例如：sex enum (‘F’, ‘M’) 避免使用NULL字段NULL字段很难查询优化NULL字段的索引需要额外空间NULL字段的复合索引无效bad case：name char(32) default nullage int not nullgood case：age int not null default 0 不在数据库里存图片 三，索引类军规 谨慎合理使用索引改善查询、减慢更新索引一定不是越多越好（能不加就不加，要加的一定得加）覆盖记录条数过多不适合建索引，例如“性别” 字符字段必须建前缀索引 不在索引做列运算bad case：select id where age +1 = 10; innodb主键合理使用自增列主键建立聚簇索引主键不应该被修改字符串不应该做主键如果不指定主键，innodb会使用唯一且非空值索引代替 不用外键，请由程序保证约束 四，sql类军规 sql语句尽可能简单一条sql只能在一个cpu运算大语句拆小语句，减少锁时间一条大sql可以堵死整个库 简单的事务事务时间尽可能短bad case：上传图片事务 避免使用触发器，用户自定义函数，请由程序取而代之 不用select *消耗cpu，io，内存，带宽这种程序不具有扩展性 OR改写为IN() OR改写为UNION 画外音：最新的mysql内核已经进行了相关优化 limit高效分页limit越大，效率越低select id from t limit 10000, 10;应该改为 =&gt;select id from t where id &gt; 10000 limit 10; 使用union all替代union，union有去重开销 尽量不用连接join 务必请使用“同类型”进行比较，否则可能全表扫面 打散批量更新 使用新能分析工具show profile;mysqlsla;mysqldumpslow;explain;show slow log;show processlist;show query_response_time(percona)]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产环境中的 redis 是怎么部署的？]]></title>
    <url>%2F2018%2F08%2F06%2Fsubject-high-concurrency-redis-production-environment%2F</url>
    <content type="text"><![CDATA[剖析redis cluster，10 台机器，5 台机器部署了 redis 主实例，另外 5 台机器部署了 redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰qps可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求/s。 机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 redis 进程的是10g内存，一般线上生产环境，redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。 5 台机器对外提供读写，一共有 50g 内存。 因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，redis 从实例会自动变成主实例继续提供读写服务。 你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。 其实大型的公司，会有基础架构的 team 负责缓存集群的运维。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis 的持久化有哪几种方式？]]></title>
    <url>%2F2018%2F08%2F05%2Fsubject-high-concurrency-redis-persistence%2F</url>
    <content type="text"><![CDATA[剖析持久化主要是做灾难恢复、数据恢复，也可以归类到高可用的一个环节中去，比如你 redis 整个挂了，然后 redis 就不可用了，你要做的事情就是让 redis 变得可用，尽快变得可用。 重启 redis，尽快让它对外提供服务，如果没做数据备份，这时候 redis 启动了，也不可用啊，数据都没了。 很可能说，大量的请求过来，缓存全部无法命中，在 redis 里根本找不到数据，这个时候就死定了，出现缓存雪崩问题。所有请求没有在 redis 命中，就会去 mysql 数据库这种数据源头中去找，一下子 mysql 承接高并发，然后就挂了… 如果你把 redis 持久化做好，备份和恢复方案做到企业级的程度，那么即使你的 redis 故障了，也可以通过备份数据，快速恢复，一旦恢复立即对外提供服务。 redis 持久化的两种方式 RDB：RDB 持久化机制，是对 redis 中的数据执行周期性的持久化。 AOF：AOF 机制对每条写入命令作为日志，以 append-only 的模式写入一个日志文件中，在 redis 重启的时候，可以通过回放 AOF 日志中的写入指令来重新构建整个数据集。 通过 RDB 或 AOF，都可以将 redis 内存中的数据给持久化到磁盘上面来，然后可以将这些数据备份到别的地方去，比如说阿里云等云服务。 如果 redis 挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动 redis，redis 就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务。 如果同时使用 RDB 和 AOF 两种持久化机制，那么在 redis 重启的时候，会使用 AOF 来重新构建数据，因为 AOF 中的数据更加完整。 RDB 优缺点 RDB 会生成多个数据文件，每个数据文件都代表了某一个时刻中 redis 的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说 Amazon 的 S3 云服务上去，在国内可以是阿里云的 ODPS 分布式存储上，以预定好的备份策略来定期备份 redis 中的数据。 RDB 对 redis 对外提供的读写服务，影响非常小，可以让 redis 保持高性能，因为 redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可。 相对于 AOF 持久化机制来说，直接基于 RDB 数据文件来重启和恢复 redis 进程，更加快速。 如果想要在 redis 故障时，尽可能少的丢失数据，那么 RDB 没有 AOF 好。一般来说，RDB 数据快照文件，都是每隔 5 分钟，或者更长时间生成一次，这个时候就得接受一旦 redis 进程宕机，那么会丢失最近 5 分钟的数据。 RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。 AOF 优缺点 AOF 可以更好的保护数据不丢失，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次fsync操作，最多丢失 1 秒钟的数据。 AOF 日志文件以 append-only 模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复。 AOF 日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在 rewrite log 的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。在创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。 AOF 日志文件的命令通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用 flushall 命令清空了所有数据，只要这个时候后台 rewrite 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 flushall 命令给删了，然后再将该 AOF 文件放回去，就可以通过恢复机制，自动恢复所有数据。 对于同一份数据来说，AOF 日志文件通常比 RDB 数据快照文件更大。 AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低，因为 AOF 一般会配置成每秒 fsync 一次日志文件，当然，每秒一次 fsync，性能也还是很高的。（如果实时写入，那么 QPS 会大降，redis 性能会大大降低） 以前 AOF 发生过 bug，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令日志 / merge / 回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。 RDB 和 AOF 到底该如何选择 不要仅仅使用 RDB，因为那样会导致你丢失很多数据； 也不要仅仅使用 AOF，因为那样有两个问题：第一，你通过 AOF 做冷备，没有 RDB 做冷备来的恢复速度更快；第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug； redis 支持同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 哨兵集群实现高可用]]></title>
    <url>%2F2018%2F08%2F04%2Fsubject-high-concurrency-redis-sentinel%2F</url>
    <content type="text"><![CDATA[Redis 哨兵集群实现高可用哨兵的介绍sentinel，中文名是哨兵。哨兵是 redis 集群机构中非常重要的一个组件，主要有以下功能： 集群监控：负责监控 redis master 和 slave 进程是否正常工作。 消息通知：如果某个 redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。 哨兵用于实现 redis 集群的高可用，本身也是分布式的，作为一个哨兵集群去运行，互相协同工作。 故障转移时，判断一个 master node 是否宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题。 即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很坑爹了。 哨兵的核心知识 哨兵至少需要 3 个实例，来保证自己的健壮性。 哨兵 + redis 主从的部署架构，是不保证数据零丢失的，只能保证 redis 集群的高可用性。 对于哨兵 + redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。 哨兵集群必须部署 2 个以上节点，如果哨兵集群仅仅部署了 2 个哨兵实例，quorum = 1。 1234+----+ +----+| M1 |---------| R1 || S1 | | S2 |+----+ +----+ 配置 quorum=1，如果 master 宕机， s1 和 s2 中只要有 1 个哨兵认为 master 宕机了，就可以进行切换，同时 s1 和 s2 会选举出一个哨兵来执行故障转移。但是同时这个时候，需要 majority，也就是大多数哨兵都是运行的。 123452 个哨兵，majority=23 个哨兵，majority=24 个哨兵，majority=25 个哨兵，majority=3... 如果此时仅仅是 M1 进程宕机了，哨兵 s1 正常运行，那么故障转移是 OK 的。但是如果是整个 M1 和 S1 运行的机器宕机了，那么哨兵只有 1 个，此时就没有 majority 来允许执行故障转移，虽然另外一台机器上还有一个 R1，但是故障转移不会执行。 经典的 3 节点哨兵集群是这样的： 123456789 +----+ | M1 | | S1 | +----+ |+----+ | +----+| R2 |----+----| R3 || S2 | | S3 |+----+ +----+ 配置 quorum=2，如果 M1 所在机器宕机了，那么三个哨兵还剩下 2 个，S2 和 S3 可以一致认为 master 宕机了，然后选举出一个来执行故障转移，同时 3 个哨兵的 majority 是 2，所以还剩下的 2 个哨兵运行着，就可以允许执行故障转移。 redis 哨兵主备切换的数据丢失问题两种情况和导致数据丢失主备切换的过程，可能会导致数据丢失： 异步复制导致的数据丢失 因为 master-&gt;slave 的复制是异步的，所以可能有部分数据还没复制到 slave，master 就宕机了，此时这部分数据就丢失了。 脑裂导致的数据丢失 脑裂，也就是说，某个 master 所在机器突然脱离了正常的网络，跟其他 slave 机器不能连接，但是实际上 master 还运行着。此时哨兵可能就会认为 master 宕机了，然后开启选举，将其他 slave 切换成了 master。这个时候，集群里就会有两个 master ，也就是所谓的脑裂。 此时虽然某个 slave 被切换成了 master，但是可能 client 还没来得及切换到新的 master，还继续向旧 master 写数据。因此旧 master 再次恢复的时候，会被作为一个 slave 挂到新的 master 上去，自己的数据会清空，重新从新的 master 复制数据。而新的 master 并没有后来 client 写入的数据，因此，这部分数据也就丢失了。 数据丢失问题的解决方案进行如下配置： 12min-slaves-to-write 1min-slaves-max-lag 10 表示，要求至少有 1 个 slave，数据复制和同步的延迟不能超过 10 秒。 如果说一旦所有的 slave，数据复制和同步的延迟都超过了 10 秒钟，那么这个时候，master 就不会再接收任何请求了。 减少异步复制数据的丢失 有了 min-slaves-max-lag 这个配置，就可以确保说，一旦 slave 复制数据和 ack 延时太长，就认为可能 master 宕机后损失的数据太多了，那么就拒绝写请求，这样可以把 master 宕机时由于部分数据未同步到 slave 导致的数据丢失降低的可控范围内。 减少脑裂的数据丢失 如果一个 master 出现了脑裂，跟其他 slave 丢了连接，那么上面两个配置可以确保说，如果不能继续给指定数量的 slave 发送数据，而且 slave 超过 10 秒没有给自己 ack 消息，那么就直接拒绝客户端的写请求。因此在脑裂场景下，最多就丢失 10 秒的数据。 sdown 和 odown 转换机制 sdown 是主观宕机，就一个哨兵如果自己觉得一个 master 宕机了，那么就是主观宕机 odown 是客观宕机，如果 quorum 数量的哨兵都觉得一个 master 宕机了，那么就是客观宕机 sdown 达成的条件很简单，如果一个哨兵 ping 一个 master，超过了 is-master-down-after-milliseconds 指定的毫秒数之后，就主观认为 master 宕机了；如果一个哨兵在指定时间内，收到了 quorum 数量的其它哨兵也认为那个 master 是 sdown 的，那么就认为是 odown 了。 哨兵集群的自动发现机制哨兵互相之间的发现，是通过 redis 的 pub/sub 系统实现的，每个哨兵都会往 __sentinel__:hello 这个 channel 里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在。 每隔两秒钟，每个哨兵都会往自己监控的某个 master+slaves 对应的 __sentinel__:hello channel 里发送一个消息，内容是自己的 host、ip 和 runid 还有对这个 master 的监控配置。 每个哨兵也会去监听自己监控的每个 master+slaves 对应的 __sentinel__:hello channel，然后去感知到同样在监听这个 master+slaves 的其他哨兵的存在。 每个哨兵还会跟其他哨兵交换对 master 的监控配置，互相进行监控配置的同步。 slave 配置的自动纠正哨兵会负责自动纠正 slave 的一些配置，比如 slave 如果要成为潜在的 master 候选人，哨兵会确保 slave 复制现有 master 的数据；如果 slave 连接到了一个错误的 master 上，比如故障转移之后，那么哨兵会确保它们连接到正确的 master 上。 slave-&gt;master 选举算法如果一个 master 被认为 odown 了，而且 majority 数量的哨兵都允许主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个 slave 来，会考虑 slave 的一些信息： 跟 master 断开连接的时长 slave 优先级 复制 offset run id 如果一个 slave 跟 master 断开连接的时间已经超过了 down-after-milliseconds 的 10 倍，外加 master 宕机的时长，那么 slave 就被认为不适合选举为 master。 1(down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state 接下来会对 slave 进行排序： 按照 slave 优先级进行排序，slave priority 越低，优先级就越高。 如果 slave priority 相同，那么看 replica offset，哪个 slave 复制了越多的数据，offset 越靠后，优先级就越高。 如果上面两个条件都相同，那么选择一个 run id 比较小的那个 slave。 quorum 和 majority每次一个哨兵要做主备切换，首先需要 quorum 数量的哨兵认为 odown，然后选举出一个哨兵来做切换，这个哨兵还需要得到 majority 哨兵的授权，才能正式执行切换。 如果 quorum &lt; majority，比如 5 个哨兵，majority 就是 3，quorum 设置为 2，那么就 3 个哨兵授权就可以执行切换。 但是如果 quorum &gt;= majority，那么必须 quorum 数量的哨兵都授权，比如 5 个哨兵，quorum 是 5，那么必须 5 个哨兵都同意授权，才能执行切换。 configuration epoch哨兵会对一套 redis master+slaves 进行监控，有相应的监控的配置。 执行切换的那个哨兵，会从要切换到的新 master（salve-&gt;master）那里得到一个 configuration epoch，这就是一个 version 号，每次切换的 version 号都必须是唯一的。 如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待 failover-timeout 时间，然后接替继续执行切换，此时会重新获取一个新的 configuration epoch，作为新的 version 号。 configuration 传播哨兵完成切换之后，会在自己本地更新生成最新的 master 配置，然后同步给其他的哨兵，就是通过之前说的 pub/sub 消息机制。 这里之前的 version 号就很重要了，因为各种消息都是通过一个 channel 去发布和监听的，所以一个哨兵完成一次新的切换之后，新的 master 配置是跟着新的 version 号的。其他的哨兵都是根据版本号的大小来更新自己的 master 配置的。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis 都有哪些数据类型？分别在哪些场景下使用比较合适？]]></title>
    <url>%2F2018%2F07%2F11%2Fsubject-high-concurrency-redis-data-types%2F</url>
    <content type="text"><![CDATA[分析除非是面试官感觉看你简历，是工作 3 年以内的比较初级的同学，可能对技术没有很深入的研究，面试官才会问这类问题。否则，在宝贵的面试时间里，面试官实在不想多问。 其实问这个问题，主要有两个原因： 看看你到底有没有全面的了解 redis 有哪些功能，一般怎么来用，啥场景用什么，就怕你别就会最简单的 KV 操作； 看看你在实际项目里都怎么玩儿过 redis。 要是你回答的不好，没说出几种数据类型，也没说什么场景，你完了，面试官对你印象肯定不好，觉得你平时就是做个简单的 set 和 get。 面试题剖析redis 主要有以下几种数据类型： string hash list set sorted set string这是最简单的类型，就是普通的 set 和 get，做简单的 KV 缓存。 1set college szu hash这个是类似 map 的一种结构，这个一般就是可以将结构化的数据，比如一个对象（前提是这个对象没嵌套其他的对象）给缓存在 redis 里，然后每次读写缓存的时候，可以就操作 hash 里的某个字段。 1234hset person name bingohset person age 20hset person id 1hget person name 12345person = &#123; "name": "bingo", "age": 20, "id": 1&#125; listlist 是有序列表，这个可以玩儿出很多花样。 比如可以通过 list 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的东西。 比如可以通过 lrange 命令，读取某个闭区间内的元素，可以基于 list 实现分页查询，这个是很棒的一个功能，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走。 12# 0开始位置，-1结束位置，结束位置为-1时，表示列表的最后一个位置，即查看所有。lrange mylist 0 -1 比如可以搞个简单的消息队列，从 list 头怼进去，从 list 尾巴那里弄出来。 123456lpush mylist 1lpush mylist 2lpush mylist 3 4 5# 1rpop mylist setset 是无序集合，自动去重。 直接基于 set 将系统里需要去重的数据扔进去，自动就给去重了，如果你需要对一些数据进行快速的全局去重，你当然也可以基于 jvm 内存里的 HashSet 进行去重，但是如果你的某个系统部署在多台机器上呢？得基于 redis 进行全局的 set 去重。 可以基于 set 玩儿交集、并集、差集的操作，比如交集吧，可以把两个人的粉丝列表整一个交集，看看俩人的共同好友是谁？对吧。 把两个大 V 的粉丝都放在两个 set 中，对两个 set 做交集。 1234567891011121314151617181920212223242526272829303132#-------操作一个set-------# 添加元素sadd mySet 1# 查看全部元素smembers mySet# 判断是否包含某个值sismember mySet 3# 删除某个/些元素srem mySet 1srem mySet 2 4# 查看元素个数scard mySet# 随机删除一个元素spop mySet#-------操作多个set-------# 将一个set的元素移动到另外一个setsmove yourSet mySet 2# 求两set的交集sinter yourSet mySet# 求两set的并集sunion yourSet mySet# 求在yourSet中而不在mySet中的元素sdiff yourSet mySet sorted setsorted set 是排序的 set，去重但可以排序，写进去的时候给一个分数，自动根据分数排序。 12345678910zadd board 85 zhangsanzadd board 72 lisizadd board 96 wangwuzadd board 63 zhaoliu# 获取排名前三的用户（默认是升序，所以需要 rev 改为降序）zrevrange board 0 3# 获取某用户的排名zrank board zhaoliu]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何保证缓存与数据库的双写一致性？]]></title>
    <url>%2F2018%2F07%2F11%2Fsubject-high-concurrency-redis-consistence%2F</url>
    <content type="text"><![CDATA[剖析一般来说，如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说如果你的系统不是严格要求 “缓存+数据库” 必须保持一致性的话，最好不要做这个方案，即：读请求和写请求串行化，串到一个内存队列里去。 串行化可以保证一定不会出现不一致的情况，但是它也会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。 Cache Aside Pattern最经典的缓存+数据库读写的模式，就是 Cache Aside Pattern。 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 更新的时候，先更新数据库，然后再删除缓存。 为什么是删除缓存，而不是更新缓存？ 原因很简单，很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值。 比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。 另外更新缓存的代价有时候是很高的。是不是说，每次修改数据库的时候，都一定要将其对应的缓存更新一份？也许有的场景是这样，但是对于比较复杂的缓存数据计算的场景，就不是这样了。如果你频繁修改一个缓存涉及的多个表，缓存也频繁更新。但是问题在于，这个缓存到底会不会被频繁访问到？ 举个栗子，一个缓存涉及的表的字段，在 1 分钟内就修改了 20 次，或者是 100 次，那么缓存更新 20 次、100 次；但是这个缓存在 1 分钟内只被读取了 1 次，有大量的冷数据。实际上，如果你只是删除缓存的话，那么在 1 分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低。用到缓存才去算缓存。 其实删除缓存，而不是更新缓存，就是一个 lazy 计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。像 mybatis，hibernate，都有懒加载思想。查询一个部门，部门带了一个员工的 list，没有必要说每次查询部门，都里面的 1000 个员工的数据也同时查出来啊。80% 的情况，查这个部门，就只是要访问这个部门的信息就可以了。先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询 1000 个员工。 最初级的缓存不一致问题及解决方案问题：先更新数据库，再删除缓存。如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据就出现了不一致。 解决思路：先删除缓存，再更新数据库。如果数据库更新失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致。因为读的时候缓存没有，所以去读了数据库中的旧数据，然后更新到缓存中。 比较复杂的数据不一致问题分析数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中。随后数据变更的程序完成了数据库的修改。完了，数据库和缓存中的数据不一样了… 为什么上亿流量高并发场景下，缓存会出现这个问题？ 只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题。其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就 1 万次，那么很少的情况下，会出现刚才描述的那种不一致的场景。但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就可能会出现上述的数据库+缓存不一致的情况。 解决方案如下： 更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个 jvm 内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个 jvm 内部队列中。 一个队列对应一个工作线程，每个工作线程串行拿到对应的操作，然后一条一条的执行。这样的话，一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没完成更新。此时如果一个读请求过来，没有读到缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。 这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。 待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中。 如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值。 高并发的场景下，该解决方案要注意的问题： 读请求长时阻塞 由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回。 该解决方案，最大的风险点在于说，可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库。务必通过一些模拟真实的测试，看看更新数据的频率是怎样的。 另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据的更新操作。如果一个内存队列里居然会挤压 100 个商品的库存修改操作，每隔库存修改操作要耗费 10ms 去完成，那么最后一个商品的读请求，可能等待 10 * 100 = 1000ms = 1s 后，才能得到数据，这个时候就导致读请求的长时阻塞。 一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会 hang 多少时间，如果读请求在 200ms 返回，如果你计算过后，哪怕是最繁忙的时候，积压 10 个更新操作，最多等待 200ms，那还可以的。 如果一个内存队列中可能积压的更新操作特别多，那么你就要加机器，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少。 其实根据之前的项目经验，一般来说，数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的。像这种针对读高并发、读缓存架构的项目，一般来说写请求是非常少的，每秒的 QPS 能到几百就不错了。 我们来实际粗略测算一下。 如果一秒有 500 的写操作，如果分成 5 个时间片，每 200ms 就 100 个写操作，放到 20 个内存队列中，每个内存队列，可能就积压 5 个写操作。每个写操作性能测试后，一般是在 20ms 左右就完成，那么针对每个内存队列的数据的读请求，也就最多 hang 一会儿，200ms 以内肯定能返回了。 经过刚才简单的测算，我们知道，单机支撑的写 QPS 在几百是没问题的，如果写 QPS 扩大了 10 倍，那么就扩容机器，扩容 10 倍的机器，每个机器 20 个队列。 读请求并发量过高 这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时 hang 在服务上，看服务能不能扛的住，需要多少机器才能扛住最大的极限情况的峰值。 但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大。 多服务实例部署的请求路由 可能这个服务部署了多个实例，那么必须保证说，执行数据更新操作，以及执行缓存更新操作的请求，都通过 Nginx 服务器路由到相同的服务实例上。 比如说，对同一个商品的读写请求，全部路由到同一台机器上。可以自己去做服务间的按照某个请求参数的 hash 路由，也可以用 Nginx 的 hash 路由功能等等。 热点商品的路由问题，导致请求的倾斜 万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能会造成某台机器的压力过大。就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以其实要根据业务系统去看，如果更新频率不是太高的话，这个问题的影响并不是特别大，但是的确可能某些机器的负载会高一些。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis 的过期策略都有哪些？内存淘汰机制都有哪些？]]></title>
    <url>%2F2018%2F07%2F09%2Fsubject-high-concurrency-redis-expiration-policies-and-lru%2F</url>
    <content type="text"><![CDATA[分析如果你连这个问题都不知道，上来就懵了，回答不出来，那线上你写代码的时候，想当然的认为写进 redis 的数据就一定会存在，后面导致系统各种 bug，谁来负责？ 常见的有两个问题： 往 redis 写入的数据怎么没了？ 可能有同学会遇到，在生产环境的 redis 经常会丢掉一些数据，写进去了，过一会儿可能就没了。我的天，同学，你问这个问题就说明 redis 你就没用对啊。redis 是缓存，你给当存储了是吧？ 啥叫缓存？用内存当缓存。内存是无限的吗，内存是很宝贵而且是有限的，磁盘是廉价而且是大量的。可能一台机器就几十个 G 的内存，但是可以有几个 T 的硬盘空间。redis 主要是基于内存来进行高性能、高并发的读写操作的。 那既然内存是有限的，比如 redis 就只能用 10G，你要是往里面写了 20G 的数据，会咋办？当然会干掉 10G 的数据，然后就保留 10G 的数据了。那干掉哪些数据？保留哪些数据？当然是干掉不常用的数据，保留常用的数据了。 数据明明过期了，怎么还占用着内存？ 这是由 redis 的过期策略来决定。 面试题剖析redis 过期策略redis 过期策略是：定期删除+惰性删除。 所谓定期删除，指的是 redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。 假设 redis 里放了 10w 个 key，都设置了过期时间，你每隔几百毫秒，就检查 10w 个 key，那 redis 基本上就死了，cpu 负载会很高的，消耗在你的检查过期 key 上了。注意，这里可不是每隔 100ms 就遍历所有的设置过期时间的 key，那样就是一场性能上的灾难。实际上 redis 是每隔 100ms 随机抽取一些 key 来检查和删除的。 但是问题是，定期删除可能会导致很多过期 key 到了时间并没有被删除掉，那咋整呢？所以就是惰性删除了。这就是说，在你获取某个 key 的时候，redis 会检查一下 ，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除，不会给你返回任何东西。 获取 key 的时候，如果此时 key 已经过期，就删除，不会返回任何东西。 但是实际上这还是有问题的，如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期 key 堆积在内存里，导致 redis 内存块耗尽了，咋整？ 答案是：走内存淘汰机制。 内存淘汰机制redis 内存淘汰机制有以下几个： noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错，这个一般没人用吧，实在是太恶心了。 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）。 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key，这个一般没人用吧，为啥要随机，肯定是把最近最少使用的 key 给干掉啊。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key（这个一般不太合适）。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。 手写一个 LRU 算法你可以现场手写最原始的 LRU 算法，那个代码量太大了，似乎不太现实。 不求自己纯手工从底层开始打造出自己的 LRU，但是起码要知道如何利用已有的 JDK 数据结构实现一个 Java 版的 LRU。 1234567891011121314151617181920class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123; private final int CACHE_SIZE; /** * 传递进来最多能缓存多少数据 * * @param cacheSize 缓存大小 */ public LRUCache(int cacheSize) &#123; // true 表示让 linkedHashMap 按照访问顺序来进行排序，最近访问的放在头部，最老访问的放在尾部。 super((int) Math.ceil(cacheSize / 0.75) + 1, 0.75f, true); CACHE_SIZE = cacheSize; &#125; @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) &#123; // 当 map中的数据量大于指定的缓存个数的时候，就自动删除最老的数据。 return size() &gt; CACHE_SIZE; &#125;&#125;]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 主从架构]]></title>
    <url>%2F2018%2F07%2F04%2Fsubject-high-concurrency-redis-master-slave%2F</url>
    <content type="text"><![CDATA[Redis 主从架构单机的 redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。因此架构做成主从(master-slave)架构，一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读。所有的读请求全部走从节点。这样也可以很轻松实现水平扩容，支撑读高并发。 redis replication -&gt; 主从架构 -&gt; 读写分离 -&gt; 水平扩容支撑读高并发 redis replication 的核心机制 redis 采用异步方式复制数据到 slave 节点，不过 redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量； 一个 master node 是可以配置多个 slave node 的； slave node 也可以连接其他的 slave node； slave node 做复制的时候，不会 block master node 的正常工作； slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了； slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量。 注意，如果采用了主从架构，那么建议必须开启 master node 的持久化，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。 另外，master 的各种备份方案，也需要做。万一本地的所有文件丢失了，从备份中挑选一份 rdb 去恢复 master，这样才能确保启动的时候，是有数据的，即使采用了后续讲解的高可用机制，slave node 可以自动接管 master node，但也可能 sentinel 还没检测到 master failure，master node 就自动重启了，还是可能导致上面所有的 slave node 数据被清空。 redis 主从复制的核心原理当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node。 如果这是 slave node 初次连接到 master node，那么会触发一次 full resynchronization 全量复制。此时 master 会启动一个后台线程，开始生成一份 RDB 快照文件，同时还会将从客户端 client 新收到的所有写命令缓存在内存中。RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave，slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中，接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。 主从复制的断点续传从 redis2.8 开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份。 master node 会在内存中维护一个 backlog，master 和 slave 都会保存一个 replica offset 还有一个 master run id，offset 就是保存在 backlog 中的。如果 master 和 slave 网络连接断掉了，slave 会让 master 从上次 replica offset 开始继续复制，如果没有找到对应的 offset，那么就会执行一次 resynchronization。 如果根据 host+ip 定位 master node，是不靠谱的，如果 master node 重启或者数据出现了变化，那么 slave node 应该根据不同的 run id 区分。 无磁盘化复制master 在内存中直接创建 RDB，然后发送给 slave，不会在自己本地落地磁盘了。只需要在配置文件中开启 repl-diskless-sync yes 即可。 1234repl-diskless-sync yes# 等待 5s 后再开始复制，因为要等更多 slave 重新连接过来repl-diskless-sync-delay 5 过期 key 处理slave 不会过期 key，只会等待 master 过期 key。如果 master 过期了一个 key，或者通过 LRU 淘汰了一个 key，那么会模拟一条 del 命令发送给 slave。 复制的完整流程slave node 启动时，会在自己本地保存 master node 的信息，包括 master node 的host和ip，但是复制流程没开始。 slave node 内部有个定时任务，每秒检查是否有新的 master node 要连接和复制，如果发现，就跟 master node 建立 socket 网络连接。然后 slave node 发送 ping 命令给 master node。如果 master 设置了 requirepass，那么 slave node 必须发送 masterauth 的口令过去进行认证。master node 第一次执行全量复制，将所有数据发给 slave node。而在后续，master node 持续将写命令，异步复制给 slave node。 全量复制 master 执行 bgsave ，在本地生成一份 rdb 快照文件。 master node 将 rdb 快照文件发送给 slave node，如果 rdb 复制时间超过 60秒（repl-timeout），那么 slave node 就会认为复制失败，可以适当调大这个参数(对于千兆网卡的机器，一般每秒传输 100MB，6G 文件，很可能超过 60s) master node 在生成 rdb 时，会将所有新的写命令缓存在内存中，在 slave node 保存了 rdb 之后，再将新的写命令复制给 slave node。 如果在复制期间，内存缓冲区持续消耗超过 64MB，或者一次性超过 256MB，那么停止复制，复制失败。 1client-output-buffer-limit slave 256MB 64MB 60 slave node 接收到 rdb 之后，清空自己的旧数据，然后重新加载 rdb 到自己的内存中，同时基于旧的数据版本对外提供服务。 如果 slave node 开启了 AOF，那么会立即执行 BGREWRITEAOF，重写 AOF。 增量复制 如果全量复制过程中，master-slave 网络连接断掉，那么 slave 重新连接 master 时，会触发增量复制。 master 直接从自己的 backlog 中获取部分丢失的数据，发送给 slave node，默认 backlog 就是 1MB。 master 就是根据 slave 发送的 psync 中的 offset 来从 backlog 中获取数据的。 heartbeat主从节点互相都会发送 heartbeat 信息。 master 默认每隔 10秒 发送一次 heartbeat，slave node 每隔 1秒 发送一个 heartbeat。 异步复制master 每次接收到写命令之后，先在内部写入数据，然后异步发送给 slave node。 redis 如何才能做到高可用如果系统在 365 天内，有 99.99% 的时间，都是可以哗哗对外提供服务的，那么就说系统是高可用的。 一个 slave 挂掉了，是不会影响可用性的，还有其它的 slave 在提供相同数据下的相同的对外的查询服务。 但是，如果 master node 死掉了，会怎么样？没法写数据了，写缓存的时候，全部失效了。slave node 还有什么用呢，没有 master 给它们复制数据了，系统相当于不可用了。 redis 的高可用架构，叫做 failover 故障转移，也可以叫做主备切换。 master node 在故障时，自动检测，并且将某个 slave node 自动切换为 master node 的过程，叫做主备切换。这个过程，实现了 redis 的主从架构下的高可用。 后面会详细说明 redis 基于哨兵的高可用性。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis 和 memcached 有什么区别？]]></title>
    <url>%2F2018%2F07%2F04%2Fsubject-high-concurrency-redis-single-thread-model%2F</url>
    <content type="text"><![CDATA[剖析redis 和 memcached 有啥区别？redis 支持复杂的数据结构redis 相比 memcached 来说，拥有更多的数据结构，能支持更丰富的数据操作。如果需要缓存能够支持更复杂的结构和操作， redis 会是不错的选择。 redis 原生支持集群模式在 redis3.x 版本中，便能支持 cluster 模式，而 memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。 性能对比由于 redis 只使用单核，而 memcached 可以使用多核，所以平均每一个核上 redis 在存储小数据时比 memcached 性能更高。而在 100k 以上的数据中，memcached 性能要高于 redis。虽然 redis 最近也在存储大数据的性能上进行优化，但是比起 memcached，还是稍有逊色。 redis 的线程模型redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。 文件事件处理器的结构包含 4 个部分： 多个 socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） 多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将产生事件的 socket 放入队列中排队，事件分派器每次从队列中取出一个 socket，根据 socket 的事件类型交给对应的事件处理器进行处理。 来看客户端与 redis 的一次通信过程： 要明白，通信是通过 socket 来完成的，不懂的同学可以先去看一看 socket 网络编程。 首先，redis 服务端进程初始化的时候，会将 server socket 的 AE_READABLE 事件与连接应答处理器关联。 客户端 socket01 向 redis 进程的 server socket 请求建立连接，此时 server socket 会产生一个 AE_READABLE 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，交给连接应答处理器。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 的 AE_READABLE 事件与命令请求处理器关联。 假设此时客户端发送了一个 set key value 请求，此时 redis 中的 socket01 会产生 AE_READABLE 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 AE_READABLE 事件，由于前面 socket01 的 AE_READABLE 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 key value 并在自己内存中完成 key value 的设置。操作完成后，它会将 socket01 的 AE_WRITABLE 事件与命令回复处理器关联。 如果此时客户端准备好接收返回结果了，那么 redis 中的 socket01 会产生一个 AE_WRITABLE 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 ok，之后解除 socket01 的 AE_WRITABLE 事件与命令回复处理器的关联。 这样便完成了一次通信。 为啥 redis 单线程模型也能效率这么高？ 纯内存操作 核心是基于非阻塞的 IO 多路复用机制 单线程反而避免了多线程的频繁上下文切换问题]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis 的雪崩、穿透和击穿？]]></title>
    <url>%2F2018%2F07%2F04%2Fsubject-high-concurrency-redis-caching-avalanche-and-caching-penetration%2F</url>
    <content type="text"><![CDATA[缓存雪崩对于系统 A，假设每天高峰期每秒 5000 个请求，本来缓存在高峰期可以扛住每秒 4000 个请求，但是缓存机器意外发生了全盘宕机。缓存挂了，此时 1 秒 5000 个请求全部落数据库，数据库必然扛不住，它会报一下警，然后就挂了。此时，如果没有采用什么特别的方案来处理这个故障，DBA 很着急，重启数据库，但是数据库立马又被新的流量给打死了。 这就是缓存雪崩。 大约在 3 年前，国内比较知名的一个互联网公司，曾因为缓存事故，导致雪崩，后台系统全部崩溃，事故从当天下午持续到晚上凌晨 3~4 点，公司损失了几千万。 缓存雪崩的事前事中事后的解决方案如下。 事前：redis 高可用，主从+哨兵，redis cluster，避免全盘崩溃。 事中：本地 ehcache 缓存 + hystrix 限流&amp;降级，避免 MySQL 被打死。 事后：redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。 用户发送一个请求，系统 A 收到请求后，先查本地 ehcache 缓存，如果没查到再查 redis。如果 ehcache 和 redis 都没有，再查数据库，将数据库中的结果，写入 ehcache 和 redis 中。 限流组件，可以设置每秒的请求，有多少能通过组件，剩余的未通过的请求，怎么办？走降级！可以返回一些默认的值，或者友情提示，或者空白的值。 好处： 数据库绝对不会死，限流组件确保了每秒只有多少个请求能通过。 只要数据库不死，就是说，对用户来说，2/5 的请求都是可以被处理的。 只要有 2/5 的请求可以被处理，就意味着你的系统没死，对用户来说，可能就是点击几次刷不出来页面，但是多点几次，就可以刷出来一次。 缓存穿透对于系统A，假设一秒 5000 个请求，结果其中 4000 个请求是黑客发出的恶意攻击。 黑客发出的那 4000 个攻击，缓存中查不到，每次你去数据库里查，也查不到。 举个栗子。数据库 id 是从 1 开始的，结果黑客发过来的请求 id 全部都是负数。这样的话，缓存中不会有，请求每次都“视缓存于无物”，直接查询数据库。这种恶意攻击场景的缓存穿透就会直接把数据库给打死。 解决方式很简单，每次系统 A 从数据库中只要没查到，就写一个空值到缓存里去，比如 set -999 UNKNOWN。然后设置一个过期时间，这样的话，下次有相同的 key 来访问的时候，在缓存失效之前，都可以直接从缓存中取数据。 缓存击穿缓存击穿，就是说某个 key 非常热点，访问非常频繁，处于集中式高并发访问的情况，当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库，就像是在一道屏障上凿开了一个洞。 解决方式也很简单，可以将热点数据设置为永远不过期；或者基于 redis or zookeeper 实现互斥锁，等待第一个请求构建完缓存之后，再释放锁，进而其它请求才能通过该 key 访问数据。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何设计才可以让系统从未分库分表**动态切换**到分库分表上？]]></title>
    <url>%2F2018%2F07%2F03%2Fsubject-high-concurrency-database-shard-method%2F</url>
    <content type="text"><![CDATA[这个其实从 low 到高大上有好几种方案，我们都玩儿过，我都给你说一下。 停机迁移方案我先给你说一个最 low 的方案，就是很简单，大家伙儿凌晨 12 点开始运维，网站或者 app 挂个公告，说 0 点到早上 6 点进行运维，无法访问。 接着到 0 点停机，系统停掉，没有流量写入了，此时老的单库单表数据库静止了。然后你之前得写好一个导数的一次性工具，此时直接跑起来，然后将单库单表的数据哗哗哗读出来，写到分库分表里面去。 导数完了之后，就 ok 了，修改系统的数据库连接配置啥的，包括可能代码和 SQL 也许有修改，那你就用最新的代码，然后直接启动连到新的分库分表上去。 验证一下，ok了，完美，大家伸个懒腰，看看看凌晨 4 点钟的北京夜景，打个滴滴回家吧。 但是这个方案比较 low，谁都能干，我们来看看高大上一点的方案。 双写迁移方案这个是我们常用的一种迁移方案，比较靠谱一些，不用停机，不用看北京凌晨 4 点的风景。 简单来说，就是在线上系统里面，之前所有写库的地方，增删改操作，除了对老库增删改，都加上对新库的增删改，这就是所谓的双写，同时写俩库，老库和新库。 然后系统部署之后，新库数据差太远，用之前说的导数工具，跑起来读老库数据写新库，写的时候要根据 gmt_modified 这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是比新库的数据新才会写。简单来说，就是不允许用老数据覆盖新数据。 导完一轮之后，有可能数据还是存在不一致，那么就程序自动做一轮校验，比对新老库每个表的每条数据，接着如果有不一样的，就针对那些不一样的，从老库读数据再次写。反复循环，直到两个库每个表的数据都完全一致为止。 接着当数据完全一致了，就 ok 了，基于仅仅使用分库分表的最新代码，重新部署一次，不就仅仅基于分库分表在操作了么，还没有几个小时的停机时间，很稳。所以现在基本玩儿数据迁移之类的，都是这么干的。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分库分表之后，id 主键如何处理？]]></title>
    <url>%2F2018%2F07%2F03%2Fsubject-high-concurrency-database-shard-global-id-generate%2F</url>
    <content type="text"><![CDATA[基于数据库的实现方案数据库自增 id这个就是说你的系统里每次得到一个 id，都是往一个库的一个表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个 id。拿到这个 id 之后再往对应的分库分表里去写入。 这个方案的好处就是方便简单，谁都会用；缺点就是单库生成自增 id，要是高并发的话，就会有瓶颈的；如果你硬是要改进一下，那么就专门开一个服务出来，这个服务每次就拿到当前 id 最大值，然后自己递增几个 id，一次性返回一批 id，然后再把当前最大 id 值修改成递增几个 id 之后的一个值；但是无论如何都是基于单个数据库。 适合的场景：你分库分表就俩原因，要不就是单库并发太高，要不就是单库数据量太大；除非是你并发不高，但是数据量太大导致的分库分表扩容，你可以用这个方案，因为可能每秒最高并发最多就几百，那么就走单独的一个库和表生成自增主键即可。 设置数据库 sequence 或者表自增字段步长可以通过设置数据库 sequence 或者表的自增字段步长来进行水平伸缩。 比如说，现在有 8 个服务节点，每个服务节点使用一个 sequence 功能来产生 ID，每个 sequence 的起始 ID 不同，并且依次递增，步长都是 8。 适合的场景：在用户防止产生的 ID 重复时，这种方案实现起来比较简单，也能达到性能目标。但是服务节点固定，步长也固定，将来如果还要增加服务节点，就不好搞了。 UUID好处就是本地生成，不要基于数据库来了；不好之处就是，UUID 太长了、占用空间大，作为主键性能太差了；更重要的是，UUID 不具有有序性，会导致 B+ 树索引在写的时候有过多的随机写操作（连续的 ID 可以产生部分顺序写），还有，由于在写的时候不能产生有顺序的 append 操作，而需要进行 insert 操作，将会读取整个 B+ 树节点到内存，在插入这条记录后会将整个节点写回磁盘，这种操作在记录占用空间比较大的情况下，性能下降明显。 适合的场景：如果你是要随机生成个什么文件名、编号之类的，你可以用 UUID，但是作为主键是不能用 UUID 的。 1UUID.randomUUID().toString().replace(“-”, “”) -&gt; sfsdf23423rr234sfdaf 获取系统当前时间这个就是获取当前时间即可，但是问题是，并发很高的时候，比如一秒并发几千，会有重复的情况，这个是肯定不合适的。基本就不用考虑了。 适合的场景：一般如果用这个方案，是将当前时间跟很多其他的业务字段拼接起来，作为一个 id，如果业务上你觉得可以接受，那么也是可以的。你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的编号。 snowflake 算法snowflake 算法是 twitter 开源的分布式 id 生成算法，采用 Scala 语言实现，是把一个 64 位的 long 型的 id，1 个 bit 是不用的，用其中的 41 bit 作为毫秒数，用 10 bit 作为工作机器 id，12 bit 作为序列号。 1 bit：不用，为啥呢？因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。 41 bit：表示的是时间戳，单位是毫秒。41 bit 可以表示的数字多达 2^41 - 1，也就是可以标识 2^41 - 1 个毫秒值，换算成年就是表示69年的时间。 10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10台机器上哪，也就是1024台机器。但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 2^5个机房（32个机房），每个机房里可以代表 2^5 个机器（32台机器）。 12 bit：这个是用来记录同一个毫秒内产生的不同 id，12 bit 可以代表的最大正整数是 2^12 - 1 = 4096，也就是说可以用这个 12 bit 代表的数字来区分同一个毫秒内的 4096 个不同的 id。 10 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000000 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public class IdWorker &#123; private long workerId; private long datacenterId; private long sequence; public IdWorker(long workerId, long datacenterId, long sequence) &#123; // sanity check for workerId // 这儿不就检查了一下，要求就是你传递进来的机房id和机器id不能超过32，不能小于0 if (workerId &gt; maxWorkerId || workerId &lt; 0) &#123; throw new IllegalArgumentException( String.format("worker Id can't be greater than %d or less than 0", maxWorkerId)); &#125; if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) &#123; throw new IllegalArgumentException( String.format("datacenter Id can't be greater than %d or less than 0", maxDatacenterId)); &#125; System.out.printf( "worker starting. timestamp left shift %d, datacenter id bits %d, worker id bits %d, sequence bits %d, workerid %d", timestampLeftShift, datacenterIdBits, workerIdBits, sequenceBits, workerId); this.workerId = workerId; this.datacenterId = datacenterId; this.sequence = sequence; &#125; private long twepoch = 1288834974657L; private long workerIdBits = 5L; private long datacenterIdBits = 5L; // 这个是二进制运算，就是 5 bit最多只能有31个数字，也就是说机器id最多只能是32以内 private long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits); // 这个是一个意思，就是 5 bit最多只能有31个数字，机房id最多只能是32以内 private long maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits); private long sequenceBits = 12L; private long workerIdShift = sequenceBits; private long datacenterIdShift = sequenceBits + workerIdBits; private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; private long sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits); private long lastTimestamp = -1L; public long getWorkerId() &#123; return workerId; &#125; public long getDatacenterId() &#123; return datacenterId; &#125; public long getTimestamp() &#123; return System.currentTimeMillis(); &#125; public synchronized long nextId() &#123; // 这儿就是获取当前时间戳，单位是毫秒 long timestamp = timeGen(); if (timestamp &lt; lastTimestamp) &#123; System.err.printf("clock is moving backwards. Rejecting requests until %d.", lastTimestamp); throw new RuntimeException(String.format( "Clock moved backwards. Refusing to generate id for %d milliseconds", lastTimestamp - timestamp)); &#125; if (lastTimestamp == timestamp) &#123; // 这个意思是说一个毫秒内最多只能有4096个数字 // 无论你传递多少进来，这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围 sequence = (sequence + 1) &amp; sequenceMask; if (sequence == 0) &#123; timestamp = tilNextMillis(lastTimestamp); &#125; &#125; else &#123; sequence = 0; &#125; // 这儿记录一下最近一次生成id的时间戳，单位是毫秒 lastTimestamp = timestamp; // 这儿就是将时间戳左移，放到 41 bit那儿； // 将机房 id左移放到 5 bit那儿； // 将机器id左移放到5 bit那儿；将序号放最后12 bit； // 最后拼接起来成一个 64 bit的二进制数字，转换成 10 进制就是个 long 型 return ((timestamp - twepoch) &lt;&lt; timestampLeftShift) | (datacenterId &lt;&lt; datacenterIdShift) | (workerId &lt;&lt; workerIdShift) | sequence; &#125; private long tilNextMillis(long lastTimestamp) &#123; long timestamp = timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = timeGen(); &#125; return timestamp; &#125; private long timeGen() &#123; return System.currentTimeMillis(); &#125; // ---------------测试--------------- public static void main(String[] args) &#123; IdWorker worker = new IdWorker(1, 1, 1); for (int i = 0; i &lt; 30; i++) &#123; System.out.println(worker.nextId()); &#125; &#125;&#125; 怎么说呢，大概这个意思吧，就是说 41 bit 是当前毫秒单位的一个时间戳，就这意思；然后 5 bit 是你传递进来的一个机房 id（但是最大只能是 32 以内），另外 5 bit 是你传递进来的机器 id（但是最大只能是 32 以内），剩下的那个 12 bit序列号，就是如果跟你上次生成 id 的时间还在一个毫秒内，那么会把顺序给你累加，最多在 4096 个序号以内。 所以你自己利用这个工具类，自己搞一个服务，然后对每个机房的每个机器都初始化这么一个东西，刚开始这个机房的这个机器的序号就是 0。然后每次接收到一个请求，说这个机房的这个机器要生成一个 id，你就找到对应的 Worker 生成。 利用这个 snowflake 算法，你可以开发自己公司的服务，甚至对于机房 id 和机器 id，反正给你预留了 5 bit + 5 bit，你换成别的有业务含义的东西也可以的。 这个 snowflake 算法相对来说还是比较靠谱的，所以你要真是搞分布式 id 生成，如果是高并发啥的，那么用这个应该性能比较好，一般每秒几万并发的场景，也足够你用了。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么要分库分表？]]></title>
    <url>%2F2018%2F07%2F03%2Fsubject-high-concurrency-database-shard%2F</url>
    <content type="text"><![CDATA[为什么要分库分表？（设计高并发系统的时候，数据库层面该如何设计？）说白了，分库分表是两回事儿，大家可别搞混了，可能是光分库不分表，也可能是光分表不分库，都有可能。 我先给大家抛出来一个场景。 假如我们现在是一个小创业公司（或者是一个 BAT 公司刚兴起的一个新部门），现在注册用户就 20 万，每天活跃用户就 1 万，每天单表数据量就 1000，然后高峰期每秒钟并发请求最多就 10。天，就这种系统，随便找一个有几年工作经验的，然后带几个刚培训出来的，随便干干都可以。 结果没想到我们运气居然这么好，碰上个 CEO 带着我们走上了康庄大道，业务发展迅猛，过了几个月，注册用户数达到了 2000 万！每天活跃用户数 100 万！每天单表数据量 10 万条！高峰期每秒最大请求达到 1000！同时公司还顺带着融资了两轮，进账了几个亿人民币啊！公司估值达到了惊人的几亿美金！这是小独角兽的节奏！ 好吧，没事，现在大家感觉压力已经有点大了，为啥呢？因为每天多 10 万条数据，一个月就多 300 万条数据，现在咱们单表已经几百万数据了，马上就破千万了。但是勉强还能撑着。高峰期请求现在是 1000，咱们线上部署了几台机器，负载均衡搞了一下，数据库撑 1000QPS 也还凑合。但是大家现在开始感觉有点担心了，接下来咋整呢…… 再接下来几个月，我的天，CEO 太牛逼了，公司用户数已经达到 1 亿，公司继续融资几十亿人民币啊！公司估值达到了惊人的几十亿美金，成为了国内今年最牛逼的明星创业公司！天，我们太幸运了。 但是我们同时也是不幸的，因为此时每天活跃用户数上千万，每天单表新增数据多达 50 万，目前一个表总数据量都已经达到了两三千万了！扛不住啊！数据库磁盘容量不断消耗掉！高峰期并发达到惊人的 5000~8000！别开玩笑了，哥。我跟你保证，你的系统支撑不到现在，已经挂掉了！ 好吧，所以你看到这里差不多就理解分库分表是怎么回事儿了，实际上这是跟着你的公司业务发展走的，你公司业务发展越好，用户就越多，数据量越大，请求量越大，那你单个数据库一定扛不住。 分表比如你单表都几千万数据了，你确定你能扛住么？绝对不行，单表数据量太大，会极大影响你的 sql 执行的性能，到了后面你的 sql 可能就跑的很慢了。一般来说，就以我的经验来看，单表到几百万的时候，性能就会相对差一些了，你就得分表了。 分表是啥意思？就是把一个表的数据放到多个表中，然后查询的时候你就查一个表。比如按照用户 id 来分表，将一个用户的数据就放在一个表中。然后操作的时候你对一个用户就操作那个表就好了。这样可以控制每个表的数据量在可控的范围内，比如每个表就固定在 200 万以内。 分库分库是啥意思？就是你一个库一般我们经验而言，最多支撑到并发 2000，一定要扩容了，而且一个健康的单库并发值你最好保持在每秒 1000 左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。 这就是所谓的分库分表，为啥要分库分表？你明白了吧。 # 分库分表前 分库分表后 并发支撑情况 MySQL 单机部署，扛不住高并发 MySQL从单机到多机，能承受的并发增加了多倍 磁盘使用情况 MySQL 单机磁盘容量几乎撑满 拆分为多个库，数据库服务器磁盘使用率大大降低 SQL 执行性能 单表数据量太大，SQL 越跑越慢 单表数据量减少，SQL 执行效率明显提升 用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？这个其实就是看看你了解哪些分库分表的中间件，各个中间件的优缺点是啥？然后你用过哪些分库分表的中间件。 比较常见的包括： cobar TDDL atlas sharding-jdbc mycat cobar阿里 b2b 团队开发和开源的，属于 proxy 层方案，就是介于应用服务器和数据库服务器之间。应用程序通过 JDBC 驱动访问 cobar 集群，cobar 根据 SQL 和分库规则对 SQL 做分解，然后分发到 MySQL 集群不同的数据库实例上执行。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。 TDDL淘宝团队开发的，属于 client 层方案。支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。 atlas360 开源的，属于 proxy 层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。 sharding-jdbc当当开源的，属于 client 层方案。确实之前用的还比较多一些，因为 SQL 语法支持也比较多，没有太多限制，而且目前推出到了 2.0 版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、TCC 事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从 2017 年一直到现在，是有不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也可以选择的方案。 mycat基于 cobar 改造的，属于 proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 sharding jdbc 来说，年轻一些，经历的锤炼少一些。 总结综上，现在其实建议考量的，就是 sharding-jdbc 和 mycat，这两个都可以去考虑使用。 sharding-jdbc 这种 client 层方案的优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合 sharding-jdbc 的依赖； mycat 这种 proxy 层方案的缺点在于需要部署，自己运维一套中间件，运维成本高，但是好处在于对于各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞就行了。 通常来说，这两个方案其实都可以选用，但是我个人建议中小型公司选用 sharding-jdbc，client 层方案轻便，而且维护成本低，不需要额外增派人手，而且中小型公司系统复杂度会低一些，项目也没那么多；但是中大型公司最好还是选用 mycat 这类 proxy 层方案，因为可能大公司系统和项目非常多，团队很大，人员充足，那么最好是专门弄个人来研究和维护 mycat，然后大量项目直接透明使用即可。 你们具体是如何对数据库如何进行垂直拆分或水平拆分的？水平拆分的意思，就是把一个表的数据给弄到多个库的多个表里去，但是每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。水平拆分的意义，就是将数据均匀放更多的库里，然后用多个库来扛更高的并发，还有就是用多个库的存储容量来进行扩容。 垂直拆分的意思，就是把一个有很多字段的表给拆分成多个表，或者是多个库上去。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，会将较少的访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另外一个表里去。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。 这个其实挺常见的，不一定我说，大家很多同学可能自己都做过，把一个大表拆开，订单表、订单支付表、订单商品表。 还有表层面的拆分，就是分表，将一个表变成 N 个表，就是让每个表的数据量控制在一定范围内，保证 SQL 的性能。否则单表数据量越大，SQL 性能就越差。一般是 200 万行左右，不要太多，但是也得看具体你怎么操作，也可能是 500 万，或者是 100 万。你的SQL越复杂，就最好让单表行数越少。 好了，无论分库还是分表，上面说的那些数据库中间件都是可以支持的。就是基本上那些中间件可以做到你分库分表之后，中间件可以根据你指定的某个字段值，比如说 userid，自动路由到对应的库上去，然后再自动路由到对应的表里去。 你就得考虑一下，你的项目里该如何分库分表？一般来说，垂直拆分，你可以在表层面来做，对一些字段特别多的表做一下拆分；水平拆分，你可以说是并发承载不了，或者是数据量太大，容量承载不了，你给拆了，按什么字段来拆，你自己想好；分表，你考虑一下，你如果哪怕是拆到每个库里去，并发和容量都ok了，但是每个库的表还是太大了，那么你就分表，将这个表分开，保证每个表的数据量并不是很大。 而且这儿还有两种分库分表的方式： 一种是按照 range 来分，就是每个库一段连续的数据，这个一般是按比如时间范围来的，但是这种一般较少用，因为很容易产生热点问题，大量的流量都打在最新的数据上了。 或者是按照某个字段 hash 一下均匀分散，这个较为常用。 range 来分，好处在于说，扩容的时候很简单，因为你只要预备好，给每个月都准备一个库就可以了，到了一个新的月份的时候，自然而然，就会写新的库了；缺点，但是大部分的请求，都是访问最新的数据。实际生产用 range，要看场景。 hash 分发，好处在于说，可以平均分配每个库的数据量和请求压力；坏处在于说扩容起来比较麻烦，会有一个数据迁移的过程，之前的数据需要重新计算 hash 值重新分配到不同的库或表。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis 集群模式的工作原理]]></title>
    <url>%2F2018%2F07%2F01%2Fsubject-high-concurrency-redis-cluster%2F</url>
    <content type="text"><![CDATA[分析在前几年，redis 如果要搞几个节点，每个节点存储一部分的数据，得借助一些中间件来实现，比如说有 codis，或者 twemproxy，都有。有一些 redis 中间件，你读写 redis 中间件，redis 中间件负责将你的数据分布式存储在多台机器上的 redis 实例中。 这两年，redis 不断在发展，redis 也不断有新的版本，现在的 redis 集群模式，可以做到在多台机器上，部署多个 redis 实例，每个实例存储一部分的数据，同时每个 redis 主实例可以挂 redis 从实例，自动确保说，如果 redis 主实例挂了，会自动切换到 redis 从实例上来。 现在 redis 的新版本，大家都是用 redis cluster 的，也就是 redis 原生支持的 redis 集群模式，那么面试官肯定会就 redis cluster 对你来个几连炮。要是你没用过 redis cluster，正常，以前很多人用 codis 之类的客户端来支持集群，但是起码你得研究一下 redis cluster 吧。 如果你的数据量很少，主要是承载高并发高性能的场景，比如你的缓存一般就几个 G，单机就足够了，可以使用 replication，一个 master 多个 slaves，要几个 slave 跟你要求的读吞吐量有关，然后自己搭建一个 sentinel 集群去保证 redis 主从架构的高可用性。 redis cluster，主要是针对海量数据+高并发+高可用的场景。redis cluster 支撑 N 个 redis master node，每个 master node 都可以挂载多个 slave node。这样整个 redis 就可以横向扩容了。如果你要支撑更大数据量的缓存，那就横向扩容更多的 master 节点，每个 master 节点就能存放更多的数据了。 面试题剖析redis cluster 介绍 自动将数据进行分片，每个 master 上放一部分数据 提供内置的高可用支持，部分 master 不可用时，还是可以继续工作的 在 redis cluster 架构下，每个 redis 要放开两个端口号，比如一个是 6379，另外一个就是 加1w 的端口号，比如 16379。 16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议，gossip 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。 节点间的内部通信机制基本通信原理集群元数据的维护有两种方式：集中式、Gossip 协议。redis cluster 节点间采用 gossip 协议进行通信。 集中式是将集群元数据（节点信息、故障等等）几种存储在某个节点上。集中式元数据集中存储的一个典型代表，就是大数据领域的 storm。它是分布式的大数据实时计算引擎，是集中式的元数据存储的结构，底层基于 zookeeper（分布式协调的中间件）对所有元数据进行存储维护。 redis 维护集群元数据采用另一个方式， gossip 协议，所有节点都持有一份元数据，不同的节点如果出现了元数据的变更，就不断将元数据发送给其它的节点，让其它节点也进行元数据的变更。 集中式的好处在于，元数据的读取和更新，时效性非常好，一旦元数据出现了变更，就立即更新到集中式的存储中，其它节点读取的时候就可以感知到；不好在于，所有的元数据的更新压力全部集中在一个地方，可能会导致元数据的存储有压力。 gossip 好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续打到所有节点上去更新，降低了压力；不好在于，元数据的更新有延时，可能导致集群中的一些操作会有一些滞后。 10000 端口：每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+10000，比如 7001，那么用于节点间通信的就是 17001 端口。每个节点每隔一段时间都会往另外几个节点发送 ping 消息，同时其它几个节点接收到 ping 之后返回 pong。 交换的信息：信息包括故障信息，节点的增加和删除，hash slot 信息等等。 gossip 协议gossip 协议包含多种消息，包含 ping,pong,meet,fail 等等。 meet：某个节点发送 meet 给新加入的节点，让新节点加入集群中，然后新节点就会开始与其它节点进行通信。 1redis-trib.rb add-node 其实内部就是发送了一个 gossip meet 消息给新加入的节点，通知那个节点去加入我们的集群。 ping：每个节点都会频繁给其它节点发送 ping，其中包含自己的状态还有自己维护的集群元数据，互相通过 ping 交换元数据。 pong：返回 ping 和 meeet，包含自己的状态和其它信息，也用于信息广播和更新。 fail：某个节点判断另一个节点 fail 之后，就发送 fail 给其它节点，通知其它节点说，某个节点宕机啦。 ping 消息深入ping 时要携带一些元数据，如果很频繁，可能会加重网络负担。 每个节点每秒会执行 10 次 ping，每次会选择 5 个最久没有通信的其它节点。当然如果发现某个节点通信延时达到了 cluster_node_timeout / 2，那么立即发送 ping，避免数据交换延时过长，落后的时间太长了。比如说，两个节点之间都 10 分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。所以 cluster_node_timeout 可以调节，如果调得比较大，那么会降低 ping 的频率。 每次 ping，会带上自己节点的信息，还有就是带上 1/10 其它节点的信息，发送出去，进行交换。至少包含 3 个其它节点的信息，最多包含 总节点数减 2 个其它节点的信息。 分布式寻址算法 hash 算法（大量缓存重建） 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡） redis cluster 的 hash slot 算法 hash 算法来了一个 key，首先计算 hash 值，然后对节点数取模。然后打在不同的 master 节点上。一旦某一个 master 节点宕机，所有请求过来，都会基于最新的剩余 master 节点数去取模，尝试去取数据。这会导致大部分的请求过来，全部无法拿到有效的缓存，导致大量的流量涌入数据库。 一致性 hash 算法一致性 hash 算法将整个 hash 值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织，下一步将各个 master 节点（使用服务器的 ip 或主机名）进行 hash。这样就能确定每个节点在其哈希环上的位置。 来了一个 key，首先计算 hash 值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，遇到的第一个 master 节点就是 key 所在位置。 在一致性哈希算法中，如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其它不受影响。增加一个节点也同理。 燃鹅，一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成缓存热点的问题。为了解决这种热点问题，一致性 hash 算法引入了虚拟节点机制，即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。 redis cluster 的 hash slot 算法redis cluster 有固定的 16384 个 hash slot，对每个 key 计算 CRC16 值，然后对 16384 取模，可以获取 key 对应的 hash slot。 redis cluster 中每个 master 都会持有部分 slot，比如有 3 个 master，那么可能每个 master 持有 5000 多个 hash slot。hash slot 让 node 的增加和移除很简单，增加一个 master，就将其他 master 的 hash slot 移动部分过去，减少一个 master，就将它的 hash slot 移动到其他 master 上去。移动 hash slot 的成本是非常低的。客户端的 api，可以对指定的数据，让他们走同一个 hash slot，通过 hash tag 来实现。 任何一台机器宕机，另外两个节点，不影响的。因为 key 找的是 hash slot，不是机器。 redis cluster 的高可用与主备切换原理redis cluster 的高可用的原理，几乎跟哨兵是类似的。 判断节点宕机如果一个节点认为另外一个节点宕机，那么就是 pfail，主观宕机。如果多个节点都认为另外一个节点宕机了，那么就是 fail，客观宕机，跟哨兵的原理几乎一样，sdown，odown。 在 cluster-node-timeout 内，某个节点一直没有返回 pong，那么就被认为 pfail。 如果一个节点认为某个节点 pfail 了，那么会在 gossip ping 消息中，ping 给其他节点，如果超过半数的节点都认为 pfail 了，那么就会变成 fail。 从节点过滤对宕机的 master node，从其所有的 slave node 中，选择一个切换成 master node。 检查每个 slave node 与 master node 断开连接的时间，如果超过了 cluster-node-timeout * cluster-slave-validity-factor，那么就没有资格切换成 master。 从节点选举每个从节点，都根据自己对 master 复制数据的 offset，来设置一个选举时间，offset 越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举。 所有的 master node 开始 slave 选举投票，给要进行选举的 slave 进行投票，如果大部分 master node（N/2 + 1）都投票给了某个从节点，那么选举通过，那个从节点可以切换成 master。 从节点执行主备切换，从节点切换为主节点。 与哨兵比较整个流程跟哨兵相比，非常类似，所以说，redis cluster 功能强大，直接集成了 replication 和 sentinel 的功能。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis 的并发竞争问题是什么？]]></title>
    <url>%2F2018%2F07%2F01%2Fsubject-high-concurrency-redis-cas%2F</url>
    <content type="text"><![CDATA[剖析某个时刻，多个系统实例都去更新某个 key。可以基于 zookeeper 实现分布式锁。每个系统通过 zookeeper 获取分布式锁，确保同一时间，只能有一个系统实例在操作某个 key，别人都不允许读和写。 你要写入缓存的数据，都是从 mysql 里查出来的，都得写入 mysql 中，写入 mysql 中的时候必须保存一个时间戳，从 mysql 查出来的时候，时间戳也查出来。 每次要写之前，先判断一下当前这个 value 的时间戳是否比缓存里的 value 的时间戳要新。如果是的话，那么可以写，否则，就不能用旧的数据覆盖新的数据。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何做可靠的分布式锁，Redlock真的可行么]]></title>
    <url>%2F2018%2F06%2F08%2Fdb-redis-%E5%A6%82%E4%BD%95%E5%81%9A%E5%8F%AF%E9%9D%A0%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%EF%BC%8CRedlock%E7%9C%9F%E7%9A%84%E5%8F%AF%E8%A1%8C%E4%B9%88%2F</url>
    <content type="text"><![CDATA[本文是对 Martin Kleppmann 的文章 How to do distributed locking 部分内容的翻译和总结，上次写 Redlock 的原因就是看到了 Martin 的这篇文章，写得很好，特此翻译和总结。感兴趣的同学可以翻看原文，相信会收获良多。 开篇作者认为现在 Redis 逐渐被使用到数据管理领域，这个领域需要更强的数据一致性和耐久性，这使得他感到担心，因为这不是 Redis 最初设计的初衷（事实上这也是很多业界程序员的误区，越来越把 Redis 当成数据库在使用），其中基于 Redis 的分布式锁就是令人担心的其一。 Martin 指出首先你要明确你为什么使用分布式锁，为了性能还是正确性？为了帮你区分这二者，在这把锁 fail 了的时候你可以询问自己以下问题： 要性能的： 拥有这把锁使得你不会重复劳动（例如一个 job 做了两次），如果这把锁 fail 了，两个节点同时做了这个 Job，那么这个 Job 增加了你的成本。 要正确性的： 拥有锁可以防止并发操作污染你的系统或者数据，如果这把锁 fail 了两个节点同时操作了一份数据，结果可能是数据不一致、数据丢失、file 冲突等，会导致严重的后果。 上述二者都是需求锁的正确场景，但是你必须清楚自己是因为什么原因需要分布式锁。 如果你只是为了性能，那没必要用 Redlock，它成本高且复杂，你只用一个 Redis 实例也够了，最多加个从防止主挂了。当然，你使用单节点的 Redis 那么断电或者一些情况下，你会丢失锁，但是你的目的只是加速性能且断电这种事情不会经常发生，这并不是什么大问题。并且如果你使用了单节点 Redis，那么很显然你这个应用需要的锁粒度是很模糊粗糙的，也不会是什么重要的服务。 那么是否 Redlock 对于要求正确性的场景就合适呢？Martin 列举了若干场景证明 Redlock 这种算法是不可靠的。 用锁保护资源这节里 Martin 先将 Redlock 放在了一边而是仅讨论总体上一个分布式锁是怎么工作的。在分布式环境下，锁比 mutex 这类复杂，因为涉及到不同节点、网络通信并且他们随时可能无征兆的 fail 。Martin 假设了一个场景，一个 client 要修改一个文件，它先申请得到锁，然后修改文件写回，放锁。另一个 client 再申请锁 … 代码流程如下： 123456789101112131415// THIS CODE IS BROKENfunction writeData(filename, data) &#123; var lock = lockService.acquireLock(filename); if (!lock) &#123; throw 'Failed to acquire lock'; &#125; try &#123; var file = storage.readFile(filename); var updated = updateContents(file, data); storage.writeFile(filename, updated); &#125; finally &#123; lock.release(); &#125;&#125; 可惜即使你的锁服务非常完美，上述代码还是可能跪，下面的流程图会告诉你为什么： 上述图中，得到锁的 client1 在持有锁的期间 pause 了一段时间，例如 GC 停顿。锁有过期时间（一般叫租约，为了防止某个 client 崩溃之后一直占有锁），但是如果 GC 停顿太长超过了锁租约时间，此时锁已经被另一个 client2 所得到，原先的 client1 还没有感知到锁过期，那么奇怪的结果就会发生，曾经 HBase 就发生过这种 Bug。即使你在 client1 写回之前检查一下锁是否过期也无助于解决这个问题，因为 GC 可能在任何时候发生，即使是你非常不便的时候（在最后的检查与写操作期间）。如果你认为自己的程序不会有长时间的 GC 停顿，还有其他原因会导致你的进程 pause。例如进程可能读取尚未进入内存的数据，所以它得到一个 page fault 并且等待 page 被加载进缓存；还有可能你依赖于网络服务；或者其他进程占用 CPU；或者其他人意外发生 SIGSTOP 等。 … …. 这里 Martin 又增加了一节列举各种进程 pause 的例子，为了证明上面的代码是不安全的，无论你的锁服务多完美。 使用 Fencing （栅栏）使得锁变安全修复问题的方法也很简单：你需要在每次写操作时加入一个 fencing token。这个场景下，fencing token 可以是一个递增的数字（lock service 可以做到），每次有 client 申请锁就递增一次： client1 申请锁同时拿到 token33，然后它进入长时间的停顿锁也过期了。client2 得到锁和 token34 写入数据，紧接着 client1 活过来之后尝试写入数据，自身 token33 比 34 小因此写入操作被拒绝。注意这需要存储层来检查 token，但这并不难实现。如果你使用 Zookeeper 作为 lock service 的话那么你可以使用 zxid 作为递增数字。但是对于 Redlock 你要知道，没什么生成 fencing token 的方式，并且怎么修改 Redlock 算法使其能产生 fencing token 呢？好像并不那么显而易见。因为产生 token 需要单调递增，除非在单节点 Redis 上完成但是这又没有高可靠性，你好像需要引进一致性协议来让 Redlock 产生可靠的 fencing token。 使用时间来解决一致性Redlock 无法产生 fencing token 早该成为在需求正确性的场景下弃用它的理由，但还有一些值得讨论的地方。 学术界有个说法，算法对时间不做假设：因为进程可能pause一段时间、数据包可能因为网络延迟延后到达、时钟可能根本就是错的。而可靠的算法依旧要在上述假设下做正确的事情。 对于 failure detector 来说，timeout 只能作为猜测某个节点 fail 的依据，因为网络延迟、本地时钟不正确等其他原因的限制。考虑到 Redis 使用 gettimeofday，而不是单调的时钟，会受到系统时间的影响，可能会突然前进或者后退一段时间，这会导致一个 key 更快或更慢地过期。 可见，Redlock 依赖于许多时间假设，它假设所有 Redis 节点都能对同一个 Key 在其过期前持有差不多的时间、跟过期时间相比网络延迟很小、跟过期时间相比进程 pause 很短。 用不可靠的时间打破 Redlock这节 Martin 举了个因为时间问题，Redlock 不可靠的例子。 client1 从 ABC 三个节点处申请到锁，DE由于网络原因请求没有到达 C节点的时钟往前推了，导致 lock 过期 client2 在CDE处获得了锁，AB由于网络原因请求未到达 此时 client1 和 client2 都获得了锁 在 Redlock 官方文档中也提到了这个情况，不过是C崩溃的时候，Redlock 官方本身也是知道 Redlock 算法不是完全可靠的，官方为了解决这种问题建议使用延时启动，相关内容可以看之前的这篇文章。但是 Martin 这里分析得更加全面，指出延时启动不也是依赖于时钟的正确性的么？ 接下来 Martin 又列举了进程 Pause 时而不是时钟不可靠时会发生的问题： client1 从 ABCDE 处获得了锁 当获得锁的 response 还没到达 client1 时 client1 进入 GC 停顿 停顿期间锁已经过期了 client2 在 ABCDE 处获得了锁 client1 GC 完成收到了获得锁的 response，此时两个 client 又拿到了同一把锁 同时长时间的网络延迟也有可能导致同样的问题。 Redlock 的同步性假设这些例子说明了，仅有在你假设了一个同步性系统模型的基础上，Redlock 才能正常工作，也就是系统能满足以下属性： 网络延时边界，即假设数据包一定能在某个最大延时之内到达 进程停顿边界，即进程停顿一定在某个最大时间之内 时钟错误边界，即不会从一个坏的 NTP 服务器处取得时间 结论Martin 认为 Redlock 实在不是一个好的选择，对于需求性能的分布式锁应用它太重了且成本高；对于需求正确性的应用来说它不够安全。因为它对高危的时钟或者说其他上述列举的情况进行了不可靠的假设，如果你的应用只需要高性能的分布式锁不要求多高的正确性，那么单节点 Redis 够了；如果你的应用想要保住正确性，那么不建议 Redlock，建议使用一个合适的一致性协调系统，例如 Zookeeper，且保证存在 fencing token。]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RedLock分布式锁]]></title>
    <url>%2F2018%2F06%2F08%2Fdb-redis-Redlock%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[这篇文章主要是对 Redis 官方网站刊登的 Distributed locks with Redis 部分内容的总结和翻译。 什么是 RedLockRedis 官方站这篇文章提出了一种权威的基于 Redis 实现分布式锁的方式名叫 Redlock，此种方式比原先的单节点的方法更安全。它可以保证以下特性： 安全特性：互斥访问，即永远只有一个 client 能拿到锁 避免死锁：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区 容错性：只要大部分 Redis 节点存活就可以正常提供服务 怎么在单节点上实现分布式锁 SET resource_name my_random_value NX PX 30000 主要依靠上述命令，该命令仅当 Key 不存在时（NX保证）set 值，并且设置过期时间 3000ms （PX保证），值 my_random_value 必须是所有 client 和所有锁请求发生期间唯一的，释放锁的逻辑是： 12345if redis.call("get",KEYS[1]) == ARGV[1] then return redis.call("del",KEYS[1])else return 0end 上述实现可以避免释放另一个client创建的锁，如果只有 del 命令的话，那么如果 client1 拿到 lock1 之后因为某些操作阻塞了很长时间，此时 Redis 端 lock1 已经过期了并且已经被重新分配给了 client2，那么 client1 此时再去释放这把锁就会造成 client2 原本获取到的锁被 client1 无故释放了，但现在为每个 client 分配一个 unique 的 string 值可以避免这个问题。至于如何去生成这个 unique string，方法很多随意选择一种就行了。 Redlock 算法算法很易懂，起 5 个 master 节点，分布在不同的机房尽量保证可用性。为了获得锁，client 会进行如下操作： 得到当前的时间，微妙单位 尝试顺序地在 5 个实例上申请锁，当然需要使用相同的 key 和 random value，这里一个 client 需要合理设置与 master 节点沟通的 timeout 大小，避免长时间和一个 fail 了的节点浪费时间 当 client 在大于等于 3 个 master 上成功申请到锁的时候，且它会计算申请锁消耗了多少时间，这部分消耗的时间采用获得锁的当下时间减去第一步获得的时间戳得到，如果锁的持续时长（lock validity time）比流逝的时间多的话，那么锁就真正获取到了。 如果锁申请到了，那么锁真正的 lock validity time 应该是 origin（lock validity time） - 申请锁期间流逝的时间 如果 client 申请锁失败了，那么它就会在少部分申请成功锁的 master 节点上执行释放锁的操作，重置状态 失败重试如果一个 client 申请锁失败了，那么它需要稍等一会在重试避免多个 client 同时申请锁的情况，最好的情况是一个 client 需要几乎同时向 5 个 master 发起锁申请。另外就是如果 client 申请锁失败了它需要尽快在它曾经申请到锁的 master 上执行 unlock 操作，便于其他 client 获得这把锁，避免这些锁过期造成的时间浪费，当然如果这时候网络分区使得 client 无法联系上这些 master，那么这种浪费就是不得不付出的代价了。 放锁放锁操作很简单，就是依次释放所有节点上的锁就行了 性能、崩溃恢复和 fsync如果我们的节点没有持久化机制，client 从 5 个 master 中的 3 个处获得了锁，然后其中一个重启了，这是注意 整个环境中又出现了 3 个 master 可供另一个 client 申请同一把锁！ 违反了互斥性。如果我们开启了 AOF 持久化那么情况会稍微好转一些，因为 Redis 的过期机制是语义层面实现的，所以在 server 挂了的时候时间依旧在流逝，重启之后锁状态不会受到污染。但是考虑断电之后呢，AOF部分命令没来得及刷回磁盘直接丢失了，除非我们配置刷回策略为 fsnyc = always，但这会损伤性能。解决这个问题的方法是，当一个节点重启之后，我们规定在 max TTL 期间它是不可用的，这样它就不会干扰原本已经申请到的锁，等到它 crash 前的那部分锁都过期了，环境不存在历史锁了，那么再把这个节点加进来正常工作。]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何实现 MySQL 的读写分离？]]></title>
    <url>%2F2018%2F06%2F04%2Fsubject-high-concurrency-mysql-read-write-separation%2F</url>
    <content type="text"><![CDATA[如何实现 MySQL 的读写分离？其实很简单，就是基于主从复制架构，简单来说，就搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。 MySQL 主从复制原理的是啥？主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。 这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。 而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。 所以 MySQL 实际上在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。 这个所谓半同步复制，也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了。 所谓并行复制，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。 MySQL 主从同步延时问题（精华）以前线上确实处理过因为主从同步延时问题而导致的线上的 bug，属于小型的生产事故。 是这个么场景。有个同学是这样写代码逻辑的。先插入一条数据，再把它查出来，然后更新这条数据。在生产环境高峰期，写并发达到了 2000/s，这个时候，主从复制延时大概是在小几十毫秒。线上会发现，每天总有那么一些数据，我们期望更新一些重要的数据状态，但在高峰期时候却没更新。用户跟客服反馈，而客服就会反馈给我们。 我们通过 MySQL 命令： 1show status 查看 Seconds_Behind_Master，可以看到从库复制主库的数据落后了几 ms。 一般来说，如果主从延迟较为严重，有以下解决方案： 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。 打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。 重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。 如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你要是这么搞，读写分离的意义就丧失了。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事务隔离级别]]></title>
    <url>%2F2018%2F06%2F03%2Fdb-mysql-%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB-%E5%9B%BE%E6%96%87%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[什么是事务?事务是逻辑上的一组操作，要么都执行，要么都不执行。 事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。 事物的特性(ACID) 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的； 隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 并发事务带来的问题在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对统一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。 脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。 丢失修改（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。 不可重复读（Unrepeatableread）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。 幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。 不可重复度和幻读区别： 不可重复读的重点是修改，幻读的重点在于新增或者删除。 例1（同样的条件, 你读取过的数据, 再次读取出来发现值不一样了 ）：事务1中的A先生读取自己的工资为 1000的操作还没完成，事务2中的B先生就修改了A的工资为2000，导 致A再读自己的工资时工资变为 2000；这就是不可重复读。 例2（同样的条件, 第1次和第2次读出来的记录数不一样 ）：假某工资单表中工资大于3000的有4人，事务1读取了所有工资大于3000的人，共查到4条记录，这时事务2 又插入了一条工资大于3000的记录，事务1再次读取时查到的记录就变为了5条，这样就导致了幻读。 事务隔离级别SQL 标准定义了四个隔离级别： READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 隔离级别 脏读 不可重复读 幻影读 READ-UNCOMMITTED √ √ √ READ-COMMITTED × √ √ REPEATABLE-READ × × √ SERIALIZABLE × × × MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。我们可以通过SELECT @@tx_isolation;命令来查看 123456mysql&gt; SELECT @@tx_isolation;+-----------------+| @@tx_isolation |+-----------------+| REPEATABLE-READ |+-----------------+ 这里需要注意的是：与 SQL 标准不同的地方在于InnoDB 存储引擎在 REPEATABLE-READ（可重读）事务隔离级别下使用的是Next-Key Lock 锁算法，因此可以避免幻读的产生，这与其他数据库系统(如 SQL Server)是不同的。所以说InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读） 已经可以完全保证事务的隔离性要求，即达到了 SQL标准的SERIALIZABLE(可串行化)隔离级别。 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是READ-COMMITTED(读取提交内容):，但是你要知道的是InnoDB 存储引擎默认使用 REPEATABLE-READ（可重读）并不会有任何性能损失。 InnoDB 存储引擎在 分布式事务 的情况下一般会用到SERIALIZABLE(可串行化)隔离级别。 实际情况演示在下面我会使用 2 个命令行mysql ，模拟多线程（多事务）对同一份数据的脏读问题。 MySQL 命令行的默认配置中事务都是自动提交的，即执行SQL语句后就会马上执行 COMMIT 操作。如果要显式地开启一个事务需要使用命令：START TARNSACTION。 我们可以通过下面的命令来设置隔离级别。 1SET [SESSION|GLOBAL] TRANSACTION ISOLATION LEVEL [READ UNCOMMITTED|READ COMMITTED|REPEATABLE READ|SERIALIZABLE] 我们再来看一下我们在下面实际操作中使用到的一些并发控制语句: START TARNSACTION |BEGIN：显式地开启一个事务。 COMMIT：提交事务，使得对数据库做的所有修改成为永久性。 ROLLBACK：回滚会结束用户的事务，并撤销正在进行的所有未提交的修改。 脏读(读未提交) 避免脏读(读已提交) 不可重复读还是刚才上面的读已提交的图，虽然避免了读未提交，但是却出现了，一个事务还没有结束，就发生了 不可重复读问题。 可重复读 防止幻读(可重复读) 一个事务对数据库进行操作，这种操作的范围是数据库的全部行，然后第二个事务也在对这个数据库操作，这种操作可以是插入一行记录或删除一行记录，那么第一个是事务就会觉得自己出现了幻觉，怎么还有没有处理的记录呢? 或者 怎么多处理了一行记录呢? 幻读和不可重复读有些相似之处 ，但是不可重复读的重点是修改，幻读的重点在于新增或者删除。]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何设计可以动态扩容缩容的分库分表方案？]]></title>
    <url>%2F2018%2F06%2F01%2Fsubject-high-concurrency-database-shard-dynamic-expand%2F</url>
    <content type="text"><![CDATA[停机扩容（不推荐）这个方案就跟停机迁移一样，步骤几乎一致，唯一的一点就是那个导数的工具，是把现有库表的数据抽出来慢慢倒入到新的库和表里去。但是最好别这么玩儿，有点不太靠谱，因为既然分库分表就说明数据量实在是太大了，可能多达几亿条，甚至几十亿，你这么玩儿，可能会出问题。 从单库单表迁移到分库分表的时候，数据量并不是很大，单表最大也就两三千万。那么你写个工具，多弄几台机器并行跑，1小时数据就导完了。这没有问题。 如果 3 个库 + 12 个表，跑了一段时间了，数据量都 1~2 亿了。光是导 2 亿数据，都要导个几个小时，6 点，刚刚导完数据，还要搞后续的修改配置，重启系统，测试验证，10 点才可以搞完。所以不能这么搞。 优化后的方案一开始上来就是 32 个库，每个库 32 个表，那么总共是 1024 张表。 我可以告诉各位同学，这个分法，第一，基本上国内的互联网肯定都是够用了，第二，无论是并发支撑还是数据量支撑都没问题。 每个库正常承载的写入并发量是 1000，那么 32 个库就可以承载32 * 1000 = 32000 的写并发，如果每个库承载 1500 的写并发，32 * 1500 = 48000 的写并发，接近 5 万每秒的写入并发，前面再加一个MQ，削峰，每秒写入 MQ 8 万条数据，每秒消费 5 万条数据。 有些除非是国内排名非常靠前的这些公司，他们的最核心的系统的数据库，可能会出现几百台数据库的这么一个规模，128个库，256个库，512个库。 1024 张表，假设每个表放 500 万数据，在 MySQL 里可以放 50 亿条数据。 每秒 5 万的写并发，总共 50 亿条数据，对于国内大部分的互联网公司来说，其实一般来说都够了。 谈分库分表的扩容，第一次分库分表，就一次性给他分个够，32 个库，1024 张表，可能对大部分的中小型互联网公司来说，已经可以支撑好几年了。 一个实践是利用 32 * 32 来分库分表，即分为 32 个库，每个库里一个表分为 32 张表。一共就是 1024 张表。根据某个 id 先根据 32 取模路由到库，再根据 32 取模路由到库里的表。 orderId id % 32 (库) id / 32 % 32 (表) 259 3 8 1189 5 5 352 0 11 4593 17 15 刚开始的时候，这个库可能就是逻辑库，建在一个数据库上的，就是一个mysql服务器可能建了 n 个库，比如 32 个库。后面如果要拆分，就是不断在库和 mysql 服务器之间做迁移就可以了。然后系统配合改一下配置即可。 比如说最多可以扩展到32个数据库服务器，每个数据库服务器是一个库。如果还是不够？最多可以扩展到 1024 个数据库服务器，每个数据库服务器上面一个库一个表。因为最多是1024个表。 这么搞，是不用自己写代码做数据迁移的，都交给 dba 来搞好了，但是 dba 确实是需要做一些库表迁移的工作，但是总比你自己写代码，然后抽数据导数据来的效率高得多吧。 哪怕是要减少库的数量，也很简单，其实说白了就是按倍数缩容就可以了，然后修改一下路由规则。 这里对步骤做一个总结： 设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是 32库 * 32表，对于大部分公司来说，可能几年都够了。 路由的规则，orderId 模 32 = 库，orderId / 32 模 32 = 表 扩容的时候，申请增加更多的数据库服务器，装好 mysql，呈倍数扩容，4 台服务器，扩到 8 台服务器，再到 16 台服务器。 由 dba 负责将原先数据库服务器的库，迁移到新的数据库服务器上去，库迁移是有一些便捷的工具的。 我们这边就是修改一下配置，调整迁移的库所在数据库服务器的地址。 重新发布系统，上线，原先的路由规则变都不用变，直接可以基于 n 倍的数据库服务器的资源，继续进行线上系统的提供服务。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker]]></title>
    <url>%2F2018%2F05%2F22%2Futils-docker-Docker%2F</url>
    <content type="text"><![CDATA[本文只是对Docker的概念做了较为详细的介绍，并不涉及一些像Docker环境的安装以及Docker的一些常见操作和命令。 Docker 是世界领先的软件容器平台，所以想要搞懂Docker的概念我们必须先从容器开始说起。 一 先从认识容器开始1.1 什么是容器?先来看看容器较为官方的解释一句话概括容器：容器就是将软件打包成标准化单元，以用于开发、交付和部署。 容器镜像是轻量的、可执行的独立软件包 ，包含软件运行所需的所有内容：代码、运行时环境、系统工具、系统库和设置。 容器化软件适用于基于Linux和Windows的应用，在任何环境中都能够始终如一地运行。 容器赋予了软件独立性 ，使其免受外在环境差异（例如，开发和预演环境的差异）的影响，从而有助于减少团队间在相同基础设施上运行不同软件时的冲突。 再来看看容器较为通俗的解释如果需要通俗的描述容器的话，我觉得容器就是一个存放东西的地方，就像书包可以装各种文具、衣柜可以放各种衣服、鞋架可以放各种鞋子一样。我们现在所说的容器存放的东西可能更偏向于应用比如网站、程序甚至是系统环境。 1.2 图解物理机,虚拟机与容器关于虚拟机与容器的对比在后面会详细介绍到，这里只是通过网上的图片加深大家对于物理机、虚拟机与容器这三者的理解。 物理机 虚拟机： 容器： 通过上面这三张抽象图，我们可以大概可以通过类比概括出： 容器虚拟化的是操作系统而不是硬件，容器之间是共享同一套操作系统资源的。虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统。因此容器的隔离级别会稍低一些。 相信通过上面的解释大家对于容器这个既陌生又熟悉的概念有了一个初步的认识，下面我们就来谈谈Docker的一些概念。 二 再来谈谈 Docker 的一些概念 2.1 什么是 Docker?说实话关于Docker是什么并太好说，下面我通过四点向你说明Docker到底是个什么东西。 Docker 是世界领先的软件容器平台。 Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核 的cgroup，namespace，以及AUFS类的UnionFS等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。 由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。Docke最初实现是基于 LXC. Docker 能够自动执行重复性任务，例如搭建和配置开发环境，从而解放了开发人员以便他们专注在真正重要的事情上：构建杰出的软件。 用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。 2.2 Docker 思想 集装箱 标准化： ①运输方式 ② 存储方式 ③ API接口 隔离 2.3 Docker 容器的特点 轻量在一台机器上运行的多个 Docker 容器可以共享这台机器的操作系统内核；它们能够迅速启动，只需占用很少的计算和内存资源。镜像是通过文件系统层进行构造的，并共享一些公共文件。这样就能尽量降低磁盘用量，并能更快地下载镜像。 标准Docker 容器基于开放式标准，能够在所有主流 Linux 版本、Microsoft Windows 以及包括 VM、裸机服务器和云在内的任何基础设施上运行。 安全Docker 赋予应用的隔离性不仅限于彼此隔离，还独立于底层的基础设施。Docker 默认提供最强的隔离，因此应用出现问题，也只是单个容器的问题，而不会波及到整台机器。 2.4 为什么要用 Docker ? Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 “这段代码在我机器上没问题啊” 这类问题；——一致的运行环境 可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间。——更快速的启动时间 避免公用的服务器，资源会容易受到其他用户的影响。——隔离性 善于处理集中爆发的服务器使用压力；——弹性伸缩，快速扩展 可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。——迁移方便 使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。——持续交付和部署 每当说起容器，我们不得不将其与虚拟机做一个比较。就我而言，对于两者无所谓谁会取代谁，而是两者可以和谐共存。 三 容器 VS 虚拟机 简单来说： 容器和虚拟机具有相似的资源隔离和分配优势，但功能有所不同，因为容器虚拟化的是操作系统，而不是硬件，因此容器更容易移植，效率也更高。 3.1 两者对比图 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便. 3.2 容器与虚拟机总结 容器是一个应用层抽象，用于将代码和依赖资源打包在一起。 多个容器可以在同一台机器上运行，共享操作系统内核，但各自作为独立的进程在用户空间中运行 。与虚拟机相比， 容器占用的空间较少（容器镜像大小通常只有几十兆），瞬间就能完成启动 。 虚拟机 (VM) 是一个物理硬件层抽象，用于将一台服务器变成多台服务器。 管理程序允许多个 VM 在一台机器上运行。每个VM都包含一整套操作系统、一个或多个应用、必要的二进制文件和库资源，因此 占用大量空间 。而且 VM 启动也十分缓慢 。 通过Docker官网，我们知道了这么多Docker的优势，但是大家也没有必要完全否定虚拟机技术，因为两者有不同的使用场景。虚拟机更擅长于彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而 Docker通常用于隔离不同的应用 ，例如前端，后端以及数据库。 3.3 容器与虚拟机两者是可以共存的就我而言，对于两者无所谓谁会取代谁，而是两者可以和谐共存。 Docker中非常重要的三个基本概念，理解了这三个概念，就理解了 Docker 的整个生命周期。 四 Docker基本概念Docker 包括三个基本概念 镜像（Image） 容器（Container） 仓库（Repository） 理解了这三个概念，就理解了 Docker 的整个生命周期 4.1 镜像(Image):一个特殊的文件系统 操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而Docker 镜像（Image），就相当于是一个 root 文件系统。 Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。 镜像不包含任何动态数据，其内容在构建之后也不会被改变。 Docker 设计时，就充分利用 Union FS的技术，将其设计为 分层存储的架构 。 镜像实际是由多层文件系统联合组成。 镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。 比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。 分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。 4.2 容器(Container):镜像运行时的实体 镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等 。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。前面讲过镜像使用的是分层存储，容器也是如此。 容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。 按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据 ，容器存储层要保持无状态化。所有的文件写入操作，都应该使用数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主(或网络存储)发生读写，其性能和稳定性更高。数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此， 使用数据卷后，容器可以随意删除、重新 run ，数据却不会丢失。 4.3仓库(Repository):集中存放镜像文件的地方 镜像构建完成后，可以很容易的在当前宿主上运行，但是， 如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry就是这样的服务。 一个 Docker Registry中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。所以说：镜像仓库是Docker用来集中存放镜像文件的地方类似于我们之前常用的代码仓库。 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本 。我们可以通过&lt;仓库名&gt;:&lt;标签&gt;的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签.。 这里补充一下Docker Registry 公开服务和私有 Docker Registry的概念： Docker Registry 公开服务 是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。 最常使用的 Registry 公开服务是官方的 Docker Hub ，这也是默认的 Registry，并拥有大量的高质量的官方镜像，网址为：https://hub.docker.com/ 。在国内访问Docker Hub 可能会比较慢国内也有一些云服务商提供类似于 Docker Hub 的公开服务。比如 时速云镜像库、网易云镜像服务、DaoCloud 镜像市场、阿里云镜像库等。 除了使用公开服务外，用户还可以在 本地搭建私有 Docker Registry 。Docker 官方提供了 Docker Registry 镜像，可以直接使用做为私有 Registry 服务。开源的 Docker Registry 镜像只提供了 Docker Registry API 的服务端实现，足以支持 docker 命令，不影响使用。但不包含图形界面，以及镜像维护、用户管理、访问控制等高级功能。 Docker的概念基本上已经讲完，最后我们谈谈：Build, Ship, and Run。 五 最后谈谈:Build Ship and Run如果你搜索Docker官网，会发现如下的字样：“Docker - Build, Ship, and Run Any App, Anywhere”。那么Build, Ship, and Run到底是在干什么呢？ Build（构建镜像） ： 镜像就像是集装箱包括文件以及运行环境等等资源。 Ship（运输镜像） ：主机和仓库间运输，这里的仓库就像是超级码头一样。 Run （运行镜像） ：运行的镜像就是一个容器，容器就是运行程序的地方。 Docker 运行过程也就是去仓库把镜像拉到本地，然后用一条命令把镜像运行起来变成容器。所以，我们也常常将Docker称为码头工人或码头装卸工，这和Docker的中文翻译搬运工人如出一辙。 六 总结本文主要把Docker中的一些常见概念做了详细的阐述，但是并不涉及Docker的安装、镜像的使用、容器的操作等内容。这部分东西，希望读者自己可以通过阅读书籍与官方文档的形式掌握。如果觉得官方文档阅读起来很费力的话，这里推荐一本书籍《Docker技术入门与实战第二版》。]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker镜像]]></title>
    <url>%2F2018%2F05%2F22%2Futils-docker-Docker-Image%2F</url>
    <content type="text"><![CDATA[镜像作为 Docker 三大核心概念中，最重要的一个关键词，它有很多操作，是您想学习容器技术不得不掌握的。本文将带您一步一步，图文并重，上手操作来学习它。 一 Docker 下载镜像如果我们想要在本地运行容器，就必须保证本地存在对应的镜像。所以，第一步，我们需要下载镜像。当我们尝试下载镜像时，Docker 会尝试先从默认的镜像仓库（默认使用 Docker Hub 公共仓库）去下载，当然了，用户也可以自定义配置想要下载的镜像仓库。 1.1 下载镜像镜像是运行容器的前提，我们可以使用 docker pull[IMAGE_NAME]:[TAG]命令来下载镜像，其中 IMAGE_NAME 表示的是镜像的名称，而 TAG 是镜像的标签，也就是说我们需要通过 “镜像 + 标签” 的方式来下载镜像。 注意： 您也可以不显式地指定 TAG, 它会默认下载 latest 标签，也就是下载仓库中最新版本的镜像。这里并不推荐您下载 latest 标签，因为该镜像的内容会跟踪镜像的最新版本，并随之变化，所以它是不稳定的。在生产环境中，可能会出现莫名其妙的 bug, 推荐您最好还是显示的指定具体的 TAG。 举个例子，如我们想要下载一个 Mysql 5.7 镜像，可以通过命令来下载： 1docker pull mysql:5.7 会看到控制台输出内容如下： 注意： 由于官方 DockerHub 仓库服务器在国外，下载速度较慢，所以我将仓库的地址更改成了国内的 docker.io 的镜像仓库，所以在上图中，镜像前面会有 docker.io 出现。 当有 Downloaded 字符串输出的时候，说明下载成功了！！ 1.2 验证让我们来验证一下，本地是否存在 Mysql5.7 的镜像，运行命令： 1docker images 可以看到本地的确存在该镜像，确实是下载成功了！ 1.3 下载镜像相关细节再说说上面下载镜像的过程： 通过下载过程，可以看到，一个镜像一般是由多个层（ layer） 组成，类似 f7e2b70d04ae这样的串表示层的唯一 ID（实际上完整的 ID 包括了 256 个 bit, 64 个十六进制字符组成）。 您可能会想，如果多个不同的镜像中，同时包含了同一个层（ layer）,这样重复下载，岂不是导致了存储空间的浪费么? 实际上，Docker 并不会这么傻会去下载重复的层（ layer）,Docker 在下载之前，会去检测本地是否会有同样 ID 的层，如果本地已经存在了，就直接使用本地的就好了。 另一个问题，不同仓库中，可能也会存在镜像重名的情况发生, 这种情况咋办？ 严格意义上，我们在使用 docker pull 命令时，还需要在镜像前面指定仓库地址( Registry), 如果不指定，则 Docker 会使用您默认配置的仓库地址。例如上面，由于我配置的是国内 docker.io 的仓库地址，我在 pull 的时候，docker 会默认为我加上 docker.io/library 的前缀。 如：当我执行 docker pull mysql:5.7 命令时，实际上相当于 docker pull docker.io/mysql:5.7，如果您未自定义配置仓库，则默认在下载的时候，会在镜像前面加上 DockerHub 的地址。 Docker 通过前缀地址的不同，来保证不同仓库中，重名镜像的唯一性。 1.4 PULL 子命令命令行中输入： 1docker pull --help 会得到如下信息： 1234[root@iZbp1j8y1bab0djl9gdp33Z ~]# docker pull --helpUsage: docker pull [OPTIONS] NAME[:TAG|@DIGEST]Pull an image or a repository from a registryOptions: -a, --all-tags Download all tagged images in the repository --disable-content-trust Skip image verification (default true) --help Print usage 我们可以看到主要支持的子命令有： -a,--all-tags=true|false: 是否获取仓库中所有镜像，默认为否； --disable-content-trust: 跳过镜像内容的校验，默认为 true; 二 Docker 查看镜像信息2.1 images 命令列出镜像通过使用如下两个命令，列出本机已有的镜像： 1docker images 或： 1docker image ls 如下图所示： 对上述红色标注的字段做一下解释： REPOSITORY: 来自于哪个仓库； TAG: 镜像的标签信息，比如 5.7、latest 表示不同的版本信息； IMAGE ID: 镜像的 ID, 如果您看到两个 ID 完全相同，那么实际上，它们指向的是同一个镜像，只是标签名称不同罢了； CREATED: 镜像最后的更新时间； SIZE: 镜像的大小，优秀的镜像一般体积都比较小，这也是我更倾向于使用轻量级的 alpine 版本的原因； 注意：图中的镜像大小信息只是逻辑上的大小信息，因为一个镜像是由多个镜像层（ layer）组成的，而相同的镜像层本地只会存储一份，所以，真实情况下，占用的物理存储空间大小，可能会小于逻辑大小。 2.2 使用 tag 命令为镜像添加标签通常情况下，为了方便在后续工作中，快速地找到某个镜像，我们可以使用 docker tag 命令，为本地镜像添加一个新的标签。如下图所示： 为 docker.io/mysql 镜像，添加新的镜像标签 allen_mysql:5.7。然后使用 docker images 命令，查看本地镜像： 可以看到，本地多了一个 allen_mysql:5.7 的镜像。细心的你一定还会发现， allen_mysql:5.7 和 docker.io/mysql:5.7 的镜像 ID 是一模一样的，说明它们是同一个镜像，只是别名不同而已。 docker tag 命令功能更像是, 为指定镜像添加快捷方式一样。 2.3 使用 inspect 命令查看镜像详细信息通过 docker inspect 命令，我们可以获取镜像的详细信息，其中，包括创建者，各层的数字摘要等。 1docker inspect docker.io/mysql:5.7 docker inspect 返回的是 JSON 格式的信息，如果您想获取其中指定的一项内容，可以通过 -f 来指定，如获取镜像大小： 1docker inspect -f &#123;&#123;&quot;.Size&quot;&#125;&#125; docker.io/mysql:5.7 2.4 使用 history 命令查看镜像历史前面的小节中，我们知道了，一个镜像是由多个层（layer）组成的，那么，我们要如何知道各个层的具体内容呢？ 通过 docker history 命令，可以列出各个层（layer）的创建信息，如我们查看 docker.io/mysql:5.7 的各层信息： 1docker history docker.io/mysql:5.7 可以看到，上面过长的信息，为了方便展示，后面都省略了，如果您想要看具体信息，可以通过添加 --no-trunc 选项，如下面命令： 1docker history --no-trunc docker.io/mysql:5.7 三 Docker 搜索镜像3.1 search 命令您可以通过下面命令进行搜索： 1docker search [option] keyword 比如，您想搜索仓库中 mysql 相关的镜像，可以输入如下命令： 1docker search mysql 3.2 search 子命令命令行输入 docker search--help, 输出如下： 123Usage: docker search [OPTIONS] TERMSearch the Docker Hub for imagesOptions: -f, --filter filter Filter output based on conditions provided --help Print usage --limit int Max number of search results (default 25) --no-index Don&apos;t truncate output --no-trunc Don&apos;t truncate output 可以看到 search 支持的子命令有： -f,--filter filter: 过滤输出的内容； --limitint：指定搜索内容展示个数; --no-index: 不截断输出内容； --no-trunc：不截断输出内容； 举个列子，比如我们想搜索官方提供的 mysql 镜像，命令如下： 1docker search --filter=is-offical=true mysql 再比如，我们想搜索 Stars 数超过 100 的 mysql 镜像： 1docker search --filter=stars=100 mysql 四 Docker 删除镜像4.1 通过标签删除镜像通过如下两个都可以删除镜像： 1docker rmi [image] 或者： 1docker image rm [image] 支持的子命令如下： -f,-force: 强制删除镜像，即便有容器引用该镜像； -no-prune: 不要删除未带标签的父镜像； Docker 查看镜像信息 例如，我们想删除上章节创建的 allen_mysql:5.7 镜像，命令如下： 1docker rmi allen_mysql:5.7 从上面章节中，我们知道 allen_mysql:5.7 和 docker.io/mysql:5.7 实际上指向的是同一个镜像，那么，您可以能会有疑问，我删除了 allen_mysql:5.7, 会不会将 docker.io/mysql:5.7 镜像也给删除了？ 实际上，当同一个镜像拥有多个标签时，执行 docker rmi 命令，只是会删除了该镜像众多标签中，您指定的标签而已，并不会影响原始的那个镜像文件。 不信的话，我们可以执行 docker images 命令，来看下 docker.io/mysql:5.7 镜像还在不在： 可以看到， docker.io/mysql:5.7 镜像依然存在！ 那么，如果某个镜像不存在多个标签，当且仅当只有一个标签时，执行删除命令时，您就要小心了，这会彻底删除镜像。 例如，这个时候，我们再执行 docker rmi docker.io/mysql:5.7 命令： 从上图可以看到，我们已经删除了 docker.io/mysql:5.7 镜像的所有文件层。该镜像在本地已不复存在了！ 4.2 通过 ID 删除镜像除了通过标签名称来删除镜像，我们还可以通过制定镜像 ID, 来删除镜像，如： 1docker rmi ee7cbd482336 一旦制定了通过 ID 来删除镜像，它会先尝试删除所有指向该镜像的标签，然后在删除镜像本身。 4.3 删除镜像的限制删除镜像很简单，但也不是我们何时何地都能删除的，它存在一些限制条件。 当通过该镜像创建的容器未被销毁时，镜像是无法被删除的。为了验证这一点，我们来做个试验。首先，我们通过 docker pull alpine 命令，拉取一个最新的 alpine 镜像, 然后启动镜像，让其输出 hello,docker!: 接下来，我们来删除这个镜像试试： 可以看到提示信息，无法删除该镜像，因为有容器正在引用他！同时，这段信息还告诉我们，除非通过添加 -f 子命令，也就是强制删除，才能移除掉该镜像！ 1docker rmi -f docker.io/alpine 但是，我们一般不推荐这样暴力的做法，正确的做法应该是： 先删除引用这个镜像的容器； 再删除这个镜像； 也就是，根据上图中提示的，引用该镜像的容器 ID ( 9d59e2278553), 执行删除命令： 1docker rm 9d59e2278553 然后，再执行删除镜像的命令： 1docker rmi 5cb3aa00f899 Docker 删除镜像 这个时候，就能正常删除了！ 4.4 清理镜像我们在使用 Docker 一段时间后，系统一般都会残存一些临时的、没有被使用的镜像文件，可以通过以下命令进行清理： 1docker image prune 它支持的子命令有： -a,--all: 删除所有没有用的镜像，而不仅仅是临时文件； -f,--force：强制删除镜像文件，无需弹出提示确认； 五 Docker 创建镜像此小节中，您将学习 Docker 如何创建镜像？Docker 创建镜像主要有三种： 基于已有的镜像创建； 基于 Dockerfile 来创建； 基于本地模板来导入； 我们将主要介绍常用的 1，2 两种。 5.1 基于已有的镜像创建通过如下命令来创建： 1docker container commit 支持的子命令如下： -a,--author=””: 作者信息； -c,--change=[]: 可以在提交的时候执行 Dockerfile 指令，如 CMD、ENTRYPOINT、ENV、EXPOSE、LABEL、ONBUILD、USER、VOLUME、WORIR 等； -m,--message=””: 提交信息； -p,--pause=true: 提交时，暂停容器运行。 接下来，基于本地已有的 Ubuntu 镜像，创建一个新的镜像： 首先，让我将它运行起来，并在其中创建一个 test.txt 文件： 命令如下： 1docker run -it docker.io/ubuntu:latest /bin/bashroot@a0a0c8cfec3a:/# touch test.txtroot@a0a0c8cfec3a:/# exit 创建完 test.txt 文件后，需要记住标注的容器 ID: a0a0c8cfec3a, 用它来提交一个新的镜像(PS: 你也可以通过名称来提交镜像，这里只演示通过 ID 的方式)。 执行命令： 1docker container commit -m &quot;Added test.txt file&quot; -a &quot;Allen&quot; a0a0c8cfec3a test:0.1 提交成功后，会返回新创建的镜像 ID 信息，如下图所示： 再次查看本地镜像信息，可以看到新创建的 test:0.1 镜像了： 5.2 基于 Dockerfile 创建通过 Dockerfile 的方式来创建镜像，是最常见的一种方式了，也是比较推荐的方式。Dockerfile 是一个文本指令文件，它描述了是如何基于一个父镜像，来创建一个新镜像的过程。 下面让我们来编写一个简单的 Dockerfile 文件，它描述了基于 Ubuntu 父镜像，安装 Python3 环境的镜像： 123FROM docker.io/ubuntu:latestLABEL version=&quot;1.0&quot; maintainer=&quot;Allen &lt;weiwosuo@github&gt;&quot;RUN apt-get update &amp;&amp; \ apt-get install -y python3 &amp;&amp; \ apt-get clean &amp;&amp; \ rm -rf /var/lib/apt/lists/* 创建完成后，通过这个 Dockerfile 文件，来构建新的镜像，执行命令： 1docker image build -t python:3 . 注意： 命令的最后有个点，如果不加的话，会构建不成功 ！ 编译成功后，再次查看本地镜像信息，就可以看到新构建的 python:3 镜像了。 六 Docker 导出&amp;加载镜像此小节中，您将学习 Docker 如何导出&amp;加载镜像。 通常我们会有下面这种需求，需要将镜像分享给别人，这个时候，我们可以将镜像导出成 tar 包，别人直接通过加载这个 tar 包，快速地将镜像引入到本地镜像库。 要想使用这两个功能，主要是通过如下两个命令： docker save docker load 6.1 导出镜像查看本地镜像如下： 例如，我们想要将 python:3 镜像导出来，执行命令： 1docker save -o python_3.tar python:3 执行成功后，查看当前目录： Docker 导出文件 可以看到 python_3.tar 镜像文件已经生成。接下来，你可以将它通过复制的方式，分享给别人了！ 6.2 加载镜像别人拿到了这个 tar 包后，要如何导入到本地的镜像库呢？ 通过执行如下命令： 1docker load -i python_3.tar 或者： 1docker load &lt; python_3.tar 导入成功后，查看本地镜像信息，你就可以获得别人分享的镜像了！怎么样，是不是很方便呢！ 七 Docker 上传镜像我们将以上传到 Docker Hub 上为示例，演示 Docker 如何上传镜像。 7.1 获取 Docker ID想要上传镜像到 Docker Hub 上，首先，我们需要注册 Docker Hub 账号。打开 Docker Hub 网址 https://hub.docker.com，开始注册： 填写您的 Docker ID (也就是账号)，以及密码，Email, 点击继续。 接下来，Docker Hub 会发送验证邮件，到您填写的邮箱当中： 点击验证即可，接下来，再次返回 Docker Hub 官网，用您刚刚注册的 Docker ID 和密码来登录账号！ 7.2 创建镜像仓库登录成功后，会出现如下页面： 选择创建一个镜像仓库： 填写仓库名称、描述信息、是否公开后，点击创建。 仓库镜像展示页 我们看到，仓库已经创建成功了，但是里面还没有任何镜像，接下来开始上传镜像，到此新创建的仓库中。 7.3 上传镜像进入命令行，用我们刚刚获取的 Docker ID 以及密码登录，执行命令： 1docker login 命令行登录 Docker ID 登录成功后，我们开始准备上传本地的 python:3 镜像： 首先，我们对其打一个新的标签，前缀与我们新创建的 Docker ID 、仓库名保持一致: 1docker tag python:3 weiwosuoai1991/python:3 查看本地信息，可以看到，标签打成功了。接下开，开始上传！执行命令： 1docker push weiwosuoai1991/python:3 上传成功！去 Docker Hub 官网，新创建的仓库的信息页面验证一下，是否真的成功了： 仓库镜像展示页 大工告成！！！ 八 总结本文中，我们着重学习了 Docker 中下载镜像,、查看镜像信息、搜索镜像、删除镜像,、创建镜像、导出&amp;加载镜像以及向 Docker Hub 上传镜像的相关操作。]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目中缓存是如何使用的？]]></title>
    <url>%2F2018%2F05%2F05%2Fsubject-high-concurrency-why-cache%2F</url>
    <content type="text"><![CDATA[项目中缓存是如何使用的？这个，需要结合自己项目的业务来。 为什么要用缓存？用缓存，主要有两个用途：高性能、高并发。 高性能假设这么个场景，你有个操作，一个请求过来，吭哧吭哧你各种乱七八糟操作 mysql，半天查出来一个结果，耗时 600ms。但是这个结果可能接下来几个小时都不会变了，或者变了也可以不用立即反馈给用户。那么此时咋办？ 缓存啊，折腾 600ms 查出来的结果，扔缓存里，一个 key 对应一个 value，下次再有人查，别走 mysql 折腾 600ms 了，直接从缓存里，通过一个 key 查出来一个 value，2ms 搞定。性能提升 300 倍。 就是说对于一些需要复杂操作耗时查出来的结果，且确定后面不怎么变化，但是有很多读请求，那么直接将查询出来的结果放在缓存中，后面直接读缓存就好。 高并发mysql 这么重的数据库，压根儿设计不是让你玩儿高并发的，虽然也可以玩儿，但是天然支持不好。mysql 单机支撑到 2000QPS 也开始容易报警了。 所以要是你有个系统，高峰期一秒钟过来的请求有 1万，那一个 mysql 单机绝对会死掉。你这个时候就只能上缓存，把很多数据放缓存，别放 mysql。缓存功能简单，说白了就是 key-value 式操作，单机支撑的并发量轻松一秒几万十几万，支撑高并发 so easy。单机承载并发量是 mysql 单机的几十倍。 缓存是走内存的，内存天然就支撑高并发。 用了缓存之后会有什么不良后果？常见的缓存问题有以下几个： 缓存与数据库双写不一致 缓存雪崩、缓存穿透 缓存并发竞争 后面再详细说明。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么使用消息队列？]]></title>
    <url>%2F2018%2F05%2F05%2Fsubject-high-concurrency-why-mq%2F</url>
    <content type="text"><![CDATA[分析其实面试官主要是想看看： 第一，你知不知道你们系统里为什么要用消息队列这个东西？不少候选人，说自己项目里用了 Redis、MQ，但是其实他并不知道自己为什么要用这个东西。其实说白了，就是为了用而用，或者是别人设计的架构，他从头到尾都没思考过。没有对自己的架构问过为什么的人，一定是平时没有思考的人，面试官对这类候选人印象通常很不好。因为面试官担心你进了团队之后只会木头木脑的干呆活儿，不会自己思考。 第二，你既然用了消息队列这个东西，你知不知道用了有什么好处&amp;坏处？你要是没考虑过这个，那你盲目弄个 MQ 进系统里，后面出了问题你是不是就自己溜了给公司留坑？你要是没考虑过引入一个技术可能存在的弊端和风险，面试官把这类候选人招进来了，基本可能就是挖坑型选手。就怕你干 1 年挖一堆坑，自己跳槽了，给公司留下无穷后患。 第三，既然你用了 MQ，可能是某一种 MQ，那么你当时做没做过调研？你别傻乎乎的自己拍脑袋看个人喜好就瞎用了一个 MQ，比如 Kafka，甚至都从没调研过业界流行的 MQ 到底有哪几种。每一个 MQ 的优点和缺点是什么。每一个 MQ 没有绝对的好坏，但是就是看用在哪个场景可以扬长避短，利用其优势，规避其劣势。如果是一个不考虑技术选型的候选人招进了团队，leader 交给他一个任务，去设计个什么系统，他在里面用一些技术，可能都没考虑过选型，最后选的技术可能并不一定合适，一样是留坑。 面试题剖析为什么使用消息队列其实就是问问你消息队列都有哪些使用场景，然后你项目里具体是什么场景，说说你在这个场景里用消息队列是什么？ 面试官问你这个问题，期望的一个回答是说，你们公司有个什么业务场景，这个业务场景有个什么技术挑战，如果不用 MQ 可能会很麻烦，但是你现在用了 MQ 之后带给了你很多的好处。 先说一下消息队列常见的使用场景吧，其实场景有很多，但是比较核心的有 3 个：解耦、异步、削峰。 解耦看这么个场景。A 系统发送数据到 BCD 三个系统，通过接口调用发送。如果 E 系统也要这个数据呢？那如果 C 系统现在不需要了呢？A 系统负责人几乎崩溃…… 在这个场景中，A 系统跟其它各种乱七八糟的系统严重耦合，A 系统产生一条比较关键的数据，很多系统都需要 A 系统将这个数据发送过来。A 系统要时时刻刻考虑 BCDE 四个系统如果挂了该咋办？要不要重发，要不要把消息存起来？头发都白了啊！ 如果使用 MQ，A 系统产生一条数据，发送到 MQ 里面去，哪个系统需要数据自己去 MQ 里面消费。如果新系统需要数据，直接从 MQ 里消费即可；如果某个系统不需要这条数据了，就取消对 MQ 消息的消费即可。这样下来，A 系统压根儿不需要去考虑要给谁发送数据，不需要维护这个代码，也不需要考虑人家是否调用成功、失败超时等情况。 总结：通过一个 MQ，Pub/Sub 发布订阅消息这么一个模型，A 系统就跟其它系统彻底解耦了。 面试技巧：你需要去考虑一下你负责的系统中是否有类似的场景，就是一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦。但是其实这个调用是不需要直接同步调用接口的，如果用 MQ 给它异步化解耦，也是可以的，你就需要去考虑在你的项目里，是不是可以运用这个 MQ 去进行系统的解耦。在简历中体现出来这块东西，用 MQ 作解耦。 异步再来看一个场景，A 系统接收一个请求，需要在自己本地写库，还需要在 BCD 三个系统写库，自己本地写库要 3ms，BCD 三个系统分别写库要 300ms、450ms、200ms。最终请求总延时是 3 + 300 + 450 + 200 = 953ms，接近 1s，用户感觉搞个什么东西，慢死了慢死了。用户通过浏览器发起请求，等待个 1s，这几乎是不可接受的。 一般互联网类的企业，对于用户直接的操作，一般要求是每个请求都必须在 200 ms 以内完成，对用户几乎是无感知的。 如果使用 MQ，那么 A 系统连续发送 3 条消息到 MQ 队列中，假如耗时 5ms，A 系统从接受一个请求到返回响应给用户，总时长是 3 + 5 = 8ms，对于用户而言，其实感觉上就是点个按钮，8ms 以后就直接返回了，爽！网站做得真好，真快！ 削峰每天 0:00 到 12:00，A 系统风平浪静，每秒并发请求数量就 50 个。结果每次一到 12:00 ~ 13:00 ，每秒并发请求数量突然会暴增到 5k+ 条。但是系统是直接基于 MySQL 的，大量的请求涌入 MySQL，每秒钟对 MySQL 执行约 5k 条 SQL。 一般的 MySQL，扛到每秒 2k 个请求就差不多了，如果每秒请求到 5k 的话，可能就直接把 MySQL 给打死了，导致系统崩溃，用户也就没法再使用系统了。 但是高峰期一过，到了下午的时候，就成了低峰期，可能也就 1w 的用户同时在网站上操作，每秒中的请求数量可能也就 50 个请求，对整个系统几乎没有任何的压力。 如果使用 MQ，每秒 5k 个请求写入 MQ，A 系统每秒钟最多处理 2k 个请求，因为 MySQL 每秒钟最多处理 2k 个。A 系统从 MQ 中慢慢拉取请求，每秒钟就拉取 2k 个请求，不要超过自己每秒能处理的最大请求数量就 ok，这样下来，哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。 这个短暂的高峰期积压是 ok 的，因为高峰期过了之后，每秒钟就 50 个请求进 MQ，但是 A 系统依然会按照每秒 2k 个请求的速度在处理。所以说，只要高峰期一过，A 系统就会快速将积压的消息给解决掉。 消息队列有什么优缺点优点上面已经说了，就是在特殊场景下有其对应的好处，解耦、异步、削峰。 缺点有以下几个： 系统可用性降低系统引入的外部依赖越多，越容易挂掉。本来你就是 A 系统调用 BCD 三个系统的接口就好了，人 ABCD 四个系统好好的，没啥问题，你偏加个 MQ 进来，万一 MQ 挂了咋整，MQ 一挂，整套系统崩溃的，你不就完了？如何保证消息队列的高可用，可以点击这里查看。 系统复杂度提高硬生生加个 MQ 进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。 一致性问题A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。 所以消息队列实际是一种非常复杂的架构，你引入它有很多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉，做好之后，你会发现，妈呀，系统复杂度提升了一个数量级，也许是复杂了 10 倍。但是关键时刻，用，还是得用的。 Kafka、ActiveMQ、RabbitMQ、RocketMQ 有什么优缺点？ 特性 ActiveMQ RabbitMQ RocketMQ Kafka 单机吞吐量 万级，比 RocketMQ、Kafka 低一个数量级 同 ActiveMQ 10 万级，支撑高吞吐 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 topic 数量对吞吐量的影响 topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 时效性 ms 级 微秒级，这是 RabbitMQ 的一大特点，延迟最低 ms 级 延迟在 ms 级以内 可用性 高，基于主从架构实现高可用 同 ActiveMQ 非常高，分布式架构 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消息可靠性 有较低的概率丢失数据 基本不丢 经过参数优化配置，可以做到 0 丢失 同 RocketMQ 功能支持 MQ 领域的功能极其完备 基于 erlang 开发，并发能力很强，性能极好，延时很低 MQ 功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用 综上，各种对比之后，有如下建议： 一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了； 后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高； 不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。 所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。 如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql索引篇]]></title>
    <url>%2F2018%2F04%2F12%2Fdb-mysql-MySQL-Index%2F</url>
    <content type="text"><![CDATA[系列思维导图源文件（数据库+架构）以及思维导图制作软件—XMind8 破解安装，公众号后台回复：“思维导图” 免费领取！（下面的图片不是很清楚，原图非常清晰，另外提供给大家源文件也是为了大家根据自己需要进行修改） 下面是我补充的一些内容 为什么索引能提高查询速度 以下内容整理自： 地址： https://juejin.im/post/5b55b842f265da0f9e589e79 作者 ：Java3y 先从 MySQL 的基本存储结构说起MySQL的基本存储结构是页(记录都存在页里边)： 各个数据页可以组成一个双向链表 每个数据页中的记录又可以组成一个单向链表 每个数据页都会为存储在它里边儿的记录生成一个页目录，在通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录 以其他列(非主键)作为搜索条件：只能从最小记录开始依次遍历单链表中的每条记录。 所以说，如果我们写select * from user where indexname = ‘xxx’这样没有进行任何优化的sql语句，默认会这样做： 定位到记录所在的页：需要遍历双向链表，找到所在的页 从所在的页内中查找相应的记录：由于不是根据主键查询，只能遍历所在页的单链表了 很明显，在数据量很大的情况下这样查找会很慢！这样的时间复杂度为O（n）。 使用索引之后索引做了些什么可以让我们查询加快速度呢？其实就是将无序的数据变成有序(相对)： 要找到id为8的记录简要步骤： 很明显的是：没有用索引我们是需要遍历双向链表来定位对应的页，现在通过 “目录” 就可以很快地定位到对应的页上了！（二分查找，时间复杂度近似为O(logn)） 其实底层结构就是B+树，B+树作为树的一种实现，能够让我们很快地查找出对应的记录。 关于索引其他重要的内容补充 以下内容整理自：《Java工程师修炼之道》 最左前缀原则MySQL中的索引可以以一定顺序引用多列，这种索引叫作联合索引。如User表的name和city加联合索引就是(name,city)，而最左前缀原则指的是，如果查询的时候查询条件精确匹配索引的左边连续一列或几列，则此列就可以被用到。如下： 123select * from user where name=xx and city=xx ; ／／可以命中索引select * from user where name=xx ; // 可以命中索引select * from user where city=xx ; // 无法命中索引 这里需要注意的是，查询的时候如果两个条件都用上了，但是顺序不同，如 city= xx and name ＝xx，那么现在的查询引擎会自动优化为匹配联合索引的顺序，这样是能够命中索引的。 由于最左前缀原则，在创建联合索引时，索引字段的顺序需要考虑字段值去重之后的个数，较多的放前面。ORDER BY子句也遵循此规则。 注意避免冗余索引冗余索引指的是索引的功能相同，能够命中 就肯定能命中 ，那么 就是冗余索引如（name,city ）和（name ）这两个索引就是冗余索引，能够命中后者的查询肯定是能够命中前者的 在大多数情况下，都应该尽量扩展已有的索引而不是创建新索引。 MySQLS.7 版本后，可以通过查询 sys 库的 schema_redundant_indexes 表来查看冗余索引 Mysql如何为表字段添加索引？？？1.添加PRIMARY KEY（主键索引） 1ALTER TABLE `table_name` ADD PRIMARY KEY ( `column` ) 2.添加UNIQUE(唯一索引) 1ALTER TABLE `table_name` ADD UNIQUE ( `column` ) 3.添加INDEX(普通索引) 1ALTER TABLE `table_name` ADD INDEX index_name ( `column` ) 4.添加FULLTEXT(全文索引) 1ALTER TABLE `table_name` ADD FULLTEXT ( `column`) 5.添加多列索引 1ALTER TABLE `table_name` ADD INDEX index_name ( `column1`, `column2`, `column3` ) 参考 《Java工程师修炼之道》 《MySQL高性能书籍_第3版》 https://juejin.im/post/5b55b842f265da0f9e589e79]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es 写入数据的工作原理]]></title>
    <url>%2F2018%2F03%2F03%2Fsubject-high-concurrency-es-write-query-search%2F</url>
    <content type="text"><![CDATA[es 写数据过程 客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node（协调节点）。 coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node。 coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。 es 读数据过程可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。 客户端发送请求到任意一个 node，成为 coordinate node。 coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。 接收请求的 node 返回 document 给 coordinate node。 coordinate node 返回 document 给客户端。 es 搜索数据过程es 最强大的是做全文检索，就是比如你有三条数据： 123java真好玩儿啊java好难学啊j2ee特别牛 你根据 java 关键词来搜索，将包含 java的 document 给搜索出来。es 就会给你返回：java真好玩儿啊，java好难学啊。 客户端发送请求到一个 coordinate node。 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard，都可以。 query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。 fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。 写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。 写数据底层原理 先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。 如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file 中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。这个过程就是 refresh。 每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file，每秒钟会产生一个新的磁盘文件 segment file，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。 但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。 操作系统里面，磁盘文件其实都有一个东西，叫做 os cache，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache中，这个数据就可以被搜索到了。 为什么叫 es 是准实时的？ NRT，全称 near real-time。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api 或者 java api，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。 重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file 中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。 commit 操作发生第一步，就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。 这个 commit 操作叫做 flush。默认 30 分钟自动执行一次 flush，但如果 translog 过大，也会触发 flush。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。 translog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。 translog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。 实际上你在这里，如果面试官没有问你 es 丢数据的问题，你可以在这里给面试官炫一把，你说，其实 es 第一是准实时的，数据写入 1 秒后可以搜索到；可能会丢失数据的。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的数据丢失。 总结一下，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。 数据写入 segment file 之后，同时就建立好了倒排索引。 删除/更新数据底层原理如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。 如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。 buffer 每 refresh 一次，就会产生一个 segment file，所以默认情况下是 1 秒钟一个 segment file，这样下来 segment file 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘，这里会写一个 commit point，标识所有新的 segment file，然后打开 segment file 供搜索使用，同时删除旧的 segment file。 底层 lucene简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 Java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。 通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。 倒排索引在搜索引擎中，每个文档都有一个对应的文档 ID，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。 那么，倒排索引就是关键词到文档 ID 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。 举个栗子。 有以下文档： DocId Doc 1 谷歌地图之父跳槽 Facebook 2 谷歌地图之父加盟 Facebook 3 谷歌地图创始人拉斯离开谷歌加盟 Facebook 4 谷歌地图之父跳槽 Facebook 与 Wave 项目取消有关 5 谷歌地图之父拉斯加盟社交网站 Facebook 对文档进行分词之后，得到以下倒排索引。 WordId Word DocIds 1 谷歌 1,2,3,4,5 2 地图 1,2,3,4,5 3 之父 1,2,4,5 4 跳槽 1,4 5 Facebook 1,2,3,4,5 6 加盟 2,3,5 7 创始人 3 8 拉斯 3,5 9 离开 3 10 与 4 .. .. .. 另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。 那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 Facebook，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。 要注意倒排索引的两个重要细节： 倒排索引中的所有词项对应一个或多个文档； 倒排索引中的词项根据字典顺序升序排列 上面只是一个简单的栗子，并没有严格按照字典顺序升序排列。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>es</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es 生产集群的部署架构是什么？]]></title>
    <url>%2F2018%2F03%2F03%2Fsubject-high-concurrency-es-production-cluster%2F</url>
    <content type="text"><![CDATA[其实这个问题没啥，如果你确实干过 es，那你肯定了解你们生产 es 集群的实际情况，部署了几台机器？有多少个索引？每个索引有多大数据量？每个索引给了多少个分片？你肯定知道！ 但是如果你确实没干过，也别虚，我给你说一个基本的版本，你到时候就简单说一下就好了。 es 生产集群我们部署了 5 台机器，每台机器是 6 核 64G 的，集群总内存是 320G。 我们 es 集群的日增量数据大概是 2000 万条，每天日增量数据大概是 500MB，每月增量数据大概是 6 亿，15G。目前系统已经运行了几个月，现在 es 集群里数据总量大概是 100G 左右。 目前线上有 5 个索引（这个结合你们自己业务来，看看自己有哪些数据可以放 es 的），每个索引的数据量大概是 20G，所以这个数据量之内，我们每个索引分配的是 8 个 shard，比默认的 5 个 shard 多了 3 个 shard。 大概就这么说一下就行了。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>es</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（数十亿级别）如何提高查询效率啊？]]></title>
    <url>%2F2018%2F03%2F03%2Fsubject-high-concurrency-es-optimizing-query-performance%2F</url>
    <content type="text"><![CDATA[说实话，es 性能优化是没有什么银弹的，啥意思呢？就是不要期待着随手调一个参数，就可以万能的应对所有的性能慢的场景。也许有的场景是你换个参数，或者调整一下语法，就可以搞定，但是绝对不是所有场景都可以这样。 性能优化的杀手锏——filesystem cache你往 es 里写的数据，实际上都写到磁盘文件里去了，查询的时候，操作系统会将磁盘文件里的数据自动缓存到 filesystem cache 里面去。 es 的搜索引擎严重依赖于底层的 filesystem cache，你如果给 filesystem cache 更多的内存，尽量让内存可以容纳所有的 idx segment file 索引数据文件，那么你搜索的时候就基本都是走内存的，性能会非常高。 性能差距究竟可以有多大？我们之前很多的测试和压测，如果走磁盘一般肯定上秒，搜索性能绝对是秒级别的，1秒、5秒、10秒。但如果是走 filesystem cache，是走纯内存的，那么一般来说性能比走磁盘要高一个数量级，基本上就是毫秒级的，从几毫秒到几百毫秒不等。 这里有个真实的案例。某个公司 es 节点有 3 台机器，每台机器看起来内存很多，64G，总内存就是 64 * 3 = 192G。每台机器给 es jvm heap 是 32G，那么剩下来留给 filesystem cache 的就是每台机器才 32G，总共集群里给 filesystem cache 的就是 32 * 3 = 96G 内存。而此时，整个磁盘上索引数据文件，在 3 台机器上一共占用了 1T 的磁盘容量，es 数据量是 1T，那么每台机器的数据量是 300G。这样性能好吗？ filesystem cache 的内存才 100G，十分之一的数据可以放内存，其他的都在磁盘，然后你执行搜索操作，大部分操作都是走磁盘，性能肯定差。 归根结底，你要让 es 性能要好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半。 根据我们自己的生产环境实践经验，最佳的情况下，是仅仅在 es 中就存少量的数据，就是你要用来搜索的那些索引，如果内存留给 filesystem cache 的是 100G，那么你就将索引数据控制在 100G 以内，这样的话，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在 1 秒以内。 比如说你现在有一行数据。id,name,age .... 30 个字段。但是你现在搜索，只需要根据 id,name,age 三个字段来搜索。如果你傻乎乎往 es 里写入一行数据所有的字段，就会导致说 90% 的数据是不用来搜索的，结果硬是占据了 es 机器上的 filesystem cache 的空间，单条数据的数据量越大，就会导致 filesystem cahce 能缓存的数据就越少。其实，仅仅写入 es 中要用来检索的少数几个字段就可以了，比如说就写入 es id,name,age 三个字段，然后你可以把其他的字段数据存在 mysql/hbase 里，我们一般是建议用 es + hbase 这么一个架构。 hbase 的特点是适用于海量数据的在线存储，就是对 hbase 可以写入海量数据，但是不要做复杂的搜索，做很简单的一些根据 id 或者范围进行查询的这么一个操作就可以了。从 es 中根据 name 和 age 去搜索，拿到的结果可能就 20 个 doc id，然后根据 doc id 到 hbase 里去查询每个 doc id 对应的完整的数据，给查出来，再返回给前端。 写入 es 的数据最好小于等于，或者是略微大于 es 的 filesystem cache 的内存容量。然后你从 es 检索可能就花费 20ms，然后再根据 es 返回的 id 去 hbase 里查询，查 20 条数据，可能也就耗费个 30ms，可能你原来那么玩儿，1T 数据都放 es，会每次查询都是 5~10s，现在可能性能就会很高，每次查询就是 50ms。 数据预热假如说，哪怕是你就按照上述的方案去做了，es 集群中每个机器写入的数据量还是超过了 filesystem cache 一倍，比如说你写入一台机器 60G 数据，结果 filesystem cache 就 30G，还是有 30G 数据留在了磁盘上。 其实可以做数据预热。 举个例子，拿微博来说，你可以把一些大V，平时看的人很多的数据，你自己提前后台搞个系统，每隔一会儿，自己的后台系统去搜索一下热数据，刷到 filesystem cache 里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快。 或者是电商，你可以将平时查看最多的一些商品，比如说 iphone 8，热数据提前后台搞个程序，每隔 1 分钟自己主动访问一次，刷到 filesystem cache 里去。 对于那些你觉得比较热的、经常会有人访问的数据，最好做一个专门的缓存预热子系统，就是对热数据每隔一段时间，就提前访问一下，让数据进入 filesystem cache 里面去。这样下次别人访问的时候，性能一定会好很多。 冷热分离es 可以做类似于 mysql 的水平拆分，就是说将大量的访问很少、频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引。最好是将冷数据写入一个索引中，然后热数据写入另外一个索引中，这样可以确保热数据在被预热之后，尽量都让他们留在 filesystem os cache 里，别让冷数据给冲刷掉。 你看，假设你有 6 台机器，2 个索引，一个放冷数据，一个放热数据，每个索引 3 个 shard。3 台机器放热数据 index，另外 3 台机器放冷数据 index。然后这样的话，你大量的时间是在访问热数据 index，热数据可能就占总数据量的 10%，此时数据量很少，几乎全都保留在 filesystem cache 里面了，就可以确保热数据的访问性能是很高的。但是对于冷数据而言，是在别的 index 里的，跟热数据 index 不在相同的机器上，大家互相之间都没什么联系了。如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就 10% 的人去访问冷数据，90% 的人在访问热数据，也无所谓了。 document 模型设计对于 MySQL，我们经常有一些复杂的关联查询。在 es 里该怎么玩儿，es 里面的复杂的关联查询尽量别用，一旦用了性能一般都不太好。 最好是先在 Java 系统里就完成关联，将关联好的数据直接写入 es 中。搜索的时候，就不需要利用 es 的搜索语法来完成 join 之类的关联搜索了。 document 模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。es 能支持的操作就那么多，不要考虑用 es 做一些它不好操作的事情。如果真的有那种操作，尽量在 document 模型设计的时候，写入的时候就完成。另外对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的。 分页性能优化es 的分页是较坑的，为啥呢？举个例子吧，假如你每页是 10 条数据，你现在要查询第 100 页，实际上是会把每个 shard 上存储的前 1000 条数据都查到一个协调节点上，如果你有个 5 个 shard，那么就有 5000 条数据，接着协调节点对这 5000 条数据进行一些合并、处理，再获取到最终第 100 页的 10 条数据。 分布式的，你要查第 100 页的 10 条数据，不可能说从 5 个 shard，每个 shard 就查 2 条数据，最后到协调节点合并成 10 条数据吧？你必须得从每个 shard 都查 1000 条数据过来，然后根据你的需求进行排序、筛选等等操作，最后再次分页，拿到里面第 100 页的数据。你翻页的时候，翻的越深，每个 shard 返回的数据就越多，而且协调节点处理的时间越长，非常坑爹。所以用 es 做分页的时候，你会发现越翻到后面，就越是慢。 我们之前也是遇到过这个问题，用 es 作分页，前几页就几十毫秒，翻到 10 页或者几十页的时候，基本上就要 5~10 秒才能查出来一页数据了。 有什么解决方案吗？ 不允许深度分页（默认深度分页性能很差）跟产品经理说，你系统不允许翻那么深的页，默认翻的越深，性能就越差。 类似于 app 里的推荐商品不断下拉出来一页一页的类似于微博中，下拉刷微博，刷出来一页一页的，你可以用 scroll api，关于如何使用，自行上网搜索。 scroll 会一次性给你生成所有数据的一个快照，然后每次滑动向后翻页就是通过游标 scroll_id 移动，获取下一页下一页这样子，性能会比上面说的那种分页性能要高很多很多，基本上都是毫秒级的。 但是，唯一的一点就是，这个适合于那种类似微博下拉翻页的，不能随意跳到任何一页的场景。也就是说，你不能先进入第 10 页，然后去第 120 页，然后又回到第 58 页，不能随意乱跳页。所以现在很多产品，都是不允许你随意翻页的，app，也有一些网站，做的就是你只能往下拉，一页一页的翻。 初始化时必须指定 scroll 参数，告诉 es 要保存此次搜索的上下文多长时间。你需要确保用户不会持续不断翻页翻几个小时，否则可能因为超时而失败。 除了用 scroll api，你也可以用 search_after 来做，search_after 的思想是使用前一页的结果来帮助检索下一页的数据，显然，这种方式也不允许你随意翻页，你只能一页页往后翻。初始化时，需要使用一个唯一值的字段作为 sort 字段。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>es</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es 的分布式架构原理]]></title>
    <url>%2F2018%2F03%2F03%2Fsubject-high-concurrency-es-architecture%2F</url>
    <content type="text"><![CDATA[ElasticSearch 设计的理念就是分布式搜索引擎，底层其实还是基于 lucene 的。核心思想就是在多台机器上启动多个 es 进程实例，组成了一个 es 集群。 es 中存储数据的基本单位是索引，比如说你现在要在 es 中存储一些订单数据，你就应该在 es 中创建一个索引 order_idx，所有的订单数据就都写到这个索引里面去，一个索引差不多就是相当于是 mysql 里的一张表。 1index -&gt; type -&gt; mapping -&gt; document -&gt; field。 这样吧，为了做个更直白的介绍，我在这里做个类比。但是切记，不要划等号，类比只是为了便于理解。 index 相当于 mysql 里的一张表。而 type 没法跟 mysql 里去对比，一个 index 里可以有多个 type，每个 type 的字段都是差不多的，但是有一些略微的差别。假设有一个 index，是订单 index，里面专门是放订单数据的。就好比说你在 mysql 中建表，有些订单是实物商品的订单，比如一件衣服、一双鞋子；有些订单是虚拟商品的订单，比如游戏点卡，话费充值。就两种订单大部分字段是一样的，但是少部分字段可能有略微的一些差别。 所以就会在订单 index 里，建两个 type，一个是实物商品订单 type，一个是虚拟商品订单 type，这两个 type 大部分字段是一样的，少部分字段是不一样的。 很多情况下，一个 index 里可能就一个 type，但是确实如果说是一个 index 里有多个 type 的情况（注意，mapping types 这个概念在 ElasticSearch 7.X 已被完全移除，详细说明可以参考官方文档），你可以认为 index 是一个类别的表，具体的每个 type 代表了 mysql 中的一个表。每个 type 有一个 mapping，如果你认为一个 type 是具体的一个表，index 就代表多个 type 同属于的一个类型，而 mapping 就是这个 type 的表结构定义，你在 mysql 中创建一个表，肯定是要定义表结构的，里面有哪些字段，每个字段是什么类型。实际上你往 index 里的一个 type 里面写的一条数据，叫做一条 document，一条 document 就代表了 mysql 中某个表里的一行，每个 document 有多个 field，每个 field 就代表了这个 document 中的一个字段的值。 你搞一个索引，这个索引可以拆分成多个 shard，每个 shard 存储部分数据。拆分多个 shard 是有好处的，一是支持横向扩展，比如你数据量是 3T，3 个 shard，每个 shard 就 1T 的数据，若现在数据量增加到 4T，怎么扩展，很简单，重新建一个有 4 个 shard 的索引，将数据导进去；二是提高性能，数据分布在多个 shard，即多台服务器上，所有的操作，都会在多台机器上并行分布式执行，提高了吞吐量和性能。 接着就是这个 shard 的数据实际是有多个备份，就是说每个 shard 都有一个 primary shard，负责写入数据，但是还有几个 replica shard。primary shard 写入数据之后，会将数据同步到其他几个 replica shard 上去。 通过这个 replica 的方案，每个 shard 的数据都有多个备份，如果某个机器宕机了，没关系啊，还有别的数据副本在别的机器上呢。高可用了吧。 es 集群多个节点，会自动选举一个节点为 master 节点，这个 master 节点其实就是干一些管理的工作的，比如维护索引元数据、负责切换 primary shard 和 replica shard 身份等。要是 master 节点宕机了，那么会重新选举一个节点为 master 节点。 如果是非 master节点宕机了，那么会由 master 节点，让那个宕机节点上的 primary shard 的身份转移到其他机器上的 replica shard。接着你要是修复了那个宕机机器，重启了之后，master 节点会控制将缺失的 replica shard 分配过去，同步后续修改的数据之类的，让集群恢复正常。 说得更简单一点，就是说如果某个非 master 节点宕机了。那么此节点上的 primary shard 不就没了。那好，master 会让 primary shard 对应的 replica shard（在其他机器上）切换为 primary shard。如果宕机的机器修复了，修复后的节点也不再是 primary shard，而是 replica shard。 其实上述就是 ElasticSearch 作为分布式搜索引擎最基本的一个架构设计。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>es</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis]]></title>
    <url>%2F2018%2F03%2F03%2Fdb-redis-Redis-2%2F</url>
    <content type="text"><![CDATA[redis 简介简单来说 redis 就是一个数据库，不过与传统数据库不同的是 redis 的数据是存在内存中的，所以读写速度非常快，因此 redis 被广泛应用于缓存方向。另外，redis 也经常用来做分布式锁。redis 提供了多种数据类型来支持不同的业务场景。除此之外，redis 支持事务 、持久化、LUA脚本、LRU驱动事件、多种集群方案。 为什么要用 redis/为什么要用缓存主要从“高性能”和“高并发”这两点来看待这个问题。 高性能： 假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在数缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！ 高并发： 直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 为什么要用 redis 而不用 map/guava 做缓存? 下面的内容来自 segmentfault 一位网友的提问，地址：https://segmentfault.com/q/1010000009106416 缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 或者 guava 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 jvm 的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓存不具有一致性。 使用 redis 或 memcached 之类的称为分布式缓存，在多实例的情况下，各实例共用一份缓存数据，缓存具有一致性。缺点是需要保持 redis 或 memcached服务的高可用，整个程序架构上较为复杂。 redis 和 memcached 的区别对于 redis 和 memcached 我总结了下面四点。现在公司一般都是用 redis 来实现缓存，而且 redis 自身也越来越强大了！ redis支持更丰富的数据类型（支持更复杂的应用场景）：Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。memcache支持简单的数据类型，String。 Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而Memecache把数据全部存在内存之中。 集群模式：memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 redis 目前是原生支持 cluster 模式的. Memcached是多线程，非阻塞IO复用的网络模型；Redis使用单线程的多路 IO 复用模型。 来自网络上的一张图，这里分享给大家！ redis 常见数据结构以及使用场景分析1.String 常用命令: set,get,decr,incr,mget 等。 String数据结构是简单的key-value类型，value其实不仅可以是String，也可以是数字。常规key-value缓存应用；常规计数：微博数，粉丝数等。 2.Hash 常用命令： hget,hset,hgetall 等。 hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值。 比如我们可以 hash 数据结构来存储用户信息，商品信息等等。比如下面我就用 hash 类型存放了我本人的一些信息： 1234567key=JavaUser293847value=&#123; “id”: 1, “name”: “SnailClimb”, “age”: 22, “location”: “Wuhan, Hubei”&#125; 3.List 常用命令: lpush,rpush,lpop,rpop,lrange等 list 就是链表，Redis list 的应用场景非常多，也是Redis最重要的数据结构之一，比如微博的关注列表，粉丝列表，消息列表等功能都可以用Redis的 list 结构来实现。 Redis list 的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。 另外可以通过 lrange 命令，就是从某个元素开始读取多少个元素，可以基于 list 实现分页查询，这个很棒的一个功能，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西（一页一页的往下走），性能高。 4.Set 常用命令：sadd,spop,smembers,sunion 等 set 对外提供的功能与list类似是一个列表的功能，特殊之处在于 set 是可以自动排重的。 当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。可以基于 set 轻易实现交集、并集、差集的操作。 比如：在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis可以非常方便的实现如共同关注、共同粉丝、共同喜好等功能。这个过程也就是求交集的过程，具体命令如下： 1sinterstore key1 key2 key3 将交集存在key1内 5.Sorted Set 常用命令： zadd,zrange,zrem,zcard等 和set相比，sorted set增加了一个权重参数score，使得集合中的元素能够按score进行有序排列。 举例： 在直播系统中，实时排行信息包含直播间在线用户列表，各种礼物排行榜，弹幕消息（可以理解为按消息维度的消息排行榜）等信息，适合使用 Redis 中的 Sorted Set 结构进行存储。 redis 设置过期时间Redis中有个设置时间过期的功能，即对存储在 redis 数据库中的值可以设置一个过期时间。作为一个缓存数据库，这是非常实用的。如我们一般项目中的 token 或者一些登录信息，尤其是短信验证码都是有时间限制的，按照传统的数据库处理方式，一般都是自己判断过期，这样无疑会严重影响项目性能。 我们 set key 的时候，都可以给一个 expire time，就是过期时间，通过过期时间我们可以指定这个 key 可以存活的时间。 如果假设你设置了一批 key 只能存活1个小时，那么接下来1小时后，redis是怎么对这批key进行删除的？ 定期删除+惰性删除。 通过名字大概就能猜出这两个删除方式的意思了。 定期删除：redis默认是每隔 100ms 就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。注意这里是随机抽取的。为什么要随机呢？你想一想假如 redis 存了几十万个 key ，每隔100ms就遍历所有的设置过期时间的 key 的话，就会给 CPU 带来很大的负载！ 惰性删除 ：定期删除可能会导致很多过期 key 到了时间并没有被删除掉。所以就有了惰性删除。假如你的过期 key，靠定期删除没有被删除掉，还停留在内存里，除非你的系统去查一下那个 key，才会被redis给删除掉。这就是所谓的惰性删除，也是够懒的哈！ 但是仅仅通过设置过期时间还是有问题的。我们想一下：如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期key堆积在内存里，导致redis内存块耗尽了。怎么解决这个问题呢？ redis 内存淘汰机制。 redis 内存淘汰机制(MySQL里有2000w数据，Redis中只存20w的数据，如何保证Redis中的数据都是热点数据?)redis 配置文件 redis.conf 中有相关注释，我这里就不贴了，大家可以自行查阅或者通过这个网址查看： http://download.redis.io/redis-stable/redis.conf redis 提供 6种数据淘汰策略： volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key（这个是最常用的） allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-eviction：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！ 备注： 关于 redis 设置过期时间以及内存淘汰机制，我这里只是简单的总结一下，后面会专门写一篇文章来总结！ redis 持久化机制(怎么保证 redis 挂掉之后再重启数据可以进行恢复)很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器、机器故障之后恢复数据），或者是为了防止系统故障而将数据备份到一个远程位置。 Redis不同于Memcached的很重一点就是，Redis支持持久化，而且支持两种不同的持久化操作。Redis的一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file,AOF）。这两种方法各有千秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方法。 快照（snapshotting）持久化（RDB） Redis可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性能），还可以将快照留在原地以便重启服务器的时候使用。 快照持久化是Redis默认采用的持久化方式，在redis.conf配置文件中默认有此下配置： 123456save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 AOF（append-only file）持久化 与快照持久化相比，AOF持久化 的实时性更好，因此已成为主流的持久化方案。默认情况下Redis没有开启AOF（append only file）方式的持久化，可以通过appendonly参数开启： 1appendonly yes 开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof。 在Redis的配置文件中存在三种不同的 AOF 持久化方式，它们分别是： 123appendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度appendfsync everysec #每秒钟同步一次，显示地将多个写命令同步到硬盘appendfsync no #让操作系统决定何时进行同步 为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec选项 ，让Redis每秒同步一次AOF文件，Redis性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。 Redis 4.0 对于持久化机制的优化 Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。 如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。 补充内容：AOF 重写 AOF重写可以产生一个新的AOF文件，这个新的AOF文件和原有的AOF文件所保存的数据库状态一样，但体积更小。 AOF重写是一个有歧义的名字，该功能是通过读取数据库中的键值对来实现的，程序无须对现有AOF文件进行任何读入、分析或者写入操作。 在执行 BGREWRITEAOF 命令时，Redis 服务器会维护一个 AOF 重写缓冲区，该缓冲区会在子进程创建新AOF文件期间，记录服务器执行的所有写命令。当子进程完成创建新AOF文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新AOF文件的末尾，使得新旧两个AOF文件所保存的数据库状态一致。最后，服务器用新的AOF文件替换旧的AOF文件，以此来完成AOF文件重写操作 更多内容可以查看我的这篇文章： Redis持久化 redis 事务Redis 通过 MULTI、EXEC、WATCH 等命令来实现事务(transaction)功能。事务提供了一种将多个命令请求打包，然后一次性、按顺序地执行多个命令的机制，并且在事务执行期间，服务器不会中断事务而改去执行其他客户端的命令请求，它会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求。 在传统的关系式数据库中，常常用 ACID 性质来检验事务功能的可靠性和安全性。在 Redis 中，事务总是具有原子性（Atomicity）、一致性（Consistency）和隔离性（Isolation），并且当 Redis 运行在某种特定的持久化模式下时，事务也具有持久性（Durability）。 缓存雪崩和缓存穿透问题解决方案缓存雪崩 简介：缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决办法（中华石杉老师在他的视频中提到过，视频地址在最后一个问题中有提到）： 事前：尽量保证整个 redis 集群的高可用性，发现机器宕机尽快补上。选择合适的内存淘汰策略。 事中：本地ehcache缓存 + hystrix限流&amp;降级，避免MySQL崩掉 事后：利用 redis 持久化机制保存的数据尽快恢复缓存 缓存穿透 简介：一般是黑客故意去请求缓存中不存在的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决办法： 有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。另外也有一个更为简单粗暴的方法（我们采用的就是这种），如果一个查询返回的数据为空（不管是数 据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。 参考： https://blog.csdn.net/zeb_perfect/article/details/54135506 如何解决 Redis 的并发竞争 Key 问题所谓 Redis 的并发竞争 Key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同！ 推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（如果不存在 Redis 的并发竞争 Key 问题，不要使用分布式锁，这样会影响性能） 基于zookeeper临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。 在实践中，当然是从以可靠性为主。所以首推Zookeeper。 如何保证缓存与数据库双写时的数据一致性?你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？ 一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况，最好不要做这个方案，读请求和写请求串行化，串到一个内存队列里去，这样就可以保证一定不会出现不一致的情况 串行化之后，就会导致系统的吞吐量会大幅度的降低，用比正常情况下多几倍的机器去支撑线上的一个请求。]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis持久化]]></title>
    <url>%2F2018%2F03%2F02%2Fdb-redis-Redis%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[非常感谢《redis实战》真本书，本文大多内容也参考了书中的内容。非常推荐大家看一下《redis实战》这本书，感觉书中的很多理论性东西还是很不错的。 为什么本文的名字要加上春夏秋冬又一春，哈哈 ，这是一部韩国的电影，我感觉电影不错，所以就用在文章名字上了，没有什么特别的含义，然后下面的有些配图也是电影相关镜头。 很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器、机器故障之后回复数据），或者是为了防止系统故障而将数据备份到一个远程位置。 Redis不同于Memcached的很重一点就是，Redis支持持久化，而且支持两种不同的持久化操作。Redis的一种持久化方式叫快照（snapshotting，RDB）,另一种方式是只追加文件（append-only file,AOF）.这两种方法各有千秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方法。 快照（snapshotting）持久化Redis可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性能），还可以将快照留在原地以便重启服务器的时候使用。 快照持久化是Redis默认采用的持久化方式，在redis.conf配置文件中默认有此下配置： 123456save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 根据配置，快照将被写入dbfilename选项指定的文件里面，并存储在dir选项指定的路径上面。如果在新的快照文件创建完毕之前，Redis、系统或者硬件这三者中的任意一个崩溃了，那么Redis将丢失最近一次创建快照写入的所有数据。 举个例子：假设Redis的上一个快照是2：35开始创建的，并且已经创建成功。下午3：06时，Redis又开始创建新的快照，并且在下午3：08快照创建完毕之前，有35个键进行了更新。如果在下午3：06到3：08期间，系统发生了崩溃，导致Redis无法完成新快照的创建工作，那么Redis将丢失下午2：35之后写入的所有数据。另一方面，如果系统恰好在新的快照文件创建完毕之后崩溃，那么Redis将丢失35个键的更新数据。 创建快照的办法有如下几种： BGSAVE命令： 客户端向Redis发送 BGSAVE命令 来创建一个快照。对于支持BGSAVE命令的平台来说（基本上所有平台支持，除了Windows平台），Redis会调用fork来创建一个子进程，然后子进程负责将快照写入硬盘，而父进程则继续处理命令请求。 SAVE命令： 客户端还可以向Redis发送 SAVE命令 来创建一个快照，接到SAVE命令的Redis服务器在快照创建完毕之前不会再响应任何其他命令。SAVE命令不常用，我们通常只会在没有足够内存去执行BGSAVE命令的情况下，又或者即使等待持久化操作执行完毕也无所谓的情况下，才会使用这个命令。 save选项： 如果用户设置了save选项（一般会默认设置），比如 save 60 10000，那么从Redis最近一次创建快照之后开始算起，当“60秒之内有10000次写入”这个条件被满足时，Redis就会自动触发BGSAVE命令。 SHUTDOWN命令： 当Redis通过SHUTDOWN命令接收到关闭服务器的请求时，或者接收到标准TERM信号时，会执行一个SAVE命令，阻塞所有客户端，不再执行客户端发送的任何命令，并在SAVE命令执行完毕之后关闭服务器。 一个Redis服务器连接到另一个Redis服务器： 当一个Redis服务器连接到另一个Redis服务器，并向对方发送SYNC命令来开始一次复制操作的时候，如果主服务器目前没有执行BGSAVE操作，或者主服务器并非刚刚执行完BGSAVE操作，那么主服务器就会执行BGSAVE命令 如果系统真的发生崩溃，用户将丢失最近一次生成快照之后更改的所有数据。因此，快照持久化只适用于即使丢失一部分数据也不会造成一些大问题的应用程序。不能接受这个缺点的话，可以考虑AOF持久化。 AOF（append-only file）持久化与快照持久化相比，AOF持久化 的实时性更好，因此已成为主流的持久化方案。默认情况下Redis没有开启AOF（append only file）方式的持久化，可以通过appendonly参数开启： 1appendonly yes 开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof。 在Redis的配置文件中存在三种同步方式，它们分别是： 1234appendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度appendfsync everysec #每秒钟同步一次，显示地将多个写命令同步到硬盘appendfsync no #让操作系统决定何时进行同步 appendfsync always 可以实现将数据丢失减到最少，不过这种方式需要对硬盘进行大量的写入而且每次只写入一个命令，十分影响Redis的速度。另外使用固态硬盘的用户谨慎使用appendfsync always选项，因为这会明显降低固态硬盘的使用寿命。 为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec选项 ，让Redis每秒同步一次AOF文件，Redis性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。 appendfsync no 选项一般不推荐，这种方案会使Redis丢失不定量的数据而且如果用户的硬盘处理写入操作的速度不够的话，那么当缓冲区被等待写入的数据填满时，Redis的写入操作将被阻塞，这会导致Redis的请求速度变慢。 虽然AOF持久化非常灵活地提供了多种不同的选项来满足不同应用程序对数据安全的不同要求，但AOF持久化也有缺陷——AOF文件的体积太大。 重写/压缩AOFAOF虽然在某个角度可以将数据丢失降低到最小而且对性能影响也很小，但是极端的情况下，体积不断增大的AOF文件很可能会用完硬盘空间。另外，如果AOF体积过大，那么还原操作执行时间就可能会非常长。 为了解决AOF体积过大的问题，用户可以向Redis发送 BGREWRITEAOF命令 ，这个命令会通过移除AOF文件中的冗余命令来重写（rewrite）AOF文件来减小AOF文件的体积。BGREWRITEAOF命令和BGSAVE创建快照原理十分相似，所以AOF文件重写也需要用到子进程，这样会导致性能问题和内存占用问题，和快照持久化一样。更糟糕的是，如果不加以控制的话，AOF文件的体积可能会比快照文件大好几倍。 文件重写流程： 和快照持久化可以通过设置save选项来自动执行BGSAVE一样，AOF持久化也可以通过设置 1auto-aof-rewrite-percentage 选项和 1auto-aof-rewrite-min-size 选项自动执行BGREWRITEAOF命令。举例：假设用户对Redis设置了如下配置选项并且启用了AOF持久化。那么当AOF文件体积大于64mb，并且AOF的体积比上一次重写之后的体积大了至少一倍（100%）的时候，Redis将执行BGREWRITEAOF命令。 12auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb 无论是AOF持久化还是快照持久化，将数据持久化到硬盘上都是非常有必要的，但除了进行持久化外，用户还必须对持久化得到的文件进行备份（最好是备份到不同的地方），这样才能尽量避免数据丢失事故发生。如果条件允许的话，最好能将快照文件和重新重写的AOF文件备份到不同的服务器上面。 随着负载量的上升，或者数据的完整性变得 越来越重要时，用户可能需要使用到复制特性。 Redis 4.0 对于持久化机制的优化Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。 如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分就是压缩格式不再是 AOF 格式，可读性较差。 参考： 《Redis实战》]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene 和 es 的前世今生]]></title>
    <url>%2F2018%2F03%2F01%2Fsubject-high-concurrency-es-introduction%2F</url>
    <content type="text"><![CDATA[lucene 和 es 的前世今生lucene 是最先进、功能最强大的搜索库。如果直接基于 lucene 开发，非常复杂，即便写一些简单的功能，也要写大量的 Java 代码，需要深入理解原理。 elasticsearch 基于 lucene，隐藏了 lucene 的复杂性，提供了简单易用的 restful api / Java api 接口（另外还有其他语言的 api 接口）。 分布式的文档存储引擎 分布式的搜索引擎和分析引擎 分布式，支持 PB 级数据 es 的核心概念Near Realtime近实时，有两层意思： 从写入数据到数据可以被搜索到有一个小延迟（大概是 1s） 基于 es 执行搜索和分析可以达到秒级 Cluster 集群集群包含多个节点，每个节点属于哪个集群都是通过一个配置来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常。 Node 节点Node 是集群中的一个节点，节点也有一个名称，默认是随机分配的。默认节点会去加入一个名称为 elasticsearch 的集群。如果直接启动一堆节点，那么它们会自动组成一个 elasticsearch 集群，当然一个节点也可以组成 elasticsearch 集群。 Document &amp; field文档是 es 中最小的数据单元，一个 document 可以是一条客户数据、一条商品分类数据、一条订单数据，通常用 json 数据结构来表示。每个 index 下的 type，都可以存储多条 document。一个 document 里面有多个 field，每个 field 就是一个数据字段。 1234567&#123; "product_id": "1", "product_name": "iPhone X", "product_desc": "苹果手机", "category_id": "2", "category_name": "电子产品"&#125; Index索引包含了一堆有相似结构的文档数据，比如商品索引。一个索引包含很多 document，一个索引就代表了一类相似或者相同的 ducument。 Type类型，每个索引里可以有一个或者多个 type，type 是 index 的一个逻辑分类，比如商品 index 下有多个 type：日化商品 type、电器商品 type、生鲜商品 type。每个 type 下的 document 的 field 可能不太一样。 shard单台机器无法存储大量数据，es 可以将一个索引中的数据切分为多个 shard，分布在多台服务器上存储。有了 shard 就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。每个 shard 都是一个 lucene index。 replica任何一个服务器随时可能故障或宕机，此时 shard 可能就会丢失，因此可以为每个 shard 创建多个 replica 副本。replica 可以在 shard 故障时提供备用服务，保证数据不丢失，多个 replica 还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能修改，默认 5 个），replica shard（随时修改数量，默认 1 个），默认每个索引 10 个 shard，5 个 primary shard，5个 replica shard，最小的高可用配置，是 2 台服务器。 这么说吧，shard 分为 primary shard 和 replica shard。而 primary shard 一般简称为 shard，而 replica shard 一般简称为 replica。 es 核心概念 vs. db 核心概念 es db index 数据库 type 数据表 docuemnt 一行数据 以上是一个简单的类比。]]></content>
      <categories>
        <category>high-concurrency</category>
      </categories>
      <tags>
        <tag>es</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bootstrap-全局 CSS 样式]]></title>
    <url>%2F2017%2F12%2F16%2Flang-bootstrap-2017-12-16-bootstrap-0%2F</url>
    <content type="text"><![CDATA[引用 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante. Someone famous in Source Title Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante. Someone famous in Source Title 标题h1. Bootstrap heading Secondary text h2. Bootstrap heading Secondary text h3. Bootstrap heading Secondary text h4. Bootstrap heading Secondary text h5. Bootstrap heading Secondary text h6. Bootstrap heading Secondary text --- 小格式文字强调 文字加粗 attr文字缩略对齐左对齐 居中对齐 右对齐 两端对齐 No wrap text. --- 两种列表方式 横向内联1 横向内联2 横向内联3 1. w2 1. w1 1. w3 a a a * Red * Green * Blue 1 1 1 --- 描述语 带有描述的短语列表。 全部内容全部部内容全部内容全部内容全部内容全部内容全部内容全部内容全部内容全部内容全部内容全部内容全部内容全部内容全部内容 水平排列的描述 全部内容全部部内容全部内容全部内容全部内容全部内容全部内容全部内容全部内容全部内容全部内容全部内容全部内容全部内容全部内容 代码应用For example, &lt;section&gt; should be wrapped as inline. To switch directories, type cd followed by the name of the directory.To edit settings, press ctrl + , y = mx + b &lt;p&gt;Sample text here...&lt;/p&gt;]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Bootstrap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bootstrap-组件]]></title>
    <url>%2F2017%2F12%2F16%2Flang-bootstrap-2017-12-16-bootstrap-2-zj%2F</url>
    <content type="text"><![CDATA[Home Profile Messages ### 组件 Left Middle Right ... ... ... Star Error: Enter a valid email address Dropup Action Another action Something else here Separated link 1 2 Dropdown Dropdown link Dropdown link Action Action Another action Something else here Separated link Action Toggle Dropdown Action Another action Something else here Separated link DefaultPrimarySuccessInfoWarningDanger Inbox 42 Messages 4]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Bootstrap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bootstrap-插件]]></title>
    <url>%2F2017%2F12%2F16%2Flang-bootstrap-2017-12-16-bootstrap-3-cj%2F</url>
    <content type="text"><![CDATA[弹出框 &times; 标题 ... 取消 保存 × Holy guacamole! Best check yo self, you're not looking too good. × Oh snap! You got an error! Change this and that and try again. Duis mollis, est non commodo luctus, nisi erat porttitor ligula, eget lacinia odio sem nec elit. Cras mattis consectetur purus sit amet fermentum. Take this action Or do this]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Bootstrap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bootstrap-Table]]></title>
    <url>%2F2017%2F12%2F16%2Flang-bootstrap-2017-12-16-bootstrap-1-table%2F</url>
    <content type="text"><![CDATA[基本表格 ... ... ... table-striped ... ... ... ... ... ... ... ... table-bordered ... ... 悬停 table-hover ... ... ... ... ... ... ... ... 压缩 table-condensed ... ... ... ... ... ... ... ... 栅格表格 .col-xs-12 .col-md-8 .col-xs-6 .col-md-4 .col-xs-6 .col-md-4 .col-xs-6 .col-md-4 .col-xs-6 .col-md-4 .col-xs-6 .col-xs-6]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Bootstrap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bootstrap-控件]]></title>
    <url>%2F2017%2F12%2F16%2Flang-bootstrap-2017-12-16-bootstrap-4-kj%2F</url>
    <content type="text"><![CDATA[按钮改变大小和颜色 （默认样式）Default btn-lg （首选项）Primary （成功）Success btn-sm （一般信息）Info btn-xs （警告）Warning （危险）Danger （链接）Link 文本和背景改变颜色.mcorper nulla non metus.. ..mcorper nulla non metus. .mcorper nulla non metus.. .mcorper nulla non metus.. .mcorper nulla non metus.. ..mcorper nulla non metus. ### 输入框和下拉框 ... ... 1 2 3 4 5 选项 选项无效 单选1 name="optionsRadios" 单选2 name="optionsRadios" 单选3-无效 name="optionsRadios" 单选和复选按钮 1 2 3 1 2 class=”form-group has-success” Input with success 说明信息 Input with warning Input with error Checkbox with success Checkbox with warning Checkbox with error 控件 Email address Password File input Example block-level help text here. Check me out Submit Name Email Send invitation Amount (in dollars) $ .00 Transfer cash &times;]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Bootstrap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RPC]]></title>
    <url>%2F2017%2F11%2F15%2Futils-%E6%95%B0%E6%8D%AE%E9%80%9A%E4%BF%A1-RESTful%E3%80%81RPC%E3%80%81%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[RPC RPC（Remote Procedure Call）—远程过程调用 ，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发分布式程序就像开发本地程序一样简单。 RPC采用客户端（服务调用方）/服务器端（服务提供方）模式， 都运行在自己的JVM中。客户端只需要引入要使用的接口，接口的实现和运行都在服务器端。RPC主要依赖的技术包括序列化、反序列化和数据传输协议，这是一种定义与实现相分离的设计。 目前Java使用比较多的RPC方案主要有RMI（JDK自带）、Hessian、Dubbo以及Thrift等。 注意： RPC主要指内部服务之间的调用，RESTful也可以用于内部服务之间的调用，但其主要用途还在于外部系统提供服务，因此没有将其包含在本知识点内。 常见RPC框架： RMI（JDK自带）： JDK自带的RPC 详细内容可以参考：从懵逼到恍然大悟之Java中RMI的使用 Dubbo: Dubbo是 阿里巴巴公司开源的一个高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和 Spring框架无缝集成。 详细内容可以参考： 高性能优秀的服务框架-dubbo介绍 Dubbo是什么？能做什么？ Hessian： Hessian是一个轻量级的remotingonhttp工具，使用简单的方法提供了RMI的功能。 相比WebService，Hessian更简单、快捷。采用的是二进制RPC协议，因为采用的是二进制协议，所以它很适合于发送二进制数据。 详细内容可以参考： Hessian的使用以及理解 Thrift： Apache Thrift是Facebook开源的跨语言的RPC通信框架，目前已经捐献给Apache基金会管理，由于其跨语言特性和出色的性能，在很多互联网公司得到应用，有能力的公司甚至会基于thrift研发一套分布式服务框架，增加诸如服务注册、服务发现等功能。 详细内容可以参考： [【Java】分布式RPC通信框架Apache Thrift 使用总结](https://www.cnblogs.com/zeze/p/8628585.html)如何进行选择： 是否允许代码侵入： 即需要依赖相应的代码生成器生成代码，比如Thrift。 是否需要长连接获取高性能： 如果对于性能需求较高的haul，那么可以果断选择基于TCP的Thrift、Dubbo。 是否需要跨越网段、跨越防火墙： 这种情况一般选择基于HTTP协议的Hessian和Thrift的HTTP Transport。 此外，Google推出的基于HTTP2.0的gRPC框架也开始得到应用，其序列化协议基于Protobuf，网络框架使用的是Netty4,但是其需要生成代码，可扩展性也比较差。 消息中间件 消息中间件，也可以叫做中央消息队列或者是消息队列（区别于本地消息队列，本地消息队列指的是JVM内的队列实现），是一种独立的队列系统，消息中间件经常用来解决内部服务之间的 异步调用问题 。请求服务方把请求队列放到队列中即可返回，然后等待服务提供方去队列中获取请求进行处理，之后通过回调等机制把结果返回给请求服务方。 异步调用只是消息中间件一个非常常见的应用场景。此外，常用的消息队列应用场景还偷如下几个： 解耦 ： 一个业务的非核心流程需要依赖其他系统，但结果并不重要，有通知即可。 最终一致性 ： 指的是两个系统的状态保持一致，可以有一定的延迟，只要最终达到一致性即可。经常用在解决分布式事务上。 广播 ： 消息队列最基本的功能。生产者只负责生产消息，订阅者接收消息。 错峰和流控]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo]]></title>
    <url>%2F2017%2F11%2F15%2Futils-dubbo%2F</url>
    <content type="text"><![CDATA[本文是作者根据官方文档以及自己平时的使用情况，对 Dubbo 所做的一个总结。如果不懂 Dubbo 的使用的话，可以参考我的这篇文章《超详细，新手都能看懂 ！使用SpringBoot+Dubbo 搭建一个简单的分布式服务》 Dubbo 官网：http://dubbo.apache.org/zh-cn/index.html Dubbo 中文文档： http://dubbo.apache.org/zh-cn/index.html 一 重要的概念1.1 什么是 Dubbo?Apache Dubbo (incubating) |ˈdʌbəʊ| 是一款高性能、轻量级的开源Java RPC 框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。简单来说 Dubbo 是一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案。 Dubbo 目前已经有接近 23k 的 Star ，Dubbo的Github 地址：https://github.com/apache/incubator-dubbo 。 另外，在开源中国举行的2018年度最受欢迎中国开源软件这个活动的评选中，Dubbo 更是凭借其超高人气仅次于 vue.js 和 ECharts 获得第三名的好成绩。 Dubbo 是由阿里开源，后来加入了 Apache 。正式由于 Dubbo 的出现，才使得越来越多的公司开始使用以及接受分布式架构。 我们上面说了 Dubbo 实际上是 RPC 框架，那么什么是 RPC呢？ 1.2 什么是 RPC?RPC原理是什么?什么是 RPC？ RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。比如两个不同的服务 A、B 部署在两台不同的机器上，那么服务 A 如果想要调用服务 B 中的某个方法该怎么办呢？使用 HTTP请求 当然可以，但是可能会比较慢而且一些优化做的并不好。 RPC 的出现就是为了解决这个问题。 RPC原理是什么？ 我这里这是简单的提一下。详细内容可以查看下面这篇文章： http://www.importnew.com/22003.html 服务消费方（client）调用以本地调用方式调用服务； client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体； client stub找到服务地址，并将消息发送到服务端； server stub收到消息后进行解码； server stub根据解码结果调用本地的服务； 本地服务执行并将结果返回给server stub； server stub将返回结果打包成消息并发送至消费方； client stub接收到消息，并进行解码； 服务消费方得到最终结果。 下面再贴一个网上的时序图： 说了这么多，我们为什么要用 Dubbo 呢？ 1.3 为什么要用 Dubbo?Dubbo 的诞生和 SOA 分布式架构的流行有着莫大的关系。SOA 面向服务的架构（Service Oriented Architecture），也就是把工程按照业务逻辑拆分成服务层、表现层两个工程。服务层中包含业务逻辑，只需要对外提供服务即可。表现层只需要处理和页面的交互，业务逻辑都是调用服务层的服务来实现。SOA架构中有两个主要角色：服务提供者（Provider）和服务使用者（Consumer）。 如果你要开发分布式程序，你也可以直接基于 HTTP 接口进行通信，但是为什么要用 Dubbo呢？ 我觉得主要可以从 Dubbo 提供的下面四点特性来说为什么要用 Dubbo： 负载均衡——同一个服务部署在不同的机器时该调用那一台机器上的服务。 服务调用链路生成——随着系统的发展，服务越来越多，服务间依赖关系变得错踪复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系。Dubbo 可以为我们解决服务之间互相是如何调用的。 服务访问压力以及时长统计、资源调度和治理——基于访问压力实时管理集群容量，提高集群利用率。 服务降级——某个服务挂掉之后调用备用服务。 另外，Dubbo 除了能够应用在分布式系统中，也可以应用在现在比较火的微服务系统中。不过，由于 Spring Cloud 在微服务中应用更加广泛，所以，我觉得一般我们提 Dubbo 的话，大部分是分布式系统的情况。 我们刚刚提到了分布式这个概念，下面再给大家介绍一下什么是分布式？为什么要分布式？ 1.4 什么是分布式?分布式或者说 SOA 分布式重要的就是面向服务，说简单的分布式就是我们把整个系统拆分成不同的服务然后将这些服务放在不同的服务器上减轻单体服务的压力提高并发量和性能。比如电商系统可以简单地拆分成订单系统、商品系统、登录系统等等，拆分之后的每个服务可以部署在不同的机器上，如果某一个服务的访问量比较大的话也可以将这个服务同时部署在多台机器上。 1.5 为什么要分布式?从开发角度来讲单体应用的代码都集中在一起，而分布式系统的代码根据业务被拆分。所以，每个团队可以负责一个服务的开发，这样提升了开发效率。另外，代码根据业务拆分之后更加便于维护和扩展。 另外，我觉得将系统拆分成分布式之后不光便于系统扩展和维护，更能提高整个系统的性能。你想一想嘛？把整个系统拆分成不同的服务/系统，然后每个服务/系统 单独部署在一台服务器上，是不是很大程度上提高了系统性能呢？ 二 Dubbo 的架构2.1 Dubbo 的架构图解 上述节点简单说明： Provider： 暴露服务的服务提供方 Consumer： 调用远程服务的服务消费方 Registry： 服务注册与发现的注册中心 Monitor： 统计服务的调用次数和调用时间的监控中心 Container： 服务运行容器 调用关系说明： 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 重要知识点总结： 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小 监控中心负责统计各服务调用次数，调用时间等，统计先在内存汇总后每分钟一次发送到监控中心服务器，并以报表展示 注册中心，服务提供者，服务消费者三者之间均为长连接，监控中心除外 注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者 注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表 注册中心和监控中心都是可选的，服务消费者可以直连服务提供者 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 2.2 Dubbo 工作原理 图中从下至上分为十层，各层均为单向依赖，右边的黑色箭头代表层之间的依赖关系，每一层都可以剥离上层被复用，其中，Service 和 Config 层为 API，其它各层均为 SPI。 各层说明： 第一层：service层，接口层，给服务提供者和消费者来实现的 第二层：config层，配置层，主要是对dubbo进行各种配置的 第三层：proxy层，服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton 第四层：registry层，服务注册层，负责服务的注册与发现 第五层：cluster层，集群层，封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务 第六层：monitor层，监控层，对rpc接口的调用次数和调用时间进行监控 第七层：protocol层，远程调用层，封装rpc调用 第八层：exchange层，信息交换层，封装请求响应模式，同步转异步 第九层：transport层，网络传输层，抽象mina和netty为统一接口 第十层：serialize层，数据序列化层，网络传输需要 三 Dubbo 的负载均衡策略3.1 先来解释一下什么是负载均衡先来个官方的解释。 维基百科对负载均衡的定义：负载均衡改善了跨多个计算资源（例如计算机，计算机集群，网络链接，中央处理单元或磁盘驱动的的工作负载分布。负载平衡旨在优化资源使用，最大化吞吐量，最小化响应时间，并避免任何单个资源的过载。使用具有负载平衡而不是单个组件的多个组件可以通过冗余提高可靠性和可用性。负载平衡通常涉及专用软件或硬件。 上面讲的大家可能不太好理解，再用通俗的话给大家说一下。 比如我们的系统中的某个服务的访问量特别大，我们将这个服务部署在了多台服务器上，当客户端发起请求的时候，多台服务器都可以处理这个请求。那么，如何正确选择处理该请求的服务器就很关键。假如，你就要一台服务器来处理该服务的请求，那该服务部署在多台服务器的意义就不复存在了。负载均衡就是为了避免单个服务器响应同一请求，容易造成服务器宕机、崩溃等问题，我们从负载均衡的这四个字就能明显感受到它的意义。 3.2 再来看看 Dubbo 提供的负载均衡策略在集群负载均衡时，Dubbo 提供了多种均衡策略，默认为 random 随机调用。可以自行扩展负载均衡策略，参见：负载均衡扩展。 备注:下面的图片来自于：尚硅谷2018Dubbo 视频。 3.2.1 Random LoadBalance(默认，基于权重的随机负载均衡机制) 随机，按权重设置随机概率。 在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 3.2.2 RoundRobin LoadBalance(不推荐，基于权重的轮询负载均衡机制) 轮循，按公约后的权重设置轮循比率。 存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 3.2.3 LeastActive LoadBalance 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。 使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 3.2.4 ConsistentHash LoadBalance 一致性 Hash，相同参数的请求总是发到同一提供者。(如果你需要的不是随机负载均衡，是要一类请求都到一个节点，那就走这个一致性hash策略。) 当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。 算法参见：http://en.wikipedia.org/wiki/Consistent_hashing 缺省只对第一个参数 Hash，如果要修改，请配置 &lt;dubbo:parameter key=&quot;hash.arguments&quot; value=&quot;0,1&quot; /&gt; 缺省用 160 份虚拟节点，如果要修改，请配置 &lt;dubbo:parameter key=&quot;hash.nodes&quot; value=&quot;320&quot; /&gt; 3.3 配置方式xml 配置方式 服务端服务级别 1&lt;dubbo:service interface="..." loadbalance="roundrobin" /&gt; 客户端服务级别 1&lt;dubbo:reference interface="..." loadbalance="roundrobin" /&gt; 服务端方法级别 123&lt;dubbo:service interface="..."&gt; &lt;dubbo:method name="..." loadbalance="roundrobin"/&gt;&lt;/dubbo:service&gt; 客户端方法级别 123&lt;dubbo:reference interface="..."&gt; &lt;dubbo:method name="..." loadbalance="roundrobin"/&gt;&lt;/dubbo:reference&gt; 注解配置方式： 消费方基于基于注解的服务级别配置方式： 12@Reference(loadbalance = "roundrobin")HelloService helloService; 四 zookeeper宕机与dubbo直连的情况zookeeper宕机与dubbo直连的情况在面试中可能会被经常问到，所以要引起重视。 在实际生产中，假如zookeeper注册中心宕掉，一段时间内服务消费方还是能够调用提供方的服务的，实际上它使用的本地缓存进行通讯，这只是dubbo健壮性的一种提现。 dubbo的健壮性表现： 监控中心宕掉不影响使用，只是丢失部分采样数据 数据库宕掉后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务 注册中心对等集群，任意一台宕掉后，将自动切换到另一台 注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 我们前面提到过：注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小。所以，我们可以完全可以绕过注册中心——采用 dubbo 直连 ，即在服务消费方配置服务提供方的位置信息。 xml配置方式： 1&lt;dubbo:reference id="userService" interface="com.zang.gmall.service.UserService" url="dubbo://localhost:20880" /&gt; 注解方式： 12@Reference(url = "127.0.0.1:20880") HelloService helloService;]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10分钟快速入门Redis]]></title>
    <url>%2F2017%2F11%2F03%2Fdb-redis-Redis%2F</url>
    <content type="text"><![CDATA[Redis安装官方编译安装12345678910111213$ wget http://download.redis.io/releases/redis-4.0.0.tar.gz$ tar xzvf redis-4.0.0.tar.gz -C /usr/local/$ cd /usr/local/redis-4.0.0$ make$ make test$ make install # 程序会自动执行:# mkdir -p /usr/local/bin# cp -pf redis-server /usr/local/bin# cp -pf redis-benchmark /usr/local/bin# cp -pf redis-cli /usr/local/bin# cp -pf redis-check-dump /usr/local/bin# cp -pf redis-check-aof /usr/local/bin 测试make test报错 123$ make testYou need tcl 8.5 or newer in order to run the Redis testmake: *** [test] Error 1 这个是需要安装tcl 123456wget http://downloads.sourceforge.net/tcl/tcl8.6.1-src.tar.gz sudo tar xzvf tcl8.6.1-src.tar.gz -C /usr/local/ cd /usr/local/tcl8.6.1/unix/ sudo ./configure sudo make sudo make install 通过EPEL源安装源安装问题在于不能安装最新，或者指定Redis版本。 1yum --enablerepo=epel -y install redis 如果没有安装源，通过下面方式安装源。 12cd /etc/yum.repos.d/rpm -Uvh http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm Redis升级首先，确保安装了以下repos，EPEL和REMI： 12sudo rpm -Uvh http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpmsudo rpm -Uvh http://rpms.remirepo.net/enterprise/remi-release-6.rpm 通过--enablerepo=epel参数查看指定源Redis版本，检查REMI repo中的Redis版本： 1234567891011121314151617181920212223yum --enablerepo=epel info redis# Loaded plugins: fastestmirror# Loading mirror speeds from cached hostfile# * base: centos.ustc.edu.cn# * epel: mirrors.tuna.tsinghua.edu.cn# * extras: centos.ustc.edu.cn# * updates: mirrors.zju.edu.cn# Available Packages# Name : redis# Arch : x86_64# Version : 2.4.10# Release : 1.el6# Size : 213 k# Repo : epel/x86_64# Summary : A persistent key-value database# URL : http://redis.io# License : BSD# Description : Redis is an advanced key-value store. It is similar to memcached but the data# : set is not volatile, and values can be strings, exactly like in memcached, but# : also lists, sets, and ordered sets. All this data types can be manipulated with# : atomic operations to push/pop elements, add/remove elements, perform server# : side union, intersection, difference between sets, and so forth. Redis supports# : different kind of sorting abilities. 然后从EPEL repo安装相关的依赖关系（jemalloc）： 1yum --enablerepo=epel install jemalloc 在安装之前，您应该停止旧的Redis守护进程： 1service redis stop 然后安装更新版本的Redis： 1sudo yum --enablerepo=remi install redis 服务管理重新启动Redis守护程序，并使其重新启动时自动启动： 12sudo service redis startsudo chkconfig redis on 基本服务操作12345678910111213141516171819## 启动并后台运行$ redis-server &amp; nohup## 查是否启动$ redis-cli ping## 关闭命令$ redis-cli shutdown# 命令行客户端启动$ redis-cli start# 启动$ service redis start# 停止$ service redis stop# 命令行客户端启动$ redis-cli -p 6380# 指定端口后台启动$ redis-server --port 6380 &amp; 查看版本检查当前安装的Redis版本： 12345# 查看 Redis 版本$ redis-cli info | grep redis_version# 查看端口号$ redis-cli info | grep tcp_port 服务管理123systemctl status redis # 查看服务状态systemctl start redis # 启动 Redissystemctl stop redis # 启动 Redis 开机启动如果你是通过yum安装，可以使用下面方式开机启动。 12systemctl enable redis.servicechkconfig redis on 如果你是编译安装可通过下面方式设置开机启动 我们将在 Redis 安装目录找到/usr/local/redis-4.0.0/utils这个目录，在这个目录中有个有个脚本 redis_init_script，将此脚本拷贝到/etc/init.d目录下，命名为redis: 1cp /usr/local/redis-4.0.0/utils/redis_init_script /etc/init.d/redis 拷贝一下redis.conf 文件到/etc/redis目录下 1cp /usr/local/redis-4.0.0/redis.conf /etc/redis/6380.conf 配置文件6380.conf需要更改几个地方 12# 是否在后台执行，yes：后台运行；no：不是后台运行（老版本默认）daemonize yes 更改权限，通过 chkconfig 命令检查、设置系统redis服务开启 12chmod +x /etc/init.d/redischkconfig redis on 必须把下面两行注释放在 /etc/init.d/redis 文件头部，不设置会报不支持的提示 service redis does not support chkconfig 12# chkconfig: 2345 90 10# description: redis is a persistent key-value database 上面的注释的意思是，redis服务必须在运行级2，3，4，5下被启动或关闭，启动的优先级是90，关闭的优先级是10。 Redis 启动警告错误解决 WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add ‘vm.overcommit_memory = 1’ to /etc/sysctl.conf and then reboot or run the command ‘sysctl vm.overcommit_memory=1’ for this to take effect. 12echo "vm.overcommit_memory=1" &gt; /etc/sysctl.conf # 或 vi /etcsysctl.conf , 然后reboot重启机器echo 1 &gt; /proc/sys/vm/overcommit_memory # 不需要启机器就生效 WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1echo 511 &gt; /proc/sys/net/core/somaxconn 更改配置Redis 的配置文件位于 Redis 安装目录下，文件名为 redis.conf。上面已经将它拷贝到/etc/redis目录下了，配置文件在 sudo vi /etc/redis/6380.conf 这里可以编辑 1sudo vi /etc/redis/6380.conf 你可以通过 CONFIG 命令查看或设置配置项。配置设置命令 12345678910## 进入redis获取127.0.0.1:6379&gt; CONFIG GET CONFIG_SETTING_NAME## 进入redis设置127.0.0.1:6379&gt; CONFIG SET CONFIG_SETTING_NAME NEW_CONFIG_VALUE## 获取所有配置127.0.0.1:6379&gt; CONFIG GET *## 设置配置开启通知功能$ redis-cli config set notify-keyspace-events KEA 设置请求密码打开 vim /etc/redis/6380.conf 配置文件编辑它，更改下面两项内容： 1234567# ...# requirepass foobared# 更改成下面内容，把 foobared 改为你想要设置的密码requirepass 111111# bind 127.0.0.1 去掉 # 注释并改为bind 0.0.0.0 更改为之后你就可以带密码访问redis了。 1redis-cli -h 127.0.0.1 -p 6379 -a 111111 主从架构配置假设有两台服务器，一台做主，一台做从 123456Redis 主信息： IP：12.168.1.114 端口：6379Redis 从信息： IP：12.168.1.115 端口：6379 编辑从机的 Redis 配置文件，找到 # slaveof开头的这一行应该是注释的：# slaveof &lt;masterip&gt; &lt;masterport&gt;我们需要去掉该注释，并且填写我们自己的主机的 IP 和 端口，比如：slaveof 192.168.1.114 6379。 123# slaveof &lt;自己的主机的 IP&gt; &lt;自己的主机的 端口&gt;# 这行更改成下面内容slaveof 192.168.1.114 6379 配置完成后重启从机 Redis 服务重启完之后，进入主机的 redis-cli 状态下，输入：INFO replication 可以查询到当前主机的 redis 处于什么角色，有哪些从机已经连上主机。 此时已经完成了主从配置，我们可以测试下： 我们进入主机的 redis-cli 状态，然后 set 某个值，比如：set mygithub jaywcjlove 我们切换进入从机的 redis-cli 的状态下，获取刚刚设置的值看是否存在：get mygithub，此时，我们可以发现是可以获取到值的。 但是有一个需要注意的：从库不具备写入数据能力，不然会报错。 从库只有只读能力。 基本操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## 命令行客户端启动$ redis-cli# 测试心跳127.0.0.1:6379&gt; pingPONG# 设置 mykey 键的值127.0.0.1:6379&gt; set mykey helloOK# 获取 mykey 键的值127.0.0.1:6379&gt; get mykey"hello"## 设置 mykey 失效事件127.0.0.1:6379&gt; expire mykey 2# 查看当前redis的配置信息127.0.0.1:6379&gt; config get *# 获取所有的key127.0.0.1:6379&gt; keys *# 删除redis当前数据库中的所有Key127.0.0.1:6379&gt; flushdb127.0.0.1:6379&gt; config get dir# 馋哭当前库 key 的数量127.0.0.1:6379&gt; dbsize# 删除所有数据库中的key127.0.0.1:6379&gt; flushall# 退出127.0.0.1:6379&gt; exit# 找出拖慢 Redis 的罪魁祸首# 通过这个工具可以查看所有命令统计的快照，# 比如命令执行了多少次，# 执行命令所耗费的毫秒数(每个命令的总时间和平均时间)# 只需要简单地执行 CONFIG RESETSTAT 命令就可以重置，这样你就可以得到一个全新的统计结果。127.0.0.1:6379&gt; commandstatscmdstat_get:calls=78,usec=608,usec_per_call=7.79cmdstat_setex:calls=5,usec=71,usec_per_call=14.20cmdstat_keys:calls=2,usec=42,usec_per_call=21.00cmdstat_info:calls=10,usec=1931,usec_per_call=193.10 支持的数据类型字符串123456# 启动客户端 ,存储字符串到redis.redis&gt; SET name forezpOK# 取字符串:redis&gt; get name "forezp" Hashes - 哈希值12345678redis &gt; HMSET king username forezp password xxdxx age 22redis &gt; HGETALL king1) "username"2) "forezp "3) "password "4) "xxdxx "5) "age "6) "22" Lists - 列表12345678910redis&gt; lpush pricess kenny(integer) 1redis 127.0.0.1:6379&gt; lpush pricess jolin(integer) 2redis 127.0.0.1:6379&gt; lpush pricess mayun(integer) 3redis 127.0.0.1:6379&gt; lrange pricess 0 101) "kenny"2) "jolin"3) "mayun" 有序集合1234567891011121314151617redis &gt; ZADD kindom 1 redis(integer) 1redis&gt; ZADD kindom 2 mongodb(integer) 1redis &gt; ZADD kindom 3 mysql(integer) 1redis &gt; ZADD kindom 3 mysql(integer) 0redis &gt; ZADD kindom 4 mysql(integer) 0redis &gt; ZRANGE kindom 0 10 WITHSCORES1) "redis"2) "1"3) "mongodb"4) "2"5) "mysql"6) "4" 开启通知键空间事件通知默认被禁用，因为这个特性消耗CPU电量不是很明智。使用redis.conf的notify-keyspace-events，或者通过CONFIG SET来开启通知。 123456## 设置配置开启通知功能$ redis-cli config set notify-keyspace-events KEA## 命令行监控所有通知$ redis-cli --csv psubscribe '__key*__:*'Reading messages... (press Ctrl-C to quit)"psubscribe","__key*__:*",1 键值说明 1234567891011K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix. E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix. g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ... $ String commands l List commands s Set commands h Hash commands z Sorted set commands x Expired events (events generated every time a key expires) e Evicted events (events generated when a key is evicted for maxmemory) A Alias for g$lshzxe, so that the "AKE" string means all the events. 不同命令产生的事件(Events generated by different commands) 按照下面的清单，不同的命令产生不同类型的事件。 Redis Keyspace Notifications DEL 为每一个被删除的键产生一个del事件。 RENAME 产生两个事件，为源键产生一个rename_from事件，为目标键产生一个rename_to事件。 EXPIRE 当为键设置过期时产生一个expire事件，或者每当设置了过期的键被删除时产生一个expired事件(查看EXPIRE文档获取更多信息)。 SORT 当STORE用于设置一个新键时产生一个sortstore事件。当结果列表为空，并且使用了STORE选项，并且已经有一个该名字的键存在，那么这个件键被删除，所以这种条件下或产生一个del事件。 SET及其所有变种(SETEX, SETNX,GETSET) 产生set事件。但是SETEX还会产生一个expire事件。 MSET 为每个键产生一个单独的set事件。 SETRANGE 产生一个setrange事件。 INCR, DECR, INCRBY, DECRBY 都产生incrby事件。 INCRBYFLOAT 产生一个incrbyfloat事件。 APPEND 产生一个append事件。 LPUSH和LPUSHX 产生单个lpush事件，即使在可变情况下(even in the variadic case)。 RPUSH和RPUSHX 产生单个rpush事件，即使在可变情况下(even in the variadic case)。 RPOP 产生一个rpop事件。如果键由于最后一个元素被从列表中弹出而导致删除，会又产生一个del事件。 LPOP 产生一个lpop事件。如果键由于最后一个元素被从列表中弹出而导致删除，会又产生一个del事件。 LINSERT 产生一个linsert事件。 LSET 产生一个lset事件。 LREM 产生一个lrem事件。如果结果列表为空并且键被删除，会又产生一个del事件。 LTRIM 产生一个ltrim事件。如果结果列表为空并且键被删除，会又产生一个del事件。 RPOPLPUSH和BRPOPLPUSH 产生一个rpop事件和一个lpush事件。两种情况下顺序都能保证 (lpush事件总是在rpop事件之后被传递) 如果结果列表长度为零并且键被删除，会又产生一个del事件。 HSET, HSETNX和HMSET 都产生单个hset事件。 HINCRBY 产生一个hincrby事件。 HINCRBYFLOAT 产生一个hincrbyfloat事件。 HDEL 产生单个hdel事件。如果结果哈希为空并且键被删除，会又产生一个del事件。 SADD 产生单个sadd事件，即使在可变情况下(even in the variadic case)。 SREM 产生单个srem事件。如果结果集合为空并且键被删除，会又产生一个del事件。 SMOVE 为源键产生一个srem事件为目标键产生一个sadd事件。 SPOP 产生一个spop事件。如果结果集合为空并且键被删除，会又产生一个del事件。 SINTERSTORE, SUNIONSTORE, SDIFFSTORE 分别产生sinterstore，sunionostore，sdiffstore事件。在特殊情况下，集合为空，且存储结果的键已经存在，由于键被删除，会产生一个del事件。 ZINCR 产生一个zincr事件。 ZADD产生单个zadd事件，即使添加了多个元素。. ZREM 产生单个zrem事件，即使删除了多个元素。当结果有序集合为空，并且键被生成时，会产生一个额外的del事件。 ZREMBYSCORE 产生单个zrembyscore事件。当结果有序集合为空，并且键被生成时，会产生一个额外的del事件。 ZREMBYRANK 产生单个zrembyrank事件。当结果有序集合为空，并且键被生成时，会产生一个额外的del事件。 ZINTERSTORE和ZUNIONSTORE 分别产生zinterstore和zunionstore事件。在特殊情况下，集合为空，且存储结果的键已经存在，由于键被删除，会产生一个del事件。 每当一个关联有生存事件的键由于过期而被从数据集中删除时会产生一个expired事件。 每当一个键由于maxmemory策略而从数据集中被淘汰以节省内存时会产生一个evicted事件。 开启远程登录连接使用 netstat 来查看端口占用情况，6379为默认Redis端口。 1netstat -nlt|grep 6379 -t：指明显示 TCP 端口 -u：指明显示 UDP 端口 -l：仅显示监听套接字 -p：显示进程标识符和程序名称，每一个套接字/端口都属于一个程序。 -n：不进行 DNS 轮询，显示 IP （可以加速操作） 修改防火墙配置修改防火墙配置 sudo vi /etc/sysconfig/iptables 1-A INPUT -m state --state NEW -m tcp -p tcp --dport 6379 -j ACCEPT 修改配置文件Redis protected-mode 是3.2 之后加入的新特性，在Redis.conf的注释中，我们可以了解到，他的具体作用和启用条件。可以在 sudo vi /etc/redis.conf 中编辑，修改配置文件。 123456789101112131415161718# Protected mode is a layer of security protection, in order to avoid that# Redis instances left open on the internet are accessed and exploited.## When protected mode is on and if:## 1) The server is not binding explicitly to a set of addresses using the# "bind" directive.# 2) No password is configured.## The server only accepts connections from clients connecting from the# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain# sockets.## By default protected mode is enabled. You should disable it only if# you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces# are explicitly listed using the "bind" directive.protected-mode yes 它启用的条件，有两个： 没有bind IP 没有设置访问密码 如果启用了，则只能够通过lookback ip（127.0.0.1）访问Redis cache，如果从外网访问，则会返回相应的错误信息： 1(error) DENIED Redis is running in protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In this mode connections are only accepted from the lookback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the --portected-mode no option. 4) Setup a bind address or an authentication password. NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside. 提供的原生监控当前链接的客户端数和连接数redis-cli --stat查看当前连接的客户端数，连接数等 123456------- data ------ --------------------- load -------------------- - child -keys mem clients blocked requests connections4 1.27M 6 0 17340 (+0) 1114 1.27M 6 0 17341 (+1) 1114 1.27M 6 0 17342 (+1) 1114 1.27M 6 0 17343 (+1) 111 内存最大的键值和平均的键值数据redis-cli --bigkeys 对当前占用内存最大的键值和平均的键值数据，也可以通过指定-i参数定时查看当前的视图情况。 1234567891011121314151617181920# Scanning the entire keyspace to find biggest keys as well as# average sizes per key type. You can use -i 0.1 to sleep 0.1 sec# per 100 SCAN commands (not usually needed).[00.00%] Biggest string found so far 'asdf.js' with 3 bytes[00.00%] Biggest string found so far 'wabg-tokeneyJhbGciOiJIUzI1NiJ9.NA.UGGRiB2I42rP-33cIMrcoPub7AzHgDlqHacAKFw1pfE' with 328 bytes[00.00%] Biggest string found so far 'wabg-token-province' with 231042 bytes-------- summary -------Sampled 4 keys in the keyspace!Total key length in bytes is 180 (avg len 45.00)Biggest string found 'wabg-token-province' has 231042 bytes4 strings with 231819 bytes (100.00% of keys, avg size 57954.75)0 lists with 0 items (00.00% of keys, avg size 0.00)0 sets with 0 members (00.00% of keys, avg size 0.00)0 hashs with 0 fields (00.00% of keys, avg size 0.00)0 zsets with 0 members (00.00% of keys, avg size 0.00) 查看当前的键值情况redis-cli --scan提供和keys *相似的功能，查看当前的键值情况，可以通过正则表达 1234567891011$ redis-cli --scansess:K4xh-bxOBrcXpy9kEW87oiy-u7I2sAA5asdf.jssess:1tGNZSXW8GyoEQsbtpqkA5tMmSFp_ZInwabg-tokeneyJhbGciOiJIUzI1NiJ9.NA.UGGRiB2I42rP-33cIMrcoPub7AzHgDlqHacAKFw1pfEsess:3e4NGIJd0wf1-RONeTt-FsXQj4EaVNjkwabg-token-provincesess:UuCLAX2sWZ50fiIO1qvDgulf0XIZRd98wabg-tokeneyJhbGciOiJIUzI1NiJ9.MQ.6z44GClzAsUED1M_UyxqdREdDKcYFnL9tSqd5ZhLhsYsess:2HEchaRLYUoaa44IF1bB6mpik7lZjBb4 原生的Monitor监控redis-cli monitor打印出所有sever接收到的命令以及其对应的客户端地址 123456789$ redis-cli monitorOK1472626566.218175 [0 127.0.0.1:62862] "info"1472626571.220948 [0 127.0.0.1:62862] "exists" "aaa"1472626571.223174 [0 127.0.0.1:62862] "set" "aaa" ""1472626571.232126 [0 127.0.0.1:62862] "type" "aaa"1472626571.243697 [0 127.0.0.1:62862] "pttl" "aaa"1472626571.243717 [0 127.0.0.1:62862] "object" "ENCODING" "aaa"1472626571.243726 [0 127.0.0.1:62862] "strlen" "aaa" 配置说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321#redis.conf# Redis configuration file example.# ./redis-server /path/to/redis.conf################################## INCLUDES ####################################这在你有标准配置模板但是每个redis服务器又需要个性设置的时候很有用。# include /path/to/local.conf# include /path/to/other.conf################################ GENERAL ###################################### 是否在后台执行，yes：后台运行；no：不是后台运行（老版本默认）daemonize yes# 3.2里的参数，是否开启保护模式，默认开启。要是配置里没有指定bind和密码。# 开启该参数后，redis只会本地进行访问，拒绝外部访问。# 要是开启了密码 和bind，可以开启。否 则最好关闭，设置为no。protected-mode yes# redis的进程文件pidfile /var/run/redis/redis-server.pid# redis监听的端口号。port 6379# 此参数确定了TCP连接中已完成队列(完成三次握手之后)的长度， # 当然此值必须不大于Linux系统定义的/proc/sys/net/core/somaxconn值，默认是511，# 而Linux的默认参数值是128。当系统并发量大并且客户端速度缓慢的时候，# 可以将这二个参数一起参考设定。该内核参数默认值一般是128，对于负载很大的服务程序来说大大的不够。# 一般会将它修改为2048或者更大。在/etc/sysctl.conf中添加:net.core.somaxconn = 2048，然后在终端中执行sysctl -p。tcp-backlog 511#指定 redis 只接收来自于该 IP 地址的请求，如果不进行设置，那么将处理所有请求bind 127.0.0.1# 配置unix socket来让redis支持监听本地连接。# unixsocket /var/run/redis/redis.sock# 配置unix socket使用文件的权限# unixsocketperm 700# 此参数为设置客户端空闲超过timeout，服务端会断开连接，为0则服务端不会主动断开连接，不能小于0。timeout 0# tcp keepalive参数。如果设置不为0，就使用配置tcp的SO_KEEPALIVE值，# 使用keepalive有两个好处:检测挂掉的对端。降低中间设备出问题而导致网络看似连接却已经与对端端口的问题。# 在Linux内核中，设置了keepalive，redis会定时给对端发送ack。检测到对端关闭需要两倍的设置值。tcp-keepalive 0# 指定了服务端日志的级别。级别包括：debug（很多信息，方便开发、测试），# verbose（许多有用的信息，但是没有debug级别信息多），notice（适当的日志级别，适合生产环境），warn（只有非常重要的信息）loglevel notice#指定了记录日志的文件。空字符串的话，日志会打印到标准输出设备。后台运行的redis标准输出是/dev/null。logfile /var/log/redis/redis-server.log# 是否打开记录syslog功能# syslog-enabled no# syslog的标识符。# syslog-ident redis# 日志的来源、设备# syslog-facility local0# 数据库的数量，默认使用的数据库是DB 0。可以通过”SELECT “命令选择一个dbdatabases 16################################ SNAPSHOTTING ################################# 快照配置# 注释掉“save”这一行配置项就可以让保存数据库功能失效# 设置sedis进行数据库镜像的频率。# 900秒（15分钟）内至少1个key值改变（则进行数据库保存--持久化） # 300秒（5分钟）内至少10个key值改变（则进行数据库保存--持久化） # 60秒（1分钟）内至少10000个key值改变（则进行数据库保存--持久化）save 900 1save 300 10save 60 10000# 当RDB持久化出现错误后，是否依然进行继续进行工作，yes：不能进行工作，no：可以继续进行工作，# 可以通过info中的rdb_last_bgsave_status了解RDB持久化是否有错误stop-writes-on-bgsave-error yes# 使用压缩rdb文件，rdb文件压缩使用LZF压缩算法，yes：压缩，但是需要一些cpu的消耗。no：不压缩，需要更多的磁盘空间rdbcompression yes# 是否校验rdb文件。从rdb格式的第五个版本开始，在rdb文件的末尾会带上CRC64的校验和。# 这跟有利于文件的容错性，但是在保存rdb文件的时候，会有大概10%的性能损耗，所以如果你追求高性能，可以关闭该配置。rdbchecksum yes# rdb文件的名称dbfilename dump.rdb# 数据目录，数据库的写入会在这个目录。rdb、aof文件也会写在这个目录dir /var/lib/redis################################# REPLICATION ################################## 复制选项，slave复制对应的master。# slaveof &lt;masterip&gt; &lt;masterport&gt;#如果master设置了requirepass，那么slave要连上master，需要有master的密码才行。masterauth就是用来配置master的密码，这样可以在连上master后进行认证。# masterauth &lt;master-password&gt;#当从库同主机失去连接或者复制正在进行，从机库有两种运行方式：1) 如果slave-serve-stale-data设置为yes(默认设置)，从库会继续响应客户端的请求。2) 如果slave-serve-stale-data设置为no，除去INFO和SLAVOF命令之外的任何请求都会返回一个错误”SYNC with master in progress”。slave-serve-stale-data yes#作为从服务器，默认情况下是只读的（yes），可以修改成NO，用于写（不建议）。slave-read-only yes#是否使用socket方式复制数据。目前redis复制提供两种方式，disk和socket。如果新的slave连上来或者重连的slave无法部分同步，就会执行全量同步，master会生成rdb文件。有2种方式：disk方式是master创建一个新的进程把rdb文件保存到磁盘，再把磁盘上的rdb文件传递给slave。socket是master创建一个新的进程，直接把rdb文件以socket的方式发给slave。disk方式的时候，当一个rdb保存的过程中，多个slave都能共享这个rdb文件。socket的方式就的一个个slave顺序复制。在磁盘速度缓慢，网速快的情况下推荐用socket方式。repl-diskless-sync no#diskless复制的延迟时间，防止设置为0。一旦复制开始，节点不会再接收新slave的复制请求直到下一个rdb传输。所以最好等待一段时间，等更多的slave连上来。repl-diskless-sync-delay 5#slave根据指定的时间间隔向服务器发送ping请求。时间间隔可以通过 repl_ping_slave_period 来设置，默认10秒。# repl-ping-slave-period 10#复制连接超时时间。master和slave都有超时时间的设置。master检测到slave上次发送的时间超过repl-timeout，即认为slave离线，清除该slave信息。slave检测到上次和master交互的时间超过repl-timeout，则认为master离线。需要注意的是repl-timeout需要设置一个比repl-ping-slave-period更大的值，不然会经常检测到超时。# repl-timeout 60#是否禁止复制tcp链接的tcp nodelay参数，可传递yes或者no。默认是no，即使用tcp nodelay。如果master设置了yes来禁止tcp nodelay设置，在把数据复制给slave的时候，会减少包的数量和更小的网络带宽。但是这也可能带来数据的延迟。默认我们推荐更小的延迟，但是在数据量传输很大的场景下，建议选择yes。repl-disable-tcp-nodelay no#复制缓冲区大小，这是一个环形复制缓冲区，用来保存最新复制的命令。这样在slave离线的时候，不需要完全复制master的数据，如果可以执行部分同步，只需要把缓冲区的部分数据复制给slave，就能恢复正常复制状态。缓冲区的大小越大，slave离线的时间可以更长，复制缓冲区只有在有slave连接的时候才分配内存。没有slave的一段时间，内存会被释放出来，默认1m。# repl-backlog-size 5mb#master没有slave一段时间会释放复制缓冲区的内存，repl-backlog-ttl用来设置该时间长度。单位为秒。# repl-backlog-ttl 3600#当master不可用，Sentinel会根据slave的优先级选举一个master。最低的优先级的slave，当选master。而配置成0，永远不会被选举。slave-priority 100#redis提供了可以让master停止写入的方式，如果配置了min-slaves-to-write，健康的slave的个数小于N，mater就禁止写入。master最少得有多少个健康的slave存活才能执行写命令。这个配置虽然不能保证N个slave都一定能接收到master的写操作，但是能避免没有足够健康的slave的时候，master不能写入来避免数据丢失。设置为0是关闭该功能。# min-slaves-to-write 3#延迟小于min-slaves-max-lag秒的slave才认为是健康的slave。# min-slaves-max-lag 10# 设置1或另一个设置为0禁用这个特性。# Setting one or the other to 0 disables the feature.# By default min-slaves-to-write is set to 0 (feature disabled) and# min-slaves-max-lag is set to 10.################################## SECURITY ####################################requirepass配置可以让用户使用AUTH命令来认证密码，才能使用其他命令。这让redis可以使用在不受信任的网络中。为了保持向后的兼容性，可以注释该命令，因为大部分用户也不需要认证。使用requirepass的时候需要注意，因为redis太快了，每秒可以认证15w次密码，简单的密码很容易被攻破，所以最好使用一个更复杂的密码。# requirepass foobared#把危险的命令给修改成其他名称。比如CONFIG命令可以重命名为一个很难被猜到的命令，这样用户不能使用，而内部工具还能接着使用。# rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52#设置成一个空的值，可以禁止一个命令# rename-command CONFIG ""################################### LIMITS ##################################### 设置能连上redis的最大客户端连接数量。默认是10000个客户端连接。# 由于redis不区分连接是客户端连接还是内部打开文件或者和slave连接等，所以maxclients最小建议设置到32。# 如果超过了maxclients，redis会给新的连接发送’max number of clients reached’，并关闭连接。# maxclients 10000# redis配置的最大内存容量。当内存满了，需要配合maxmemory-policy策略进行处理。# 注意slave的输出缓冲区是不计算在maxmemory内的。所以为了防止主机内存使用完，建议设置的maxmemory需要更小一些。# maxmemory &lt;bytes&gt;# 内存容量超过maxmemory后的处理策略。# volatile-lru：利用LRU算法移除设置过过期时间的key。# volatile-random：随机移除设置过过期时间的key。# volatile-ttl：移除即将过期的key，根据最近过期时间来删除（辅以TTL）# allkeys-lru：利用LRU算法移除任何key。# allkeys-random：随机移除任何key。# noeviction：不移除任何key，只是返回一个写错误。# 上面的这些驱逐策略，如果redis没有合适的key驱逐，对于写命令，还是会返回错误。# redis将不再接收写请求，只接收get请求。# 写命令包括：set setnx setex append incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby getset mset msetnx exec sort。# maxmemory-policy noeviction#lru检测的样本数。使用lru或者ttl淘汰算法，从需要淘汰的列表中随机选择sample个key，选出闲置时间最长的key移除。# maxmemory-samples 5############################## APPEND ONLY MODE ################################默认redis使用的是rdb方式持久化，这种方式在许多应用中已经足够用了。但是redis如果中途宕机，会导致可能有几分钟的数据丢失，根据save来策略进行持久化，Append Only File是另一种持久化方式，可以提供更好的持久化特性。Redis会把每次写入的数据在接收后都写入 appendonly.aof 文件，每次启动时Redis都会先把这个文件的数据读入内存里，先忽略RDB文件。appendonly no#aof文件名appendfilename "appendonly.aof"# aof持久化策略的配置# no表示不执行fsync，由操作系统保证数据同步到磁盘，速度最快。# always表示每次写入都执行fsync，以保证数据同步到磁盘。# everysec表示每秒执行一次fsync，可能会导致丢失这1s数据。appendfsync everysec# 在aof重写或者写入rdb文件的时候，会执行大量IO，# 此时对于everysec和always的aof模式来说，执行fsync会造成阻塞过长时间，# no-appendfsync-on-rewrite字段设置为默认设置为no。# 如果对延迟要求很高的应用，这个字段可以设置为yes，否则还是设置为no，# 这样对持久化特性来说这是更安全的选择。设置为yes表示rewrite期间对新写操作不fsync,# 暂时存在内存中,等rewrite完成后再写入，默认为no，建议yes。Linux的默认fsync策略是30秒。可能丢失30秒数据。no-appendfsync-on-rewrite no# aof自动重写配置。当目前aof文件大小超过上一次重写的aof文件大小的百分之多少进行重写，# 即当aof文件增长到一定大小的时候Redis能够调用bgrewriteaof对日志文件进行重写。# 当前AOF文件大小是上次日志重写得到AOF文件大小的二倍（设置为100）时，自动启动新的日志重写过程。auto-aof-rewrite-percentage 100#设置允许重写的最小aof文件大小，避免了达到约定百分比但尺寸仍然很小的情况还要重写auto-aof-rewrite-min-size 64mb# aof文件可能在尾部是不完整的，当redis启动的时候，aof文件的数据被载入内存。# 重启可能发生在redis所在的主机操作系统宕机后，# 尤其在ext4文件系统没有加上data=ordered选项（redis宕机或者异常终止不会造成尾部不完整现象。）# 出现这种现象，可以选择让redis退出，或者导入尽可能多的数据。如果选择的是yes，# 当截断的aof文件被导入的时候，会自动发布一个log给客户端然后load。如果是no，用户必须手动redis-check-aof修复AOF文件才可以。aof-load-truncated yes################################ LUA SCRIPTING ################################ 如果达到最大时间限制（毫秒），redis会记个log，然后返回error。# 当一个脚本超过了最大时限。# 只有SCRIPT KILL和SHUTDOWN NOSAVE可以用。# 第一个可以杀没有调write命令的东西。要是已经调用了write，只能用第二个命令杀。lua-time-limit 5000################################ REDIS CLUSTER ################################ 集群开关，默认是不开启集群模式。# cluster-enabled yes# 集群配置文件的名称，每个节点都有一个集群相关的配置文件，持久化保存集群的信息。# 这个文件并不需要手动配置，这个配置文件有Redis生成并更新，每个Redis集群节点需要一个单独的配置文件，# 请确保与实例运行的系统中配置文件名称不冲突# cluster-config-file nodes-6379.conf# 节点互连超时的阀值。集群节点超时毫秒数# cluster-node-timeout 15000# 在进行故障转移的时候，全部slave都会请求申请为master，但是有些slave可能与master断开连接一段时间了，# 导致数据过于陈旧，这样的slave不应该被提升为master。该参数就是用来判断slave节点与master断线的时间是否过长。判断方法是：# 比较slave断开连接的时间和(node-timeout * slave-validity-factor) + repl-ping-slave-period# 如果节点超时时间为三十秒, 并且slave-validity-factor为10,假设默认的repl-ping-slave-period是10秒，# 即如果超过310秒slave将不会尝试进行故障转移 # cluster-slave-validity-factor 10# master的slave数量大于该值，slave才能迁移到其他孤立master上，# 如这个参数若被设为2，那么只有当一个主节点拥有2 个可工作的从节点时，它的一个从节点会尝试迁移。# cluster-migration-barrier 1# 默认情况下，集群全部的slot有节点负责，集群状态才为ok，才能提供服务。# 设置为no，可以在slot没有全部分配的时候提供服务。# 不建议打开该配置，这样会造成分区的时候，小分区的master一直在接受写请求，而造成很长时间数据不一致。# cluster-require-full-coverage yes################################## SLOW LOG ######################################slog log是用来记录redis运行中执行比较慢的命令耗时。当命令的执行超过了指定时间，就记录在slow log中，slog log保存在内存中，所以没有IO操作。#执行时间比slowlog-log-slower-than大的请求记录到slowlog里面，单位是微秒，所以1000000就是1秒。注意，负数时间会禁用慢查询日志，而0则会强制记录所有命令。slowlog-log-slower-than 10000#慢查询日志长度。当一个新的命令被写进日志的时候，最老的那个记录会被删掉。这个长度没有限制。只要有足够的内存就行。你可以通过 SLOWLOG RESET 来释放内存。slowlog-max-len 128################################ LATENCY MONITOR ###############################延迟监控功能是用来监控redis中执行比较缓慢的一些操作，用LATENCY打印redis实例在跑命令时的耗时图表。只记录大于等于下边设置的值的操作。0的话，就是关闭监视。默认延迟监控功能是关闭的，如果你需要打开，也可以通过CONFIG SET命令动态设置。latency-monitor-threshold 0############################# EVENT NOTIFICATION ###############################键空间通知使得客户端可以通过订阅频道或模式，来接收那些以某种方式改动了 Redis 数据集的事件。因为开启键空间通知功能需要消耗一些 CPU ，所以在默认配置下，该功能处于关闭状态。#notify-keyspace-events 的参数可以是以下字符的任意组合，它指定了服务器该发送哪些类型的通知：##K 键空间通知，所有通知以 __keyspace@__ 为前缀##E 键事件通知，所有通知以 __keyevent@__ 为前缀##g DEL 、 EXPIRE 、 RENAME 等类型无关的通用命令的通知##$ 字符串命令的通知##l 列表命令的通知##s 集合命令的通知##h 哈希命令的通知##z 有序集合命令的通知##x 过期事件：每当有过期键被删除时发送##e 驱逐(evict)事件：每当有键因为 maxmemory 政策而被删除时发送##A 参数 g$lshzxe 的别名#输入的参数中至少要有一个 K 或者 E，否则的话，不管其余的参数是什么，都不会有任何 通知被分发。详细使用可以参考http://redis.io/topics/notificationsnotify-keyspace-events "KEA"############################### ADVANCED CONFIG ################################ 数据量小于等于hash-max-ziplist-entries的用ziplist，大于hash-max-ziplist-entries用hashhash-max-ziplist-entries 512# value大小小于等于hash-max-ziplist-value的用ziplist，大于hash-max-ziplist-value用hash。hash-max-ziplist-value 64# 数据量小于等于list-max-ziplist-entries用ziplist，大于list-max-ziplist-entries用list。list-max-ziplist-entries 512# value大小小于等于list-max-ziplist-value的用ziplist，大于list-max-ziplist-value用list。list-max-ziplist-value 64# 数据量小于等于set-max-intset-entries用iniset，大于set-max-intset-entries用set。set-max-intset-entries 512# 数据量小于等于zset-max-ziplist-entries用ziplist，大于zset-max-ziplist-entries用zset。zset-max-ziplist-entries 128# value大小小于等于zset-max-ziplist-value用ziplist，大于zset-max-ziplist-value用zset。zset-max-ziplist-value 64# value大小小于等于hll-sparse-max-bytes使用稀疏数据结构（sparse），# 大于hll-sparse-max-bytes使用稠密的数据结构（dense）。# 一个比16000大的value是几乎没用的，建议的value大概为3000。# 如果对CPU要求不高，对空间要求较高的，建议设置到10000左右。hll-sparse-max-bytes 3000# Redis将在每100毫秒时使用1毫秒的CPU时间来对redis的hash表进行重新hash，可以降低内存的使用。# 当你的使用场景中，有非常严格的实时性需要，不能够接受Redis时不时的对请求有2毫秒的延迟的话，把这项配置为no。# 如果没有这么严格的实时性要求，可以设置为yes，以便能够尽可能快的释放内存。activerehashing yes# 对客户端输出缓冲进行限制可以强迫那些不从服务器读取数据的客户端断开连接，用来强制关闭传输缓慢的客户端。# 对于normal client，第一个0表示取消hard limit，第二个0和第三个0表示取消soft limit，normal client默认取消限制，因为如果没有寻问，他们是不会接收数据的。client-output-buffer-limit normal 0 0 0# 对于slave client和MONITER client，如果client-output-buffer一旦超过256mb，又或者超过64mb持续60秒，那么服务器就会立即断开客户端连接。client-output-buffer-limit slave 256mb 64mb 60# 对于pubsub client，如果client-output-buffer一旦超过32mb，又或者超过8mb持续60秒，那么服务器就会立即断开客户端连接。client-output-buffer-limit pubsub 32mb 8mb 60# redis执行任务的频率为1s除以hz。hz 10# 在aof重写的时候，如果打开了aof-rewrite-incremental-fsync开关，系统会每32MB执行一次fsync。这对于把文件写入磁盘是有帮助的，可以避免过大的延迟峰值。aof-rewrite-incremental-fsync yes]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring- MVC 模式]]></title>
    <url>%2F2017%2F09%2F28%2Flang-java-spring-SpringMVC-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[先来看一下什么是 MVC 模式MVC 是一种设计模式. MVC 的原理图如下： SpringMVC 简单介绍SpringMVC 框架是以请求为驱动，围绕 Servlet 设计，将请求发给控制器，然后通过模型对象，分派器来展示请求结果视图。其中核心类是 DispatcherServlet，它是一个 Servlet，顶层是实现的Servlet接口。 SpringMVC 使用需要在 web.xml 中配置 DispatcherServlet 。并且需要配置 Spring 监听器ContextLoaderListener 1234567891011121314151617181920&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt;&lt;/listener&gt;&lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet &lt;/servlet-class&gt; &lt;!-- 如果不设置init-param标签，则必须在/WEB-INF/下创建xxx-servlet.xml文件，其中xxx是servlet-name中配置的名称。 --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/springmvc-servlet.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; SpringMVC 工作原理（重要）简单来说： 客户端发送请求-&gt; 前端控制器 DispatcherServlet 接受客户端请求 -&gt; 找到处理器映射 HandlerMapping 解析请求对应的 Handler-&gt; HandlerAdapter 会根据 Handler 来调用真正的处理器开处理请求，并处理相应的业务逻辑 -&gt; 处理器返回一个模型视图 ModelAndView -&gt; 视图解析器进行解析 -&gt; 返回一个视图对象-&gt;前端控制器 DispatcherServlet 渲染数据（Moder）-&gt;将得到视图对象返回给用户 如下图所示： 上图的一个笔误的小问题：Spring MVC 的入口函数也就是前端控制器 DispatcherServlet 的作用是接收请求，响应结果。 流程说明（重要）： （1）客户端（浏览器）发送请求，直接请求到 DispatcherServlet。 （2）DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。 （3）解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由 HandlerAdapter 适配器处理。 （4）HandlerAdapter 会根据 Handler 来调用真正的处理器开处理请求，并处理相应的业务逻辑。 （5）处理器处理完业务后，会返回一个 ModelAndView 对象，Model 是返回的数据对象，View 是个逻辑上的 View。 （6）ViewResolver 会根据逻辑 View 查找实际的 View。 （7）DispaterServlet 把返回的 Model 传给 View（视图渲染）。 （8）把 View 返回给请求者（浏览器） SpringMVC 重要组件说明1、前端控制器DispatcherServlet（不需要工程师开发）,由框架提供（重要） 作用：Spring MVC 的入口函数。接收请求，响应结果，相当于转发器，中央处理器。有了 DispatcherServlet 减少了其它组件之间的耦合度。用户请求到达前端控制器，它就相当于mvc模式中的c，DispatcherServlet是整个流程控制的中心，由它调用其它组件处理用户的请求，DispatcherServlet的存在降低了组件之间的耦合性。 2、处理器映射器HandlerMapping(不需要工程师开发),由框架提供 作用：根据请求的url查找Handler。HandlerMapping负责根据用户请求找到Handler即处理器（Controller），SpringMVC提供了不同的映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 3、处理器适配器HandlerAdapter 作用：按照特定规则（HandlerAdapter要求的规则）去执行Handler通过HandlerAdapter对处理器进行执行，这是适配器模式的应用，通过扩展适配器可以对更多类型的处理器进行执行。 4、处理器Handler(需要工程师开发) 注意：编写Handler时按照HandlerAdapter的要求去做，这样适配器才可以去正确执行HandlerHandler 是继DispatcherServlet前端控制器的后端控制器，在DispatcherServlet的控制下Handler对具体的用户请求进行处理。由于Handler涉及到具体的用户业务请求，所以一般情况需要工程师根据业务需求开发Handler。 5、视图解析器View resolver(不需要工程师开发),由框架提供 作用：进行视图解析，根据逻辑视图名解析成真正的视图（view）View Resolver负责将处理结果生成View视图，View Resolver首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。 springmvc框架提供了很多的View视图类型，包括：jstlView、freemarkerView、pdfView等。一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户，需要由工程师根据业务需求开发具体的页面。 6、视图View(需要工程师开发) View是一个接口，实现类支持不同的View类型（jsp、freemarker、pdf…） 注意：处理器Handler（也就是我们平常说的Controller控制器）以及视图层view都是需要我们自己手动开发的。其他的一些组件比如：前端控制器DispatcherServlet、处理器映射器HandlerMapping、处理器适配器HandlerAdapter等等都是框架提供给我们的，不需要自己手动开发。 DispatcherServlet详细解析首先看下源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104package org.springframework.web.servlet; @SuppressWarnings("serial")public class DispatcherServlet extends FrameworkServlet &#123; public static final String MULTIPART_RESOLVER_BEAN_NAME = "multipartResolver"; public static final String LOCALE_RESOLVER_BEAN_NAME = "localeResolver"; public static final String THEME_RESOLVER_BEAN_NAME = "themeResolver"; public static final String HANDLER_MAPPING_BEAN_NAME = "handlerMapping"; public static final String HANDLER_ADAPTER_BEAN_NAME = "handlerAdapter"; public static final String HANDLER_EXCEPTION_RESOLVER_BEAN_NAME = "handlerExceptionResolver"; public static final String REQUEST_TO_VIEW_NAME_TRANSLATOR_BEAN_NAME = "viewNameTranslator"; public static final String VIEW_RESOLVER_BEAN_NAME = "viewResolver"; public static final String FLASH_MAP_MANAGER_BEAN_NAME = "flashMapManager"; public static final String WEB_APPLICATION_CONTEXT_ATTRIBUTE = DispatcherServlet.class.getName() + ".CONTEXT"; public static final String LOCALE_RESOLVER_ATTRIBUTE = DispatcherServlet.class.getName() + ".LOCALE_RESOLVER"; public static final String THEME_RESOLVER_ATTRIBUTE = DispatcherServlet.class.getName() + ".THEME_RESOLVER"; public static final String THEME_SOURCE_ATTRIBUTE = DispatcherServlet.class.getName() + ".THEME_SOURCE"; public static final String INPUT_FLASH_MAP_ATTRIBUTE = DispatcherServlet.class.getName() + ".INPUT_FLASH_MAP"; public static final String OUTPUT_FLASH_MAP_ATTRIBUTE = DispatcherServlet.class.getName() + ".OUTPUT_FLASH_MAP"; public static final String FLASH_MAP_MANAGER_ATTRIBUTE = DispatcherServlet.class.getName() + ".FLASH_MAP_MANAGER"; public static final String EXCEPTION_ATTRIBUTE = DispatcherServlet.class.getName() + ".EXCEPTION"; public static final String PAGE_NOT_FOUND_LOG_CATEGORY = "org.springframework.web.servlet.PageNotFound"; private static final String DEFAULT_STRATEGIES_PATH = "DispatcherServlet.properties"; protected static final Log pageNotFoundLogger = LogFactory.getLog(PAGE_NOT_FOUND_LOG_CATEGORY); private static final Properties defaultStrategies; static &#123; try &#123; ClassPathResource resource = new ClassPathResource(DEFAULT_STRATEGIES_PATH, DispatcherServlet.class); defaultStrategies = PropertiesLoaderUtils.loadProperties(resource); &#125; catch (IOException ex) &#123; throw new IllegalStateException("Could not load 'DispatcherServlet.properties': " + ex.getMessage()); &#125; &#125; /** Detect all HandlerMappings or just expect "handlerMapping" bean? */ private boolean detectAllHandlerMappings = true; /** Detect all HandlerAdapters or just expect "handlerAdapter" bean? */ private boolean detectAllHandlerAdapters = true; /** Detect all HandlerExceptionResolvers or just expect "handlerExceptionResolver" bean? */ private boolean detectAllHandlerExceptionResolvers = true; /** Detect all ViewResolvers or just expect "viewResolver" bean? */ private boolean detectAllViewResolvers = true; /** Throw a NoHandlerFoundException if no Handler was found to process this request? **/ private boolean throwExceptionIfNoHandlerFound = false; /** Perform cleanup of request attributes after include request? */ private boolean cleanupAfterInclude = true; /** MultipartResolver used by this servlet */ private MultipartResolver multipartResolver; /** LocaleResolver used by this servlet */ private LocaleResolver localeResolver; /** ThemeResolver used by this servlet */ private ThemeResolver themeResolver; /** List of HandlerMappings used by this servlet */ private List&lt;HandlerMapping&gt; handlerMappings; /** List of HandlerAdapters used by this servlet */ private List&lt;HandlerAdapter&gt; handlerAdapters; /** List of HandlerExceptionResolvers used by this servlet */ private List&lt;HandlerExceptionResolver&gt; handlerExceptionResolvers; /** RequestToViewNameTranslator used by this servlet */ private RequestToViewNameTranslator viewNameTranslator; private FlashMapManager flashMapManager; /** List of ViewResolvers used by this servlet */ private List&lt;ViewResolver&gt; viewResolvers; public DispatcherServlet() &#123; super(); &#125; public DispatcherServlet(WebApplicationContext webApplicationContext) &#123; super(webApplicationContext); &#125; @Override protected void onRefresh(ApplicationContext context) &#123; initStrategies(context); &#125; protected void initStrategies(ApplicationContext context) &#123; initMultipartResolver(context); initLocaleResolver(context); initThemeResolver(context); initHandlerMappings(context); initHandlerAdapters(context); initHandlerExceptionResolvers(context); initRequestToViewNameTranslator(context); initViewResolvers(context); initFlashMapManager(context); &#125;&#125; DispatcherServlet类中的属性beans： HandlerMapping：用于handlers映射请求和一系列的对于拦截器的前处理和后处理，大部分用@Controller注解。 HandlerAdapter：帮助DispatcherServlet处理映射请求处理程序的适配器，而不用考虑实际调用的是 哪个处理程序。- - - ViewResolver：根据实际配置解析实际的View类型。 ThemeResolver：解决Web应用程序可以使用的主题，例如提供个性化布局。 MultipartResolver：解析多部分请求，以支持从HTML表单上传文件。- FlashMapManager：存储并检索可用于将一个请求属性传递到另一个请求的input和output的FlashMap，通常用于重定向。 在Web MVC框架中，每个DispatcherServlet都拥自己的WebApplicationContext，它继承了ApplicationContext。WebApplicationContext包含了其上下文和Servlet实例之间共享的所有的基础框架beans。 HandlerMapping HandlerMapping接口处理请求的映射HandlerMapping接口的实现类： SimpleUrlHandlerMapping类通过配置文件把URL映射到Controller类。 DefaultAnnotationHandlerMapping类通过注解把URL映射到Controller类。 HandlerAdapter HandlerAdapter接口-处理请求映射 AnnotationMethodHandlerAdapter：通过注解，把请求URL映射到Controller类的方法上。 HandlerExceptionResolver HandlerExceptionResolver接口-异常处理接口 SimpleMappingExceptionResolver通过配置文件进行异常处理。 AnnotationMethodHandlerExceptionResolver：通过注解进行异常处理。 ViewResolver ViewResolver接口解析View视图。 UrlBasedViewResolver类 通过配置文件，把一个视图名交给到一个View来处理。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-Bean]]></title>
    <url>%2F2017%2F09%2F28%2Flang-java-spring-SpringBean%2F</url>
    <content type="text"><![CDATA[前言在 Spring 中，那些组成应用程序的主体及由 Spring IOC 容器所管理的对象，被称之为 bean。简单地讲，bean 就是由 IOC 容器初始化、装配及管理的对象，除此之外，bean 就与应用程序中的其他对象没有什么区别了。而 bean 的定义以及 bean 相互间的依赖关系将通过配置元数据来描述。 Spring中的bean默认都是单例的，这些单例Bean在多线程程序下如何保证线程安全呢？ 例如对于Web应用来说，Web容器对于每个用户请求都创建一个单独的Sevlet线程来处理请求，引入Spring框架之后，每个Action都是单例的，那么对于Spring托管的单例Service Bean，如何保证其安全呢？ Spring的单例是基于BeanFactory也就是Spring容器的，单例Bean在此容器内只有一个，Java的单例是基于 JVM，每个 JVM 内只有一个实例。 在大多数情况下。单例 bean 是很理想的方案。不过，有时候你可能会发现你所使用的类是易变的，它们会保持一些状态，因此重用是不安全的。在这种情况下，将 class 声明为单例的就不是那么明智了。因为对象会被污染，稍后重用的时候会出现意想不到的问题。所以 Spring 定义了多种作用域的bean。 一 bean的作用域创建一个bean定义，其实质是用该bean定义对应的类来创建真正实例的“配方”。把bean定义看成一个配方很有意义，它与class很类似，只根据一张“处方”就可以创建多个实例。不仅可以控制注入到对象中的各种依赖和配置值，还可以控制该对象的作用域。这样可以灵活选择所建对象的作用域，而不必在Java Class级定义作用域。Spring Framework支持五种作用域，分别阐述如下表。 五种作用域中，request、session 和 global session 三种作用域仅在基于web的应用中使用（不必关心你所采用的是什么web应用框架），只能用在基于 web 的 Spring ApplicationContext 环境。 1. singleton——唯一 bean 实例当一个 bean 的作用域为 singleton，那么Spring IoC容器中只会存在一个共享的 bean 实例，并且所有对 bean 的请求，只要 id 与该 bean 定义相匹配，则只会返回bean的同一实例。 singleton 是单例类型(对应于单例模式)，就是在创建起容器时就同时自动创建了一个bean的对象，不管你是否使用，但我们可以指定Bean节点的 lazy-init=”true” 来延迟初始化bean，这时候，只有在第一次获取bean时才会初始化bean，即第一次请求该bean时才初始化。 每次获取到的对象都是同一个对象。注意，singleton 作用域是Spring中的缺省作用域。要在XML中将 bean 定义成 singleton ，可以这样配置： 1&lt;bean id="ServiceImpl" class="cn.csdn.service.ServiceImpl" scope="singleton"&gt; 也可以通过 @Scope 注解（它可以显示指定bean的作用范围。）的方式 12345@Service@Scope("singleton")public class ServiceImpl&#123;&#125; 2. prototype——每次请求都会创建一个新的 bean 实例当一个bean的作用域为 prototype，表示一个 bean 定义对应多个对象实例。 prototype 作用域的 bean 会导致在每次对该 bean 请求（将其注入到另一个 bean 中，或者以程序的方式调用容器的 getBean() 方法）时都会创建一个新的 bean 实例。prototype 是原型类型，它在我们创建容器的时候并没有实例化，而是当我们获取bean的时候才会去创建一个对象，而且我们每次获取到的对象都不是同一个对象。根据经验，对有状态的 bean 应该使用 prototype 作用域，而对无状态的 bean 则应该使用 singleton 作用域。 在 XML 中将 bean 定义成 prototype ，可以这样配置： 123&lt;bean id="account" class="com.foo.DefaultAccount" scope="prototype"/&gt; 或者&lt;bean id="account" class="com.foo.DefaultAccount" singleton="false"/&gt; 通过 @Scope 注解的方式实现就不做演示了。 3. request——每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效request只适用于Web程序，每一次 HTTP 请求都会产生一个新的bean，同时该bean仅在当前HTTP request内有效，当请求结束后，该对象的生命周期即告结束。 在 XML 中将 bean 定义成 request ，可以这样配置： 1&lt;bean id="loginAction" class=cn.csdn.LoginAction" scope="request"/&gt; 4. session——每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效session只适用于Web程序，session 作用域表示该针对每一次 HTTP 请求都会产生一个新的 bean，同时该 bean 仅在当前 HTTP session 内有效.与request作用域一样，可以根据需要放心的更改所创建实例的内部状态，而别的 HTTP session 中根据 userPreferences 创建的实例，将不会看到这些特定于某个 HTTP session 的状态变化。当HTTP session最终被废弃的时候，在该HTTP session作用域内的bean也会被废弃掉。 1&lt;bean id="userPreferences" class="com.foo.UserPreferences" scope="session"/&gt; 5. globalSessionglobal session 作用域类似于标准的 HTTP session 作用域，不过仅仅在基于 portlet 的 web 应用中才有意义。Portlet 规范定义了全局 Session 的概念，它被所有构成某个 portlet web 应用的各种不同的 portle t所共享。在global session 作用域中定义的 bean 被限定于全局portlet Session的生命周期范围内。 1&lt;bean id="user" class="com.foo.Preferences "scope="globalSession"/&gt; 二 bean的生命周期Spring Bean是Spring应用中最最重要的部分了。所以来看看Spring容器在初始化一个bean的时候会做那些事情，顺序是怎样的，在容器关闭的时候，又会做哪些事情。 spring版本：4.2.3.RELEASE鉴于Spring源码是用gradle构建的，我也决定舍弃我大maven，尝试下洪菊推荐过的gradle。运行beanLifeCycle模块下的junit test即可在控制台看到如下输出，可以清楚了解Spring容器在创建，初始化和销毁Bean的时候依次做了那些事情。 12345678910111213141516171819202122232425Spring容器初始化=====================================调用GiraffeService无参构造函数GiraffeService中利用set方法设置属性值调用setBeanName:: Bean Name defined in context=giraffeService调用setBeanClassLoader,ClassLoader Name = sun.misc.Launcher$AppClassLoader调用setBeanFactory,setBeanFactory:: giraffe bean singleton=true调用setEnvironment调用setResourceLoader:: Resource File Name=spring-beans.xml调用setApplicationEventPublisher调用setApplicationContext:: Bean Definition Names=[giraffeService, org.springframework.context.annotation.CommonAnnotationBeanPostProcessor#0, com.giraffe.spring.service.GiraffeServicePostProcessor#0]执行BeanPostProcessor的postProcessBeforeInitialization方法,beanName=giraffeService调用PostConstruct注解标注的方法执行InitializingBean接口的afterPropertiesSet方法执行配置的init-method执行BeanPostProcessor的postProcessAfterInitialization方法,beanName=giraffeServiceSpring容器初始化完毕=====================================从容器中获取Beangiraffe Name=李光洙=====================================调用preDestroy注解标注的方法执行DisposableBean接口的destroy方法执行配置的destroy-methodSpring容器关闭 先来看看，Spring在Bean从创建到销毁的生命周期中可能做得事情。 initialization 和 destroy有时我们需要在Bean属性值set好之后和Bean销毁之前做一些事情，比如检查Bean中某个属性是否被正常的设置好值了。Spring框架提供了多种方法让我们可以在Spring Bean的生命周期中执行initialization和pre-destroy方法。 1.实现InitializingBean和DisposableBean接口 这两个接口都只包含一个方法。通过实现InitializingBean接口的afterPropertiesSet()方法可以在Bean属性值设置好之后做一些操作，实现DisposableBean接口的destroy()方法可以在销毁Bean之前做一些操作。 例子如下： 12345678910public class GiraffeService implements InitializingBean,DisposableBean &#123; @Override public void afterPropertiesSet() throws Exception &#123; System.out.println("执行InitializingBean接口的afterPropertiesSet方法"); &#125; @Override public void destroy() throws Exception &#123; System.out.println("执行DisposableBean接口的destroy方法"); &#125;&#125; 这种方法比较简单，但是不建议使用。因为这样会将Bean的实现和Spring框架耦合在一起。 2.在bean的配置文件中指定init-method和destroy-method方法 Spring允许我们创建自己的 init 方法和 destroy 方法，只要在 Bean 的配置文件中指定 init-method 和 destroy-method 的值就可以在 Bean 初始化时和销毁之前执行一些操作。 例子如下： 12345678910public class GiraffeService &#123; //通过&lt;bean&gt;的destroy-method属性指定的销毁方法 public void destroyMethod() throws Exception &#123; System.out.println("执行配置的destroy-method"); &#125; //通过&lt;bean&gt;的init-method属性指定的初始化方法 public void initMethod() throws Exception &#123; System.out.println("执行配置的init-method"); &#125;&#125; 配置文件中的配置： 12&lt;bean name=&quot;giraffeService&quot; class=&quot;com.giraffe.spring.service.GiraffeService&quot; init-method=&quot;initMethod&quot; destroy-method=&quot;destroyMethod&quot;&gt;&lt;/bean&gt; 需要注意的是自定义的init-method和post-method方法可以抛异常但是不能有参数。 这种方式比较推荐，因为可以自己创建方法，无需将Bean的实现直接依赖于spring的框架。 3.使用@PostConstruct和@PreDestroy注解 除了xml配置的方式，Spring 也支持用 @PostConstruct和 @PreDestroy注解来指定 init 和 destroy 方法。这两个注解均在javax.annotation 包中。为了注解可以生效，需要在配置文件中定义org.springframework.context.annotation.CommonAnnotationBeanPostProcessor或context:annotation-config 例子如下： 12345678910public class GiraffeService &#123; @PostConstruct public void initPostConstruct()&#123; System.out.println("执行PostConstruct注解标注的方法"); &#125; @PreDestroy public void preDestroy()&#123; System.out.println("执行preDestroy注解标注的方法"); &#125;&#125; 配置文件： 12 &lt;bean class="org.springframework.context.annotation.CommonAnnotationBeanPostProcessor" /&gt; 实现*Aware接口 在Bean中使用Spring框架的一些对象有些时候我们需要在 Bean 的初始化中使用 Spring 框架自身的一些对象来执行一些操作，比如获取 ServletContext 的一些参数，获取 ApplicaitionContext 中的 BeanDefinition 的名字，获取 Bean 在容器中的名字等等。为了让 Bean 可以获取到框架自身的一些对象，Spring 提供了一组名为*Aware的接口。 这些接口均继承于org.springframework.beans.factory.Aware标记接口，并提供一个将由 Bean 实现的set*方法,Spring通过基于setter的依赖注入方式使相应的对象可以被Bean使用。网上说，这些接口是利用观察者模式实现的，类似于servlet listeners，目前还不明白，不过这也不在本文的讨论范围内。介绍一些重要的Aware接口： ApplicationContextAware: 获得ApplicationContext对象,可以用来获取所有Bean definition的名字。 BeanFactoryAware:获得BeanFactory对象，可以用来检测Bean的作用域。 BeanNameAware:获得Bean在配置文件中定义的名字。 ResourceLoaderAware:获得ResourceLoader对象，可以获得classpath中某个文件。 ServletContextAware:在一个MVC应用中可以获取ServletContext对象，可以读取context中的参数。 ServletConfigAware： 在一个MVC应用中可以获取ServletConfig对象，可以读取config中的参数。 12345678910111213141516171819202122232425262728293031323334353637383940public class GiraffeService implements ApplicationContextAware, ApplicationEventPublisherAware, BeanClassLoaderAware, BeanFactoryAware, BeanNameAware, EnvironmentAware, ImportAware, ResourceLoaderAware&#123; @Override public void setBeanClassLoader(ClassLoader classLoader) &#123; System.out.println("执行setBeanClassLoader,ClassLoader Name = " + classLoader.getClass().getName()); &#125; @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123; System.out.println("执行setBeanFactory,setBeanFactory:: giraffe bean singleton=" + beanFactory.isSingleton("giraffeService")); &#125; @Override public void setBeanName(String s) &#123; System.out.println("执行setBeanName:: Bean Name defined in context=" + s); &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; System.out.println("执行setApplicationContext:: Bean Definition Names=" + Arrays.toString(applicationContext.getBeanDefinitionNames())); &#125; @Override public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) &#123; System.out.println("执行setApplicationEventPublisher"); &#125; @Override public void setEnvironment(Environment environment) &#123; System.out.println("执行setEnvironment"); &#125; @Override public void setResourceLoader(ResourceLoader resourceLoader) &#123; Resource resource = resourceLoader.getResource("classpath:spring-beans.xml"); System.out.println("执行setResourceLoader:: Resource File Name=" + resource.getFilename()); &#125; @Override public void setImportMetadata(AnnotationMetadata annotationMetadata) &#123; System.out.println("执行setImportMetadata"); &#125;&#125; BeanPostProcessor上面的*Aware接口是针对某个实现这些接口的Bean定制初始化的过程，Spring同样可以针对容器中的所有Bean，或者某些Bean定制初始化过程，只需提供一个实现BeanPostProcessor接口的类即可。 该接口中包含两个方法，postProcessBeforeInitialization和postProcessAfterInitialization。 postProcessBeforeInitialization方法会在容器中的Bean初始化之前执行， postProcessAfterInitialization方法在容器中的Bean初始化之后执行。 例子如下： 123456789101112public class CustomerBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println("执行BeanPostProcessor的postProcessBeforeInitialization方法,beanName=" + beanName); return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println("执行BeanPostProcessor的postProcessAfterInitialization方法,beanName=" + beanName); return bean; &#125;&#125; 要将BeanPostProcessor的Bean像其他Bean一样定义在配置文件中 1&lt;bean class="com.giraffe.spring.service.CustomerBeanPostProcessor"/&gt; 总结所以。。。结合第一节控制台输出的内容，Spring Bean的生命周期是这样纸的： Bean容器找到配置文件中 Spring Bean 的定义。 Bean容器利用Java Reflection API创建一个Bean的实例。 如果涉及到一些属性值 利用set方法设置一些属性值。 如果Bean实现了BeanNameAware接口，调用setBeanName()方法，传入Bean的名字。 如果Bean实现了BeanClassLoaderAware接口，调用setBeanClassLoader()方法，传入ClassLoader对象的实例。 如果Bean实现了BeanFactoryAware接口，调用setBeanClassLoader()方法，传入ClassLoader对象的实例。 与上面的类似，如果实现了其他*Aware接口，就调用相应的方法。 如果有和加载这个Bean的Spring容器相关的BeanPostProcessor对象，执行postProcessBeforeInitialization()方法 如果Bean实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果Bean在配置文件中的定义包含init-method属性，执行指定的方法。 如果有和加载这个Bean的Spring容器相关的BeanPostProcessor对象，执行postProcessAfterInitialization()方法 当要销毁Bean的时候，如果Bean实现了DisposableBean接口，执行destroy()方法。 当要销毁Bean的时候，如果Bean在配置文件中的定义包含destroy-method属性，执行指定的方法。 用图表示一下(图来源:http://www.jianshu.com/p/d00539babca5)： 与之比较类似的中文版本: 其实很多时候我们并不会真的去实现上面说描述的那些接口，那么下面我们就除去那些接口，针对bean的单例和非单例来描述下bean的生命周期： 单例管理的对象当scope=”singleton”，即默认情况下，会在启动容器时（即实例化容器时）时实例化。但我们可以指定Bean节点的lazy-init=”true”来延迟初始化bean，这时候，只有在第一次获取bean时才会初始化bean，即第一次请求该bean时才初始化。如下配置： 1&lt;bean id="ServiceImpl" class="cn.csdn.service.ServiceImpl" lazy-init="true"/&gt; 如果想对所有的默认单例bean都应用延迟初始化，可以在根节点beans设置default-lazy-init属性为true，如下所示： 1&lt;beans default-lazy-init="true" …&gt; 默认情况下，Spring 在读取 xml 文件的时候，就会创建对象。在创建对象的时候先调用构造器，然后调用 init-method 属性值中所指定的方法。对象在被销毁的时候，会调用 destroy-method 属性值中所指定的方法（例如调用Container.destroy()方法的时候）。写一个测试类，代码如下： 1234567891011121314151617181920212223public class LifeBean &#123; private String name; public LifeBean()&#123; System.out.println("LifeBean()构造函数"); &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; System.out.println("setName()"); this.name = name; &#125; public void init()&#123; System.out.println("this is init of lifeBean"); &#125; public void destory()&#123; System.out.println("this is destory of lifeBean " + this); &#125; &#125; life.xml配置如下： 12&lt;bean id="life_singleton" class="com.bean.LifeBean" scope="singleton" init-method="init" destroy-method="destory" lazy-init="true"/&gt; 测试代码： 12345678910public class LifeTest &#123; @Test public void test() &#123; AbstractApplicationContext container = new ClassPathXmlApplicationContext("life.xml"); LifeBean life1 = (LifeBean)container.getBean("life"); System.out.println(life1); container.close(); &#125;&#125; 运行结果： 12345LifeBean()构造函数this is init of lifeBeancom.bean.LifeBean@573f2bb1……this is destory of lifeBean com.bean.LifeBean@573f2bb1 非单例管理的对象当scope=”prototype”时，容器也会延迟初始化 bean，Spring 读取xml 文件的时候，并不会立刻创建对象，而是在第一次请求该 bean 时才初始化（如调用getBean方法时）。在第一次请求每一个 prototype 的bean 时，Spring容器都会调用其构造器创建这个对象，然后调用init-method属性值中所指定的方法。对象销毁的时候，Spring 容器不会帮我们调用任何方法，因为是非单例，这个类型的对象有很多个，Spring容器一旦把这个对象交给你之后，就不再管理这个对象了。 为了测试prototype bean的生命周期life.xml配置如下： 1&lt;bean id="life_prototype" class="com.bean.LifeBean" scope="prototype" init-method="init" destroy-method="destory"/&gt; 测试程序： 123456789101112public class LifeTest &#123; @Test public void test() &#123; AbstractApplicationContext container = new ClassPathXmlApplicationContext("life.xml"); LifeBean life1 = (LifeBean)container.getBean("life_singleton"); System.out.println(life1); LifeBean life3 = (LifeBean)container.getBean("life_prototype"); System.out.println(life3); container.close(); &#125;&#125; 运行结果： 12345678LifeBean()构造函数this is init of lifeBeancom.bean.LifeBean@573f2bb1LifeBean()构造函数this is init of lifeBeancom.bean.LifeBean@5ae9a829……this is destory of lifeBean com.bean.LifeBean@573f2bb1 可以发现，对于作用域为 prototype 的 bean ，其destroy方法并没有被调用。如果 bean 的 scope 设为prototype时，当容器关闭时，destroy 方法不会被调用。对于 prototype 作用域的 bean，有一点非常重要，那就是 Spring不能对一个 prototype bean 的整个生命周期负责：容器在初始化、配置、装饰或者是装配完一个prototype实例后，将它交给客户端，随后就对该prototype实例不闻不问了。 不管何种作用域，容器都会调用所有对象的初始化生命周期回调方法。但对prototype而言，任何配置好的析构生命周期回调方法都将不会被调用。清除prototype作用域的对象并释放任何prototype bean所持有的昂贵资源，都是客户端代码的职责（让Spring容器释放被prototype作用域bean占用资源的一种可行方式是，通过使用bean的后置处理器，该处理器持有要被清除的bean的引用）。谈及prototype作用域的bean时，在某些方面你可以将Spring容器的角色看作是Java new操作的替代者，任何迟于该时间点的生命周期事宜都得交由客户端来处理。 Spring 容器可以管理 singleton 作用域下 bean 的生命周期，在此作用域下，Spring 能够精确地知道bean何时被创建，何时初始化完成，以及何时被销毁。而对于 prototype 作用域的bean，Spring只负责创建，当容器创建了 bean 的实例后，bean 的实例就交给了客户端的代码管理，Spring容器将不再跟踪其生命周期，并且不会管理那些被配置成prototype作用域的bean的生命周期。 三 说明本文的完成结合了下面两篇文章，并做了相应修改： https://blog.csdn.net/fuzhongmin05/article/details/73389779 https://yemengying.com/2016/07/14/spring-bean-life-cycle/ 由于本文非本人独立原创，所以未声明为原创！在此说明！]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[excel2016打开文件会有十几秒的卡顿]]></title>
    <url>%2F2017%2F09%2F11%2Foffice-excel-excel2016%E6%89%93%E5%BC%80%E6%96%87%E4%BB%B6%E4%BC%9A%E6%9C%89%E5%8D%81%E5%87%A0%E7%A7%92%E7%9A%84%E5%8D%A1%E9%A1%BF%2F</url>
    <content type="text"><![CDATA[场景在运用华为云桌面开发，并进行了网络管控的情况下，导致云桌面内的excel2016打开文件会有十几秒的卡顿. 解决经过检查发现，excel2016启动时会连接多个美国IP的443端口，因为是SSL无法监听内容，内网机器会导致20秒左右的阻塞，可以利用系统本身的防火墙阻止excel.exe联网，即可解决问题。管理工具-windows防火墙-加出站策略]]></content>
      <categories>
        <category>Office</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列]]></title>
    <url>%2F2017%2F05%2F28%2Fsubject-mq-message-queue%2F</url>
    <content type="text"><![CDATA[消息队列其实很简单 “RabbitMQ？”“Kafka？”“RocketMQ？”…在日常学习与开发过程中，我们常常听到消息队列这个关键词。我也在我的多篇文章中提到了这个概念。可能你是熟练使用消息队列的老手，又或者你是不懂消息队列的新手，不论你了不了解消息队列，本文都将带你搞懂消息队列的一些基本理论。如果你是老手，你可能从本文学到你之前不曾注意的一些关于消息队列的重要概念，如果你是新手，相信本文将是你打开消息队列大门的一板砖。 一 什么是消息队列 我们可以把消息队列比作是一个存放消息的容器，当我们需要使用消息的时候可以取出消息供自己使用。消息队列是分布式系统中重要的组件，使用消息队列主要是为了通过异步处理提高系统性能和削峰、降低系统耦合性。目前使用较多的消息队列有ActiveMQ，RabbitMQ，Kafka，RocketMQ，我们后面会一一对比这些消息队列。 另外，我们知道队列 Queue 是一种先进先出的数据结构，所以消费消息时也是按照顺序来消费的。比如生产者发送消息1,2,3…对于消费者就会按照1,2,3…的顺序来消费。但是偶尔也会出现消息被消费的顺序不对的情况，比如某个消息消费失败又或者一个 queue 多个consumer 也会导致消息被消费的顺序不对，我们一定要保证消息被消费的顺序正确。 除了上面说的消息消费顺序的问题，使用消息队列，我们还要考虑如何保证消息不被重复消费？如何保证消息的可靠性传输（如何处理消息丢失的问题）？……等等问题。所以说使用消息队列也不是十全十美的，使用它也会让系统可用性降低、复杂度提高，另外需要我们保障一致性等问题。 二 为什么要用消息队列 我觉得使用消息队列主要有两点好处：1.通过异步处理提高系统性能（削峰、减少响应所需时间）;2.降低系统耦合性。如果在面试的时候你被面试官问到这个问题的话，一般情况是你在你的简历上涉及到消息队列这方面的内容，这个时候推荐你结合你自己的项目来回答。 《大型网站技术架构》第四章和第七章均有提到消息队列对应用性能及扩展性的提升。 (1) 通过异步处理提高系统性能（削峰、减少响应所需时间） 如上图，在不使用消息队列服务器的时候，用户的请求数据直接写入数据库，在高并发的情况下数据库压力剧增，使得响应速度变慢。但是在使用消息队列之后，用户的请求数据发送给消息队列之后立即 返回，再由消息队列的消费者进程从消息队列中获取数据，异步写入数据库。由于消息队列服务器处理速度快于数据库（消息队列也比数据库有更好的伸缩性），因此响应速度得到大幅改善。 通过以上分析我们可以得出消息队列具有很好的削峰作用的功能——即通过异步处理，将短时间高并发产生的事务消息存储在消息队列中，从而削平高峰期的并发事务。 举例：在电子商务一些秒杀、促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。如下图所示： 因为用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败。因此使用消息队列进行异步处理之后，需要适当修改业务流程进行配合，比如用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功，以免交易纠纷。这就类似我们平时手机订火车票和电影票。 (2) 降低系统耦合性 我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。 我们最常见的事件驱动架构类似生产者消费者模式，在大型网站中通常用利用消息队列实现事件驱动结构。如下图所示： 消息队列使利用发布-订阅模式工作，消息发送者（生产者）发布消息，一个或多个消息接受者（消费者）订阅消息。 从上图可以看到消息发送者（生产者）和消息接受者（消费者）之间没有直接耦合，消息发送者将消息发送至分布式消息队列即结束对消息的处理，消息接受者从分布式消息队列获取该消息后进行后续处理，并不需要知道该消息从何而来。对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计。 消息接受者对消息进行过滤、处理、包装后，构造成一个新的消息类型，将消息继续发送出去，等待其他消息接受者订阅该消息。因此基于事件（消息对象）驱动的业务架构可以是一系列流程。 另外为了避免消息队列服务器宕机造成消息丢失，会将成功发送到消息队列的消息存储在消息生产者服务器上，等消息真正被消费者服务器处理后才删除消息。在消息队列服务器宕机后，生产者服务器会选择分布式消息队列服务器集群中的其他服务器发布消息。 备注： 不要认为消息队列只能利用发布-订阅模式工作，只不过在解耦这个特定业务环境下是使用发布-订阅模式的。除了发布-订阅模式，还有点对点订阅模式（一个消息只有一个消费者），我们比较常用的是发布-订阅模式。 另外，这两种消息模型是 JMS 提供的，AMQP 协议还提供了 5 种消息模型。 三 使用消息队列带来的一些问题 系统可用性降低： 系统可用性在某种程度上降低，为什么这样说呢？在加入MQ之前，你不用考虑消息丢失或者说MQ挂掉等等的情况，但是，引入MQ之后你就需要去考虑了！ 系统复杂性提高： 加入MQ之后，你需要保证消息没有被重复消费、处理消息丢失的情况、保证消息传递的顺序性等等问题！ 一致性问题： 我上面讲了消息队列可以实现异步，消息队列带来的异步确实可以提高系统响应速度。但是，万一消息的真正消费者并没有正确消费消息怎么办？这样就会导致数据不一致的情况了! 四 JMS VS AMQP4.1 JMS4.1.1 JMS 简介 JMS（JAVA Message Service,java消息服务）是java的消息服务，JMS的客户端之间可以通过JMS服务进行异步的消息传输。JMS（JAVA Message Service，Java消息服务）API是一个消息服务的标准或者说是规范，允许应用程序组件基于JavaEE平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。 ActiveMQ 就是基于 JMS 规范实现的。 4.1.2 JMS两种消息模型①点到点（P2P）模型 使用队列（Queue）作为消息通信载体；满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。比如：我们生产者发送100条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费。） ② 发布/订阅（Pub/Sub）模型 发布订阅模型（Pub/Sub） 使用主题（Topic）作为消息通信载体，类似于广播模式；发布者发布一条消息，该消息通过主题传递给所有的订阅者，在一条消息广播之后才订阅的用户则是收不到该条消息的。 4.1.3 JMS 五种不同的消息正文格式 JMS定义了五种不同的消息正文格式，以及调用的消息类型，允许你发送并接收以一些不同形式的数据，提供现有消息格式的一些级别的兼容性。 StreamMessage – Java原始值的数据流 MapMessage–一套名称-值对 TextMessage–一个字符串对象 ObjectMessage–一个序列化的 Java对象 BytesMessage–一个字节的数据流 4.2 AMQP ​ AMQP，即Advanced Message Queuing Protocol，一个提供统一消息服务的应用层标准 高级消息队列协议（二进制应用层协议），是应用层协议的一个开放标准,为面向消息的中间件设计，兼容 JMS。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件同产品，不同的开发语言等条件的限制。 RabbitMQ 就是基于 AMQP 协议实现的。 4.3 JMS vs AMQP 对比方向 JMS AMQP 定义 Java API 协议 跨语言 否 是 跨平台 否 是 支持消息类型 提供两种消息模型：①Peer-2-Peer;②Pub/sub 提供了五种消息模型：①direct exchange；②fanout exchange；③topic change；④headers exchange；⑤system exchange。本质来讲，后四种和JMS的pub/sub模型没有太大差别，仅是在路由机制上做了更详细的划分； 支持消息类型 支持多种消息类型 ，我们在上面提到过 byte[]（二进制） 总结： AMQP 为消息定义了线路层（wire-level protocol）的协议，而JMS所定义的是API规范。在 Java 体系中，多个client均可以通过JMS进行交互，不需要应用修改代码，但是其对跨平台的支持较差。而AMQP天然具有跨平台、跨语言特性。 JMS 支持TextMessage、MapMessage 等复杂的消息类型；而 AMQP 仅支持 byte[] 消息类型（复杂的类型可序列化后发送）。 由于Exchange 提供的路由算法，AMQP可以提供多样化的路由方式来传递消息到消息队列，而 JMS 仅支持 队列 和 主题/订阅 方式两种。 五 常见的消息队列对比 对比方向 概要 吞吐量 万级的 ActiveMQ 和 RabbitMQ 的吞吐量（ActiveMQ 的性能最差）要比 十万级甚至是百万级的 RocketMQ 和 Kafka 低一个数量级。 可用性 都可以实现高可用。ActiveMQ 和 RabbitMQ 都是基于主从架构实现高可用性。RocketMQ 基于分布式架构。 kafka 也是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 时效性 RabbitMQ 基于erlang开发，所以并发能力很强，性能极其好，延时很低，达到微秒级。其他三个都是 ms 级。 功能支持 除了 Kafka，其他三个功能都较为完备。 Kafka 功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用，是事实上的标准 消息丢失 ActiveMQ 和 RabbitMQ 丢失的可能性非常低， RocketMQ 和 Kafka 理论上不会丢失。 总结： ActiveMQ 的社区算是比较成熟，但是较目前来说，ActiveMQ 的性能比较差，而且版本迭代很慢，不推荐使用。 RabbitMQ 在吞吐量方面虽然稍逊于 Kafka 和 RocketMQ ，但是由于它基于 erlang 开发，所以并发能力很强，性能极其好，延时很低，达到微秒级。但是也因为 RabbitMQ 基于 erlang 开发，所以国内很少有公司有实力做erlang源码级别的研究和定制。如果业务场景对并发量要求不是太高（十万级、百万级），那这四种消息队列中，RabbitMQ 一定是你的首选。如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 RocketMQ 阿里出品，Java 系开源项目，源代码我们可以直接阅读，然后可以定制自己公司的MQ，并且 RocketMQ 有阿里巴巴的实际业务场景的实战考验。RocketMQ 社区活跃度相对较为一般，不过也还可以，文档相对来说简单一些，然后接口这块不是按照标准 JMS 规范走的有些系统要迁移需要修改大量代码。还有就是阿里出台的技术，你得做好这个技术万一被抛弃，社区黄掉的风险，那如果你们公司有技术实力我觉得用RocketMQ 挺好的 kafka 的特点其实很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms 级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展。同时 kafka 最好是支撑较少的 topic 数量即可，保证其超高吞吐量。kafka 唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略这个特性天然适合大数据实时计算以及日志收集。]]></content>
      <categories>
        <category>mq</category>
      </categories>
      <tags>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ 介绍]]></title>
    <url>%2F2017%2F05%2F28%2Fsubject-mq-rabbitmq%2F</url>
    <content type="text"><![CDATA[一文搞懂 RabbitMQ 的重要概念以及安装一 RabbitMQ 介绍这部分参考了 《RabbitMQ实战指南》这本书的第 1 章和第 2 章。 1.1 RabbitMQ 简介RabbitMQ 是采用 Erlang 语言实现 AMQP(Advanced Message Queuing Protocol，高级消息队列协议）的消息中间件，它最初起源于金融系统，用于在分布式系统中存储转发消息。 RabbitMQ 发展到今天，被越来越多的人认可，这和它在易用性、扩展性、可靠性和高可用性等方面的卓著表现是分不开的。RabbitMQ 的具体特点可以概括为以下几点： 可靠性： RabbitMQ使用一些机制来保证消息的可靠性，如持久化、传输确认及发布确认等。 灵活的路由： 在消息进入队列之前，通过交换器来路由消息。对于典型的路由功能，RabbitMQ 己经提供了一些内置的交换器来实现。针对更复杂的路由功能，可以将多个交换器绑定在一起，也可以通过插件机制来实现自己的交换器。这个后面会在我们将 RabbitMQ 核心概念的时候详细介绍到。 扩展性： 多个RabbitMQ节点可以组成一个集群，也可以根据实际业务情况动态地扩展集群中节点。 高可用性： 队列可以在集群中的机器上设置镜像，使得在部分节点出现问题的情况下队列仍然可用。 支持多种协议： RabbitMQ 除了原生支持 AMQP 协议，还支持 STOMP、MQTT 等多种消息中间件协议。 多语言客户端： RabbitMQ几乎支持所有常用语言，比如 Java、Python、Ruby、PHP、C#、JavaScript等。 易用的管理界面： RabbitMQ提供了一个易用的用户界面，使得用户可以监控和管理消息、集群中的节点等。在安装 RabbitMQ 的时候会介绍到，安装好 RabbitMQ 就自带管理界面。 插件机制： RabbitMQ 提供了许多插件，以实现从多方面进行扩展，当然也可以编写自己的插件。感觉这个有点类似 Dubbo 的 SPI机制。 1.2 RabbitMQ 核心概念RabbitMQ 整体上是一个生产者与消费者模型，主要负责接收、存储和转发消息。可以把消息传递的过程想象成：当你将一个包裹送到邮局，邮局会暂存并最终将邮件通过邮递员送到收件人的手上，RabbitMQ就好比由邮局、邮箱和邮递员组成的一个系统。从计算机术语层面来说，RabbitMQ 模型更像是一种交换机模型。 下面再来看看图1—— RabbitMQ 的整体模型架构。 下面我会一一介绍上图中的一些概念。 1.2.1 Producer(生产者) 和 Consumer(消费者) Producer(生产者) :生产消息的一方（邮件投递者） Consumer(消费者) :消费消息的一方（邮件收件人） 消息一般由 2 部分组成：消息头（或者说是标签 Label）和 消息体。消息体也可以称为 payLoad ,消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括 routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。生产者把消息交由 RabbitMQ 后，RabbitMQ 会根据消息头把消息发送给感兴趣的 Consumer(消费者)。 1.2.2 Exchange(交换器)在 RabbitMQ 中，消息并不是直接被投递到 Queue(消息队列) 中的，中间还必须经过 Exchange(交换器) 这一层，Exchange(交换器) 会把我们的消息分配到对应的 Queue(消息队列) 中。 Exchange(交换器) 用来接收生产者发送的消息并将这些消息路由给服务器中的队列中，如果路由不到，或许会返回给 Producer(生产者) ，或许会被直接丢弃掉 。这里可以将RabbitMQ中的交换器看作一个简单的实体。 RabbitMQ 的 Exchange(交换器) 有4种类型，不同的类型对应着不同的路由策略：direct(默认)，fanout, topic, 和 headers，不同类型的Exchange转发消息的策略有所区别。这个会在介绍 Exchange Types(交换器类型) 的时候介绍到。 Exchange(交换器) 示意图如下： 生产者将消息发给交换器的时候，一般会指定一个 RoutingKey(路由键)，用来指定这个消息的路由规则，而这个 RoutingKey 需要与交换器类型和绑定键(BindingKey)联合使用才能最终生效。 RabbitMQ 中通过 Binding(绑定) 将 Exchange(交换器) 与 Queue(消息队列) 关联起来，在绑定的时候一般会指定一个 BindingKey(绑定建) ,这样 RabbitMQ 就知道如何正确将消息路由到队列了,如下图所示。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。Exchange 和 Queue 的绑定可以是多对多的关系。 Binding(绑定) 示意图： 生产者将消息发送给交换器时，需要一个RoutingKey,当 BindingKey 和 RoutingKey 相匹配时，消息会被路由到对应的队列中。在绑定多个队列到同一个交换器的时候，这些绑定允许使用相同的 BindingKey。BindingKey 并不是在所有的情况下都生效，它依赖于交换器类型，比如fanout类型的交换器就会无视，而是将消息路由到所有绑定到该交换器的队列中。 1.2.3 Queue(消息队列)Queue(消息队列) 用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 RabbitMQ 中消息只能存储在 队列 中，这一点和 Kafka 这种消息中间件相反。Kafka 将消息存储在 topic（主题） 这个逻辑层面，而相对应的队列逻辑只是topic实际存储文件中的位移标识。 RabbitMQ 的生产者生产消息并最终投递到队列中，消费者可以从队列中获取消息并消费。 多个消费者可以订阅同一个队列，这时队列中的消息会被平均分摊（Round-Robin，即轮询）给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理，这样避免的消息被重复消费。 RabbitMQ 不支持队列层面的广播消费,如果有广播消费的需求，需要在其上进行二次开发,这样会很麻烦，不建议这样做。 1.2.4 Broker（消息中间件的服务节点）对于 RabbitMQ 来说，一个 RabbitMQ Broker 可以简单地看作一个 RabbitMQ 服务节点，或者RabbitMQ服务实例。大多数情况下也可以将一个 RabbitMQ Broker 看作一台 RabbitMQ 服务器。 下图展示了生产者将消息存入 RabbitMQ Broker,以及消费者从Broker中消费数据的整个流程。 这样图1中的一些关于 RabbitMQ 的基本概念我们就介绍完毕了，下面再来介绍一下 Exchange Types(交换器类型) 。 1.2.5 Exchange Types(交换器类型)RabbitMQ 常用的 Exchange Type 有 fanout、direct、topic、headers 这四种（AMQP规范里还提到两种 Exchange Type，分别为 system 与 自定义，这里不予以描述）。 ① fanoutfanout 类型的Exchange路由规则非常简单，它会把所有发送到该Exchange的消息路由到所有与它绑定的Queue中，不需要做任何判断操作，所以 fanout 类型是所有的交换机类型里面速度最快的。fanout 类型常用来广播消息。 ② directdirect 类型的Exchange路由规则也很简单，它会把消息路由到那些 Bindingkey 与 RoutingKey 完全匹配的 Queue 中。 以上图为例，如果发送消息的时候设置路由键为“warning”,那么消息会路由到 Queue1 和 Queue2。如果在发送消息的时候设置路由键为”Info”或者”debug”，消息只会路由到Queue2。如果以其他的路由键发送消息，则消息不会路由到这两个队列中。 direct 类型常用在处理有优先级的任务，根据任务的优先级把消息发送到对应的队列，这样可以指派更多的资源去处理高优先级的队列。 ③ topic前面讲到direct类型的交换器路由规则是完全匹配 BindingKey 和 RoutingKey ，但是这种严格的匹配方式在很多情况下不能满足实际业务的需求。topic类型的交换器在匹配规则上进行了扩展，它与 direct 类型的交换器相似，也是将消息路由到 BindingKey 和 RoutingKey 相匹配的队列中，但这里的匹配规则有些不同，它约定： RoutingKey 为一个点号“．”分隔的字符串（被点号“．”分隔开的每一段独立的字符串称为一个单词），如 “com.rabbitmq.client”、“java.util.concurrent”、“com.hidden.client”; BindingKey 和 RoutingKey 一样也是点号“．”分隔的字符串； BindingKey 中可以存在两种特殊字符串“*”和“#”，用于做模糊匹配，其中“.”用于匹配一个单词，“#”用于匹配多个单词(可以是零个)。 以上图为例： 路由键为 “com.rabbitmq.client” 的消息会同时路由到 Queuel 和 Queue2; 路由键为 “com.hidden.client” 的消息只会路由到 Queue2 中； 路由键为 “com.hidden.demo” 的消息只会路由到 Queue2 中； 路由键为 “java.rabbitmq.demo” 的消息只会路由到Queuel中； 路由键为 “java.util.concurrent” 的消息将会被丢弃或者返回给生产者（需要设置 mandatory 参数），因为它没有匹配任何路由键。 ④ headers(不推荐)headers 类型的交换器不依赖于路由键的匹配规则来路由消息，而是根据发送的消息内容中的 headers 属性进行匹配。在绑定队列和交换器时制定一组键值对，当发送消息到交换器时，RabbitMQ会获取到该消息的 headers（也是一个键值对的形式)’对比其中的键值对是否完全匹配队列和交换器绑定时指定的键值对，如果完全匹配则消息会路由到该队列，否则不会路由到该队列。headers 类型的交换器性能会很差，而且也不实用，基本上不会看到它的存在。 二 安装 RabbitMq通过 Docker 安装非常方便，只需要几条命令就好了，我这里是只说一下常规安装方法。 前面提到了 RabbitMQ 是由 Erlang语言编写的，也正因如此，在安装RabbitMQ 之前需要安装 Erlang。 注意：在安装 RabbitMQ 的时候需要注意 RabbitMQ 和 Erlang 的版本关系，如果不注意的话会导致出错，两者对应关系如下: 2.1 安装 erlang1 下载 erlang 安装包 在官网下载然后上传到 Linux 上或者直接使用下面的命令下载对应的版本。 1[root@SnailClimb local]#wget http://erlang.org/download/otp_src_19.3.tar.gz erlang 官网下载：http://www.erlang.org/downloads 2 解压 erlang 安装包 1[root@SnailClimb local]#tar -xvzf otp_src_19.3.tar.gz 3 删除 erlang 安装包 1[root@SnailClimb local]#rm -rf otp_src_19.3.tar.gz 4 安装 erlang 的依赖工具 1[root@SnailClimb local]#yum -y install make gcc gcc-c++ kernel-devel m4 ncurses-devel openssl-devel unixODBC-devel 5 进入erlang 安装包解压文件对 erlang 进行安装环境的配置 新建一个文件夹 1[root@SnailClimb local]# mkdir erlang 对 erlang 进行安装环境的配置 12[root@SnailClimb otp_src_19.3]# ./configure --prefix=/usr/local/erlang --without-javac 6 编译安装 12[root@SnailClimb otp_src_19.3]# make &amp;&amp; make install 7 验证一下 erlang 是否安装成功了 1[root@SnailClimb otp_src_19.3]# ./bin/erl 运行下面的语句输出“hello world” 1io:format("hello world~n", []). 大功告成，我们的 erlang 已经安装完成。 8 配置 erlang 环境变量 1[root@SnailClimb etc]# vim profile 追加下列环境变量到文件末尾 1234#erlangERL_HOME=/usr/local/erlangPATH=$ERL_HOME/bin:$PATHexport ERL_HOME PATH 运行下列命令使配置文件profile生效 1[root@SnailClimb etc]# source /etc/profile 输入 erl 查看 erlang 环境变量是否配置正确 1[root@SnailClimb etc]# erl 2.2 安装 RabbitMQ1. 下载rpm 1wget https://www.rabbitmq.com/releases/rabbitmq-server/v3.6.8/rabbitmq-server-3.6.8-1.el7.noarch.rpm 或者直接在官网下载 https://www.rabbitmq.com/install-rpm.html[enter link description here](https://www.rabbitmq.com/install-rpm.html) 2. 安装rpm 1rpm --import https://www.rabbitmq.com/rabbitmq-release-signing-key.asc 紧接着执行： 1yum install rabbitmq-server-3.6.8-1.el7.noarch.rpm 中途需要你输入”y”才能继续安装。 3 开启 web 管理插件 1rabbitmq-plugins enable rabbitmq_management 4 设置开机启动 1chkconfig rabbitmq-server on 4. 启动服务 1service rabbitmq-server start 5. 查看服务状态 1service rabbitmq-server status 6. 访问 RabbitMQ 控制台 浏览器访问：http://你的ip地址:15672/ 默认用户名和密码： guest/guest;但是需要注意的是：guestuest用户只是被容许从localhost访问。官网文档描述如下： 1“guest” user can only connect via localhost 解决远程访问 RabbitMQ 远程访问密码错误 新建用户并授权 12345678[root@SnailClimb rabbitmq]# rabbitmqctl add_user root rootCreating user "root" ...[root@SnailClimb rabbitmq]# rabbitmqctl set_user_tags root administratorSetting tags for user "root" to [administrator] ...[root@SnailClimb rabbitmq]# [root@SnailClimb rabbitmq]# rabbitmqctl set_permissions -p / root ".*" ".*" ".*"Setting permissions for user "root" in vhost "/" ... 再次访问:http://你的ip地址:15672/ ,输入用户名和密码：root root]]></content>
      <categories>
        <category>mq</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 设计模式]]></title>
    <url>%2F2017%2F05%2F23%2Fsubject-design-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[创建型模式创建型模式概述 创建型模式(Creational Pattern)对类的实例化过程进行了抽象，能够将软件模块中对象的创建和对象的使用分离。为了使软件的结构更加清晰，外界对于这些对象只需要知道它们共同的接口，而不清楚其具体的实现细节，使整个系统的设计更加符合单一职责原则。 创建型模式在创建什么(What)，由谁创建(Who)，何时创建(When)等方面都为软件设计者提供了尽可能大的灵活性。创建型模式隐藏了类的实例的创建细节，通过隐藏对象如何被创建和组合在一起达到使整个系统独立的目的。 常见创建型模式详解 单例模式： 深入理解单例模式——只有一个实例 工厂模式： 深入理解工厂模式——由对象工厂生成对象 建造者模式： 深入理解建造者模式 ——组装复杂的实例 原型模式： 深入理解原型模式 ——通过复制生成实例 结构型模式结构型模式概述 结构型模式(Structural Pattern)： 描述如何将类或者对象结合在一起形成更大的结构，就像搭积木，可以通过简单积木的组合形成复杂的、功能更为强大的结构 结构型模式可以分为类结构型模式和对象结构型模式： 类结构型模式关心类的组合，由多个类可以组合成一个更大的系统，在类结构型模式中一般只存在继承关系和实现关系。 对象结构型模式关心类与对象的组合，通过关联关系使得在一个类中定义另一个类的实例对象，然后通过该对象调用其方法。根据“合成复用原则”，在系统中尽量使用关联关系来替代继承关系，因此大部分结构型模式都是对象结构型模式。 常见结构型模式详解 适配器模式： 深入理解适配器模式——加个“适配器”以便于复用 适配器模式原理及实例介绍-IBM 桥接模式： 设计模式笔记16：桥接模式(Bridge Pattern) 组合模式： 大话设计模式—组合模式 装饰模式： java模式—装饰者模式、Java设计模式-装饰者模式 外观模式： java设计模式之外观模式（门面模式） 享元模式： 享元模式 代理模式： 代理模式原理及实例讲解 （IBM出品，很不错） 轻松学，Java 中的代理模式及动态代理 Java代理模式及其应用 行为型模式行为型模式概述 行为型模式(Behavioral Pattern)是对在不同的对象之间划分责任和算法的抽象化。 行为型模式不仅仅关注类和对象的结构，而且重点关注它们之间的相互作用。 通过行为型模式，可以更加清晰地划分类与对象的职责，并研究系统在运行时实例对象之间的交互。在系统运行时，对象并不是孤立的，它们可以通过相互通信与协作完成某些复杂功能，一个对象在运行时也将影响到其他对象的运行。 行为型模式分为类行为型模式和对象行为型模式两种： 类行为型模式： 类的行为型模式使用继承关系在几个类之间分配行为，类行为型模式主要通过多态等方式来分配父类与子类的职责。 对象行为型模式： 对象的行为型模式则使用对象的聚合关联关系来分配行为，对象行为型模式主要是通过对象关联等方式来分配两个或多个类的职责。根据“合成复用原则”，系统中要尽量使用关联关系来取代继承关系，因此大部分行为型设计模式都属于对象行为型设计模式。 职责链模式： Java设计模式之责任链模式、职责链模式 责任链模式实现的三种方式 命令模式： https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/command.html 在软件设计中，我们经常需要向某些对象发送请求，但是并不知道请求的接收者是谁，也不知道被请求的操作是哪个，我们只需在程序运行时指定具体的请求接收者即可，此时，可以使用命令模式来进行设计，使得请求发送者与请求接收者消除彼此之间的耦合，让对象之间的调用关系更加灵活。命令模式可以对发送者和接收者完全解耦，发送者与接收者之间没有直接引用关系，发送请求的对象只需要知道如何发送请求，而不必知道如何完成请求。这就是命令模式的模式动机。 解释器模式： https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/mediator.html 迭代器模式： 中介者模式： 备忘录模式： 观察者模式： https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/observer.html 状态模式：https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/state.html 策略模式：https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/strategy.html 策略模式作为设计原则中开闭原则最典型的体现，也是经常使用的。下面这篇博客介绍了策略模式一般的组成部分和概念，并用了一个小demo去说明了策略模式的应用。 java设计模式之策略模式 模板方法模式： 访问者模式：]]></content>
      <categories>
        <category>design</category>
      </categories>
      <tags>
        <tag>design-Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown]]></title>
    <url>%2F2016%2F11%2F26%2Futils-2016-11-26-markdown%2F</url>
    <content type="text"><![CDATA[标题-2个井标题-3个井标题-4个井 引用内容引用内容引用内容引用内容 双重引用双重引用双重引用双重引用 文字 三个相互空格的星 实现 换行线 minima 内容 单星倾斜单星倾斜单星倾斜单星倾斜 内容内容行内代码内容内容内容 双星加粗双星加粗双星加粗 内容内容内容 显示宽度测试字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字很多文字显示宽度测试显示宽度测试 两种highlight： 1234567def show @widget = Widget(params[:id]) respond_to do |format| format.html # show.html.erb format.json &#123; render json: @widget &#125; endend … 从下面的截图可以看到： （无）标题-1个井（不建议用）等于号等于号等于号（不建议用）减号减号减号]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文档管理]]></title>
    <url>%2F2016%2F11%2F25%2Fmgr-2016-11-25-%E6%96%87%E6%A1%A3%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[在线文档：腾讯文档、wps文档、石墨文档 网盘钉钉网盘、百度网盘、腾讯微云、和彩云]]></content>
      <categories>
        <category>mgr</category>
      </categories>
      <tags>
        <tag>规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你好，博客世界]]></title>
    <url>%2F2016%2F11%2F24%2Fdiary-2016-11-24-hello-world%2F</url>
    <content type="text"><![CDATA[博客建立 - 第一次采用Jekyll和github做了这个博客。 - 后来又采用hexo的框架进行了重构。]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>Hello</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8接口规范]]></title>
    <url>%2F2016%2F11%2F16%2Flang-java-What-s-New-in-JDK8-JDK8%E6%8E%A5%E5%8F%A3%E8%A7%84%E8%8C%83-%E9%9D%99%E6%80%81%E3%80%81%E9%BB%98%E8%AE%A4%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在JDK8中引入了lambda表达式，出现了函数式接口的概念，为了在扩展接口时保持向前兼容性(比如泛型也是为了保持兼容性而失去了在一些别的语言泛型拥有的功能)，Java接口规范发生了一些改变。。 1.JDK8以前的接口规范 JDK8以前接口可以定义的变量和方法 所有变量(Field)不论是否显式 的声明为static final```，它实际上都是```public static final```的。123456 - 所有方法(Method)不论是否&lt;i&gt;显示&lt;/i&gt; 的声明为```public abstract```，它实际上都是```public abstract```的。```javapublic interface AInterfaceBeforeJDK8 &#123; int FIELD = 0; void simpleMethod();&#125; 以上接口信息反编译以后可以看到字节码信息里Filed是public static final的，而方法是public abstract的，即是你没有显示的去声明它。 12345678910&#123; public static final int FIELD; descriptor: I flags: (0x0019) ACC_PUBLIC, ACC_STATIC, ACC_FINAL ConstantValue: int 0 public abstract void simpleMethod(); descriptor: ()V flags: (0x0401) ACC_PUBLIC, ACC_ABSTRACT&#125; 2.JDK8之后的接口规范 JDK8之后接口可以定义的变量和方法 变量(Field)仍然必须是 public static final```的1234567891011121314151617 - 方法(Method)除了可以是public abstract之外，还可以是public static或者是default(相当于仅public修饰的实例方法)的。从以上改变不难看出，修改接口的规范主要是为了能在扩展接口时保持向前兼容。&lt;br&gt;下面是一个JDK8之后的接口例子```javapublic interface AInterfaceInJDK8 &#123; int simpleFiled = 0; static int staticField = 1; public static void main(String[] args) &#123; &#125; static void staticMethod()&#123;&#125; default void defaultMethod()&#123;&#125; void simpleMethod() throws IOException;&#125; 进行反编译(去除了一些没用信息) 123456789101112131415161718192021&#123; public static final int simpleFiled; flags: (0x0019) ACC_PUBLIC, ACC_STATIC, ACC_FINAL public static final int staticField; flags: (0x0019) ACC_PUBLIC, ACC_STATIC, ACC_FINAL public static void main(java.lang.String[]); flags: (0x0009) ACC_PUBLIC, ACC_STATIC public static void staticMethod(); flags: (0x0009) ACC_PUBLIC, ACC_STATIC public void defaultMethod(); flags: (0x0001) ACC_PUBLIC public abstract void simpleMethod() throws java.io.IOException; flags: (0x0401) ACC_PUBLIC, ACC_ABSTRACT Exceptions: throws java.io.IOException&#125; 可以看到 default关键字修饰的方法是像实例方法一样定义的，所以我们来定义一个只有default的方法并且实现一下试一试。 1234567891011121314interface Default &#123; default int defaultMethod() &#123; return 4396; &#125;&#125;public class DefaultMethod implements Default &#123; public static void main(String[] args) &#123; DefaultMethod defaultMethod = new DefaultMethod(); System.out.println(defaultMethod.defaultMethod()); //compile error : Non-static method 'defaultMethod()' cannot be referenced from a static context //! DefaultMethod.defaultMethod(); &#125;&#125; 可以看到default方法确实像实例方法一样，必须有实例对象才能调用，并且子类在实现接口时，可以不用实现default方法，也可以覆盖该方法。这有点像子类继承父类实例方法。接口静态方法就像是类静态方法，唯一的区别是接口静态方法只能通过接口名调用，而类静态方法既可以通过类名调用也可以通过实例调用 123456789interface Static &#123; static int staticMethod() &#123; return 4396; &#125;&#125; ... main(String...args) //!compile error: Static method may be invoked on containing interface class only //!aInstanceOfStatic.staticMethod(); ... 另一个问题是多继承问题，大家知道Java中类是不支持多继承的，但是接口是多继承和多实现(implements后跟多个接口)的，那么如果一个接口继承另一个接口，两个接口都有同名的default方法会怎么样呢？答案是会像类继承一样覆写(@Override)，以下代码在IDE中可以顺利编译 12345678910111213141516171819interface Default &#123; default int defaultMethod() &#123; return 4396; &#125;&#125;interface Default2 extends Default &#123; @Override default int defaultMethod() &#123; return 9527; &#125;&#125;public class DefaultMethod implements Default,Default2 &#123; public static void main(String[] args) &#123; DefaultMethod defaultMethod = new DefaultMethod(); System.out.println(defaultMethod.defaultMethod()); &#125;&#125;输出 : 9527 出现上面的情况时，会优先找继承树上近的方法，类似于“短路优先”。那么如果一个类实现了两个没有继承关系的接口，且这两个接口有同名方法的话会怎么样呢？IDE会要求你重写这个冲突的方法，让你自己选择去执行哪个方法，因为IDE它还没智能到你不告诉它，它就知道你想执行哪个方法。可以通过接口名.super```指针来访问接口中定义的实例(default)方法。12345678910111213141516171819202122232425262728293031```javainterface Default &#123; default int defaultMethod() &#123; return 4396; &#125;&#125;interface Default2 &#123; default int defaultMethod() &#123; return 9527; &#125;&#125;//如果不重写//compile error : defaults.DefaultMethod inherits unrelated defaults for defaultMethod() from types defaults.Default and defaults.Default2public class DefaultMethod implements Default,Default2 &#123;@Override public int defaultMethod() &#123; System.out.println(Default.super.defaultMethod()); System.out.println(Default2.super.defaultMethod()); return 996; &#125; public static void main(String[] args) &#123; DefaultMethod defaultMethod = new DefaultMethod(); System.out.println(defaultMethod.defaultMethod()); &#125;&#125;运行输出 : 43969527996]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8--Lambda表达式]]></title>
    <url>%2F2016%2F11%2F15%2Flang-java-What-s-New-in-JDK8-Lambda%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1.什么是Lambda表达式Lambda表达式实质上是一个可传递的代码块，Lambda又称为闭包或者匿名函数，是函数式编程语法，让方法可以像普通参数一样传递 2.Lambda表达式语法-> &#123;执行代码块&#125;```1&lt;br&gt;参数列表可以为空```()-&gt;&#123;&#125; 可以加类型声明比如para1, int para2) -> &#123;return para1 + para2;&#125;```我们可以看到，lambda同样可以有返回值.1&lt;br&gt;在编译器可以推断出类型的时候，可以将类型声明省略，比如```(para1, para2) -&gt; &#123;return para1 + para2;&#125; (lambda有点像动态类型语言语法。lambda在字节码层面是用invokedynamic实现的，而这条指令就是为了让JVM更好的支持运行在其上的动态类型语言) 3.函数式接口在了解Lambda表达式之前，有必要先了解什么是函数式接口1234567**函数式接口指的是有且只有一个抽象(abstract)方法的接口**&lt;br&gt;当需要一个函数式接口的对象时，就可以用Lambda表达式来实现，举个常用的例子:&lt;br&gt;```java Thread thread = new Thread(() -&gt; &#123; System.out.println(&quot;This is JDK8&apos;s Lambda!&quot;); &#125;); 这段代码和函数式接口有啥关系？我们回忆一下，Thread类的构造函数里是不是有一个以Runnable接口为参数的？ 123456789public Thread(Runnable target) &#123;...&#125;/** * Runnable Interface */@FunctionalInterfacepublic interface Runnable &#123; public abstract void run();&#125; 到这里大家可能已经明白了，Lambda表达式相当于一个匿名类或者说是一个匿名方法。上面Thread的例子相当于 123456Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("Anonymous class"); &#125;&#125;); 也就是说，上面的lambda表达式相当于实现了这个run()方法，然后当做参数传入(个人感觉可以这么理解,lambda表达式就是一个函数，只不过它的返回值、参数列表都由编译器帮我们推断，因此可以减少很多代码量)。Lambda也可以这样用 : 1Runnable runnable = () -&gt; &#123;...&#125;; 其实这和上面的用法没有什么本质上的区别。至此大家应该明白什么是函数式接口以及函数式接口和lambda表达式之间的关系了。在JDK8中修改了接口的规范，目的是为了在给接口添加新的功能时保持向前兼容(个人理解)，比如一个已经定义了的函数式接口，某天我们想给它添加新功能，那么就不能保持向前兼容了，因为在旧的接口规范下，添加新功能必定会破坏这个函数式接口(JDK8中接口规范)除了上面说的Runnable接口之外，JDK中已经存在了很多函数式接口比如(当然不止这些): 1- ```java.util.Comparator 1234567891011121314151617181920212223242526272829303132333435363738&lt;br&gt;**关于JDK中的预定义的函数式接口**- JDK在```java.util.function```下预定义了很多函数式接口 - ```Function&lt;T, R&gt; &#123;R apply(T t);&#125;``` 接受一个T对象，然后返回一个R对象，就像普通的函数。 - ```Consumer&lt;T&gt; &#123;void accept(T t);&#125;``` 消费者 接受一个T对象，没有返回值。 - ```Predicate&lt;T&gt; &#123;boolean test(T t);&#125;``` 判断，接受一个T对象，返回一个布尔值。 - ```Supplier&lt;T&gt; &#123;T get();&#125; 提供者(工厂)``` 返回一个T对象。 - 其他的跟上面的相似，大家可以看一下function包下的具体接口。## 4.变量作用域```javapublic class VaraibleHide &#123; @FunctionalInterface interface IInner &#123; void printInt(int x); &#125; public static void main(String[] args) &#123; int x = 20; IInner inner = new IInner() &#123; int x = 10; @Override public void printInt(int x) &#123; System.out.println(x); &#125; &#125;; inner.printInt(30); inner = (s) -&gt; &#123; //Variable used in lambda expression should be final or effectively final //!int x = 10; //!x= 50; error System.out.print(x); &#125;; inner.printInt(30); &#125;&#125;输出 :3020 对于lambda表达式inner 123456789101112&lt;br&gt;lambda表达式和内部类一样，对外部自由变量捕获时，外部自由变量必须为final或者是最终变量(effectively final)的，也就是说这个变量初始化后就不能为它赋新值，同时lambda不像内部类/匿名类，lambda表达式与外围嵌套块有着相同的作用域，因此对变量命名的有关规则对lambda同样适用。大家阅读上面的代码对这些概念应该不难理解。## 5.方法引用**只需要提供方法的名字，具体的调用过程由Lambda和函数式接口来确定，这样的方法调用成为方法引用。**&lt;br&gt;下面的例子会打印list中的每个元素:```javaList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; ++i) &#123; list.add(i); &#125; list.forEach(System.out::println); 其中```(para)->&#123;System.out.println(para);&#125;```12345&lt;br&gt;我们看一下List#forEach方法 ```default void forEach(Consumer&lt;? super T&gt; action)```可以看到它的参数是一个Consumer接口，该接口是一个函数式接口```java@FunctionalInterfacepublic interface Consumer&lt;T&gt; &#123; void accept(T t); 大家能发现这个函数接口的方法和1234567891011121314151617&lt;br&gt;我们自己定义一个方法，看看能不能像标准输出的打印函数一样被调用```javapublic class MethodReference &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; ++i) &#123; list.add(i); &#125; list.forEach(MethodReference::myPrint); &#125; static void myPrint(int i) &#123; System.out.print(i + &quot;, &quot;); &#125;&#125;输出: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 可以看到，我们自己定义的方法也可以当做方法引用。到这里大家多少对方法引用有了一定的了解，我们再来说一下方法引用的形式。 方法引用 类名::静态方法名 类名::实例方法名 类名::new (构造方法引用) 实例名::实例方法名可以看出，方法引用是通过(方法归属名)::(方法名)来调用的。通过上面的例子已经讲解了一个类名::静态方法名的使用方法了，下面再依次介绍其余的几种方法引用的使用方法。 类名::实例方法名先来看一段代码 12String[] strings = new String[10];Arrays.sort(strings, String::compareToIgnoreCase); 上面的String::compareToIgnoreCase等价于(x, y) -&gt; {return x.compareToIgnoreCase(y);}我们看一下Arrays#sort方法public static &lt;T&gt; void sort(T[] a, Comparator&lt;? super T&gt; c),可以看到第二个参数是一个Comparator接口，该接口也是一个函数式接口，其中的抽象方法是int compare(T o1, T o2);，再看一下String#compareToIgnoreCase方法,public int compareToIgnoreCase(String str)，这个方法好像和上面讲方法引用中类名::静态方法名不大一样啊，它的参数列表和函数式接口的参数列表不一样啊，虽然它的返回值一样？是的，确实不一样但是别忘了，String类的这个方法是个实例方法，而不是静态方法，也就是说，这个方法是需要有一个接收者的。所谓接收者就是instance.method(x)中的instance，它是某个类的实例，有的朋友可能已经明白了。上面函数式接口的compare(T o1, T o2)中的第一个参数作为了实例方法的接收者，而第二个参数作为了实例方法的参数。我们再举一个自己实现的例子: 12345678910public class MethodReference &#123; static Random random = new Random(47); public static void main(String[] args) &#123; MethodReference[] methodReferences = new MethodReference[10]; Arrays.sort(methodReferences, MethodReference::myCompare); &#125; int myCompare(MethodReference o) &#123; return random.nextInt(2) - 1; &#125;&#125; 上面的例子可以在IDE里通过编译，大家有兴趣的可以模仿上面的例子自己写一个程序，打印出排序后的结果。构造器引用构造器引用仍然需要与特定的函数式接口配合使用，并不能像下面这样直接使用。IDE会提示String不是一个函数式接口 12//compile error : String is not a functional interfaceString str = String::new; 下面是一个使用构造器引用的例子，可以看出构造器引用可以和这种工厂型的函数式接口一起使用的。 12345678910111213141516 interface IFunctional&lt;T&gt; &#123; T func();&#125;public class ConstructorReference &#123; public ConstructorReference() &#123; &#125; public static void main(String[] args) &#123; Supplier&lt;ConstructorReference&gt; supplier0 = () -&gt; new ConstructorReference(); Supplier&lt;ConstructorReference&gt; supplier1 = ConstructorReference::new; IFunctional&lt;ConstructorReference&gt; functional = () -&gt; new ConstructorReference(); IFunctional&lt;ConstructorReference&gt; functional1 = ConstructorReference::new; &#125;&#125; 下面是一个JDK官方的例子 12345678910111213141516public static &lt;T, SOURCE extends Collection&lt;T&gt;, DEST extends Collection&lt;T&gt;&gt; DEST transferElements( SOURCE sourceCollection, Supplier&lt;DEST&gt; collectionFactory) &#123; DEST result = collectionFactory.get(); for (T t : sourceCollection) &#123; result.add(t); &#125; return result; &#125; ... Set&lt;Person&gt; rosterSet = transferElements( roster, HashSet::new); 实例::实例方法其实开始那个例子就是一个实例::实例方法的引用 12345List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; ++i) &#123; list.add(i); &#125; list.forEach(System.out::println); 其中System.out就是一个实例，println是一个实例方法。相信不用再给大家做解释了。 总结Lambda表达式是JDK8引入Java的函数式编程语法，使用Lambda需要直接或者间接的与函数式接口配合，在开发中使用Lambda可以减少代码量，但是并不是说必须要使用Lambda(虽然它是一个很酷的东西)。有些情况下使用Lambda会使代码的可读性急剧下降，并且也节省不了多少代码，所以在实际开发中还是需要仔细斟酌是否要使用Lambda。和Lambda相似的还有JDK10中加入的var类型推断，同样对于这个特性需要斟酌使用。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改进的类型推断]]></title>
    <url>%2F2016%2F11%2F15%2Flang-java-What-s-New-in-JDK8-%E6%94%B9%E8%BF%9B%E7%9A%84%E7%B1%BB%E5%9E%8B%E6%8E%A8%E6%96%AD%2F</url>
    <content type="text"><![CDATA[1.什么是类型推断类型推断就像它的字面意思一样，编译器根据你显示声明的已知的信息 推断出你没有显示声明的类型，这就是类型推断。看过《Java编程思想 第四版》的朋友可能还记得里面讲解泛型一章的时候，里面很多例子是下面这样的: 1Map&lt;String, Object&gt; map = new Map&lt;String, Object&gt;(); 而我们平常写的都是这样的: 1Map&lt;String, Object&gt; map = new Map&lt;&gt;(); 这就是类型推断，《Java编程思想 第四版》这本书出书的时候最新的JDK只有1.6(JDK7推出的类型推断)，在Java编程思想里Bruce Eckel大叔还提到过这个问题(可能JDK的官方人员看了Bruce Eckel大叔的Thinking in Java才加的类型推断，☺)，在JDK7中推出了上面这样的类型推断，可以减少一些无用的代码。(Java编程思想到现在还只有第四版，是不是因为Bruce Eckel大叔觉得Java新推出的语言特性“然并卵”呢？/滑稽)在JDK7中，类型推断只有上面例子的那样的能力，即只有在使用赋值语句时才能自动推断出泛型参数信息(即&lt;&gt;里的信息)，下面的官方文档里的例子在JDK7里会编译错误 1234List&lt;String&gt; stringList = new ArrayList&lt;&gt;();stringList.add("A");//error : addAll(java.util.Collection&lt;? extends java.lang.String&gt;)in List cannot be applied to (java.util.List&lt;java.lang.Object&gt;)stringList.addAll(Arrays.asList()); 但是上面的代码在JDK8里可以通过，也就说，JDK8里，类型推断不仅可以用于赋值语句，而且可以根据代码中上下文里的信息推断出更多的信息，因此我们需要些的代码会更少。加强的类型推断还有一个就是用于Lambda表达式了。大家其实不必细究类型推断，在日常使用中IDE会自动判断，当IDE自己无法推断出足够的信息时，就需要我们额外做一下工作，比如在&lt;&gt;里添加更多的类型信息，相信随着Java的进化，这些便利的功能会越来越强大。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stream API]]></title>
    <url>%2F2016%2F11%2F15%2Flang-java-What-s-New-in-JDK8-Stream%2F</url>
    <content type="text"><![CDATA[Stream API 旨在让编码更高效率、干净、简洁。 从迭代器到Stream操作当使用 Stream 时，我们一般会通过三个阶段建立一个流水线： 创建一个 Stream； 进行一个或多个中间操作; 使用终止操作产生一个结果,Stream 就不会再被使用了。 案例1：统计 List 中的单词长度大于6的个数 12345678910111213/*** 案例1：统计 List 中的单词长度大于6的个数*/ArrayList&lt;String&gt; wordsList = new ArrayList&lt;String&gt;();wordsList.add("Charles");wordsList.add("Vincent");wordsList.add("William");wordsList.add("Joseph");wordsList.add("Henry");wordsList.add("Bill");wordsList.add("Joan");wordsList.add("Linda");int count = 0; Java8之前我们通常用迭代方法来完成上面的需求： 12345678//迭代（Java8之前的常用方法）//迭代不好的地方：1. 代码多；2 很难被并行运算。for (String word : wordsList) &#123; if (word.length() &gt; 6) &#123; count++; &#125;&#125;System.out.println(count);//3 Java8之前我们使用 Stream 一行代码就能解决了，而且可以瞬间转换为并行执行的效果： 12345678910//Stream//将stream()改为parallelStream()就可以瞬间将代码编程并行执行的效果long count2=wordsList.stream() .filter(w-&gt;w.length()&gt;6) .count();long count3=wordsList.parallelStream() .filter(w-&gt;w.length()&gt;6) .count();System.out.println(count2);System.out.println(count3); distinct()去除 List 中重复的 String 123List&lt;String&gt; list = list.stream() .distinct() .collect(Collectors.toList()); mapmap 方法用于映射每个元素到对应的结果，以下代码片段使用 map 输出了元素对应的平方数： 1234List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);// 获取 List 中每个元素对应的平方数并去重List&lt;Integer&gt; squaresList = numbers.stream().map( i -&gt; i*i).distinct().collect(Collectors.toList());System.out.println(squaresList.toString());//[9, 4, 49, 25]]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过反射获得方法的参数信息]]></title>
    <url>%2F2016%2F11%2F15%2Flang-java-What-s-New-in-JDK8-%E9%80%9A%E8%BF%87%E5%8F%8D%E5%B0%84%E8%8E%B7%E5%BE%97%E6%96%B9%E6%B3%95%E7%9A%84%E5%8F%82%E6%95%B0%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[JDK8之前 .class文件是不会存储方法参数信息的，因此也就无法通过反射获取该信息(想想反射获取类信息的入口是什么？当然就是Class类了)。即是是在JDK11里也不会默认生成这些信息，可以通过在javac加上-parameters参数来让javac生成这些信息(javac就是java编译器，可以把java文件编译成.class文件)。生成额外的信息(运行时非必须信息)会消耗内存并且有可能公布敏感信息(某些方法参数比如password，JDK文档里这么说的)，并且确实很多信息javac并不会为我们生成，比如LocalVariableTable，javac就不会默认生成，需要你加上 -g:vars来强制让编译器生成，同样的，方法参数信息也需要加上-parameters来让javac为你在.class文件中生成这些信息，否则运行时反射是无法获取到这些信息的。在讲解Java语言层面的方法之前，先看一下javac加上该参数和不加生成的信息有什么区别(不感兴趣想直接看运行代码的可以跳过这段)。下面是随便写的一个类。 12345public class ByteCodeParameters &#123; public String simpleMethod(String canUGetMyName, Object yesICan) &#123; return "9527"; &#125;&#125; 先来不加参数编译和反编译一下这个类javac ByteCodeParameters.java , javap -v ByteCodeParameters: 1234567891011//只截取了部分信息public java.lang.String simpleMethod(java.lang.String, java.lang.Object); descriptor: (Ljava/lang/String;Ljava/lang/Object;)Ljava/lang/String; flags: (0x0001) ACC_PUBLIC Code: stack=1, locals=3, args_size=3 0: ldc #2 // String 9527 2: areturn LineNumberTable: line 5: 0//这个方法的描述到这里就结束了 接下来我们加上参数javac -parameters ByteCodeParameters.java 再来看反编译的信息: 12345678910111213public java.lang.String simpleMethod(java.lang.String, java.lang.Object); descriptor: (Ljava/lang/String;Ljava/lang/Object;)Ljava/lang/String; flags: (0x0001) ACC_PUBLIC Code: stack=1, locals=3, args_size=3 0: ldc #2 // String 9527 2: areturn LineNumberTable: line 8: 0 MethodParameters: Name Flags canUGetMyName yesICan 可以看到.class文件里多了一个MethodParameters信息，这就是参数的名字，可以看到默认是不保存的。下面看一下在Intelj Idea里运行的这个例子，我们试一下通过反射获取方法名 : 1234567891011121314151617public class ByteCodeParameters &#123; public String simpleMethod(String canUGetMyName, Object yesICan) &#123; return "9527"; &#125; public static void main(String[] args) throws NoSuchMethodException &#123; Class&lt;?&gt; clazz = ByteCodeParameters.class; Method simple = clazz.getDeclaredMethod("simpleMethod", String.class, Object.class); Parameter[] parameters = simple.getParameters(); for (Parameter p : parameters) &#123; System.out.println(p.getName()); &#125; &#125;&#125;输出 :arg0arg1 ？？？说好的方法名呢？？？？别急，哈哈。前面说了，默认是不生成参数名信息的，因此我们需要做一些配置，我们找到IDEA的settings里的Java Compiler选项，在Additional command line parameters:一行加上-parameters(Eclipse 也是找到Java Compiler选中Stoer information about method parameters)，或者自己编译一个.class文件放在IDEA的out下，然后再来运行 : 123输出 :canUGetMyNameyesICan 这样我们就通过反射获取到参数信息了。想要了解更多的同学可以自己研究一下 [官方文档](https://docs.oracle.com/javase/tutorial/reflect/member/methodparameterreflection.html) 总结与补充在JDK8之后，可以通过-parameters参数来让编译器生成参数信息然后在运行时通过反射获取方法参数信息，其实在SpringFramework里面也有一个LocalVariableTableParameterNameDiscoverer对象可以获取方法参数名信息，有兴趣的同学可以自行百度(这个类在打印日志时可能会比较有用吧，个人感觉)。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 8 Tutoria]]></title>
    <url>%2F2016%2F10%2F12%2Flang-java-What-s-New-in-JDK8-Java8Tutorial%2F</url>
    <content type="text"><![CDATA[随着 Java 8 的普及度越来越高，很多人都提到面试中关于Java 8 也是非常常问的知识点。应各位要求和需要，我打算对这部分知识做一个总结。本来准备自己总结的，后面看到Github 上有一个相关的仓库，地址：https://github.com/winterbe/java8-tutorial。这个仓库是英文的，我对其进行了翻译并添加和修改了部分内容，下面是正文了。 Java 8 Tutorial 接口的默认方法(Default Methods for Interfaces) Lambda表达式(Lambda expressions) 函数式接口(Functional Interfaces) 方法和构造函数引用(Method and Constructor References) Lamda 表达式作用域(Lambda Scopes) 访问局部变量 访问字段和静态变量 访问默认接口方法 内置函数式接口(Built-in Functional Interfaces) Predicates Functions Suppliers Consumers Comparators Optionals Streams(流) Filter(过滤) Sorted(排序) Map(映射) Match(匹配) Count(计数) Reduce(规约) Parallel Streams(并行流) Sequential Sort(串行排序) Parallel Sort(并行排序) Maps Date API(日期相关API) Clock Timezones(时区) LocalTime(本地时间) LocalDate(本地日期) LocalDateTime(本地日期时间) Annotations(注解) Whete to go from here? Java 8 Tutorial欢迎阅读我对Java 8的介绍。本教程将逐步指导您完成所有新语言功能。 在简短的代码示例的基础上，您将学习如何使用默认接口方法，lambda表达式，方法引用和可重复注释。 在本文的最后，您将熟悉最新的 API 更改，如流，函数式接口(Functional Interfaces)，Map 类的扩展和新的 Date API。 没有大段枯燥的文字，只有一堆注释的代码片段。 接口的默认方法(Default Methods for Interfaces)Java 8使我们能够通过使用 default 关键字向接口添加非抽象方法实现。 此功能也称为虚拟扩展方法。 第一个例子： 123456789interface Formula&#123; double calculate(int a); default double sqrt(int a) &#123; return Math.sqrt(a); &#125;&#125; Formula 接口中除了抽象方法计算接口公式还定义了默认方法 sqrt。 实现该接口的类只需要实现抽象方法 calculate。 默认方法sqrt 可以直接使用。当然你也可以直接通过接口创建对象，然后实现接口中的默认方法就可以了，我们通过代码演示一下这种方式。 1234567891011121314151617public class Main &#123; public static void main(String[] args) &#123; // TODO 通过匿名内部类方式访问接口 Formula formula = new Formula() &#123; @Override public double calculate(int a) &#123; return sqrt(a * 100); &#125; &#125;; System.out.println(formula.calculate(100)); // 100.0 System.out.println(formula.sqrt(16)); // 4.0 &#125;&#125; formula 是作为匿名对象实现的。该代码非常容易理解，6行代码实现了计算 sqrt(a * 100)。在下一节中，我们将会看到在 Java 8 中实现单个方法对象有一种更好更方便的方法。 译者注： 不管是抽象类还是接口，都可以通过匿名内部类的方式访问。不能通过抽象类或者接口直接创建对象。对于上面通过匿名内部类方式访问接口，我们可以这样理解：一个内部类实现了接口里的抽象方法并且返回一个内部类对象，之后我们让接口的引用来指向这个对象。 Lambda表达式(Lambda expressions)首先看看在老版本的Java中是如何排列字符串的： 12345678List&lt;String&gt; names = Arrays.asList("peter", "anna", "mike", "xenia");Collections.sort(names, new Comparator&lt;String&gt;() &#123; @Override public int compare(String a, String b) &#123; return b.compareTo(a); &#125;&#125;); 只需要给静态方法Collections.sort 传入一个 List 对象以及一个比较器来按指定顺序排列。通常做法都是创建一个匿名的比较器对象然后将其传递给 sort 方法。 在Java 8 中你就没必要使用这种传统的匿名对象的方式了，Java 8提供了更简洁的语法，lambda表达式： 123Collections.sort(names, (String a, String b) -&gt; &#123; return b.compareTo(a);&#125;); 可以看出，代码变得更段且更具有可读性，但是实际上还可以写得更短： 1Collections.sort(names, (String a, String b) -&gt; b.compareTo(a)); 对于函数体只有一行代码的，你可以去掉大括号{}以及return关键字，但是你还可以写得更短点： 1names.sort((a, b) -&gt; b.compareTo(a)); List 类本身就有一个 sort 方法。并且Java编译器可以自动推导出参数类型，所以你可以不用再写一次类型。接下来我们看看lambda表达式还有什么其他用法。 函数式接口(Functional Interfaces)译者注： 原文对这部分解释不太清楚，故做了修改！ Java 语言设计者们投入了大量精力来思考如何使现有的函数友好地支持Lambda。最终采取的方法是：增加函数式接口的概念。“函数式接口”是指仅仅只包含一个抽象方法,但是可以有多个非抽象方法(也就是上面提到的默认方法)的接口。 像这样的接口，可以被隐式转换为lambda表达式。java.lang.Runnable 与 java.util.concurrent.Callable 是函数式接口最典型的两个例子。Java 8增加了一种特殊的注解@FunctionalInterface,但是这个注解通常不是必须的(某些情况建议使用)，只要接口只包含一个抽象方法，虚拟机会自动判断该接口为函数式接口。一般建议在接口上使用@FunctionalInterface 注解进行声明，这样的话，编译器如果发现你标注了这个注解的接口有多于一个抽象方法的时候会报错的，如下图所示 示例： 1234@FunctionalInterfacepublic interface Converter&lt;F, T&gt; &#123; T convert(F from);&#125; 1234// TODO 将数字字符串转换为整数类型Converter&lt;String, Integer&gt; converter = (from) -&gt; Integer.valueOf(from);Integer converted = converter.convert("123");System.out.println(converted.getClass()); //class java.lang.Integer 译者注： 大部分函数式接口都不用我们自己写，Java8都给我们实现好了，这些接口都在java.util.function包里。 方法和构造函数引用(Method and Constructor References)前一节中的代码还可以通过静态方法引用来表示： 123Converter&lt;String, Integer&gt; converter = Integer::valueOf;Integer converted = converter.convert("123");System.out.println(converted.getClass()); //class java.lang.Integer Java 8允许您通过::关键字传递方法或构造函数的引用。 上面的示例显示了如何引用静态方法。 但我们也可以引用对象方法： 12345class Something &#123; String startsWith(String s) &#123; return String.valueOf(s.charAt(0)); &#125;&#125; 1234Something something = new Something();Converter&lt;String, String&gt; converter = something::startsWith;String converted = converter.convert("Java");System.out.println(converted); // "J" 接下来看看构造函数是如何使用::关键字来引用的，首先我们定义一个包含多个构造函数的简单类： 1234567891011class Person &#123; String firstName; String lastName; Person() &#123;&#125; Person(String firstName, String lastName) &#123; this.firstName = firstName; this.lastName = lastName; &#125;&#125; 接下来我们指定一个用来创建Person对象的对象工厂接口： 123interface PersonFactory&lt;P extends Person&gt; &#123; P create(String firstName, String lastName);&#125; 这里我们使用构造函数引用来将他们关联起来，而不是手动实现一个完整的工厂： 12PersonFactory&lt;Person&gt; personFactory = Person::new;Person person = personFactory.create("Peter", "Parker"); 我们只需要使用 Person::new 来获取Person类构造函数的引用，Java编译器会自动根据PersonFactory.create方法的参数类型来选择合适的构造函数。 Lamda 表达式作用域(Lambda Scopes)访问局部变量我们可以直接在 lambda 表达式中访问外部的局部变量： 12345final int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);stringConverter.convert(2); // 3 但是和匿名对象不同的是，这里的变量num可以不用声明为final，该代码同样正确： 12345int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);stringConverter.convert(2); // 3 不过这里的 num 必须不可被后面的代码修改（即隐性的具有final的语义），例如下面的就无法编译： 1234int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);num = 3;//在lambda表达式中试图修改num同样是不允许的。 访问字段和静态变量与局部变量相比，我们对lambda表达式中的实例字段和静态变量都有读写访问权限。 该行为和匿名对象是一致的。 12345678910111213141516class Lambda4 &#123; static int outerStaticNum; int outerNum; void testScopes() &#123; Converter&lt;Integer, String&gt; stringConverter1 = (from) -&gt; &#123; outerNum = 23; return String.valueOf(from); &#125;; Converter&lt;Integer, String&gt; stringConverter2 = (from) -&gt; &#123; outerStaticNum = 72; return String.valueOf(from); &#125;; &#125;&#125; 访问默认接口方法还记得第一节中的 formula 示例吗？ Formula 接口定义了一个默认方法sqrt，可以从包含匿名对象的每个 formula 实例访问该方法。 这不适用于lambda表达式。 无法从 lambda 表达式中访问默认方法,故以下代码无法编译： 1Formula formula = (a) -&gt; sqrt(a * 100); 内置函数式接口(Built-in Functional Interfaces)JDK 1.8 API包含许多内置函数式接口。 其中一些借口在老版本的 Java 中是比较常见的比如： Comparator 或Runnable，这些接口都增加了@FunctionalInterface注解以便能用在 lambda 表达式上。 但是 Java 8 API 同样还提供了很多全新的函数式接口来让你的编程工作更加方便，有一些接口是来自 Google Guava 库里的，即便你对这些很熟悉了，还是有必要看看这些是如何扩展到lambda上使用的。 PredicatesPredicate 接口是只有一个参数的返回布尔类型值的 断言型 接口。该接口包含多种默认方法来将 Predicate 组合成其他复杂的逻辑（比如：与，或，非）： 译者注： Predicate 接口源码如下 1234567891011121314151617181920212223242526272829package java.util.function;import java.util.Objects;@FunctionalInterfacepublic interface Predicate&lt;T&gt; &#123; // 该方法是接受一个传入类型,返回一个布尔值.此方法应用于判断. boolean test(T t); //and方法与关系型运算符"&amp;&amp;"相似，两边都成立才返回true default Predicate&lt;T&gt; and(Predicate&lt;? super T&gt; other) &#123; Objects.requireNonNull(other); return (t) -&gt; test(t) &amp;&amp; other.test(t); &#125; // 与关系运算符"!"相似，对判断进行取反 default Predicate&lt;T&gt; negate() &#123; return (t) -&gt; !test(t); &#125; //or方法与关系型运算符"||"相似，两边只要有一个成立就返回true default Predicate&lt;T&gt; or(Predicate&lt;? super T&gt; other) &#123; Objects.requireNonNull(other); return (t) -&gt; test(t) || other.test(t); &#125; // 该方法接收一个Object对象,返回一个Predicate类型.此方法用于判断第一个test的方法与第二个test方法相同(equal). static &lt;T&gt; Predicate&lt;T&gt; isEqual(Object targetRef) &#123; return (null == targetRef) ? Objects::isNull : object -&gt; targetRef.equals(object); &#125; 示例： 12345678910Predicate&lt;String&gt; predicate = (s) -&gt; s.length() &gt; 0;predicate.test("foo"); // truepredicate.negate().test("foo"); // falsePredicate&lt;Boolean&gt; nonNull = Objects::nonNull;Predicate&lt;Boolean&gt; isNull = Objects::isNull;Predicate&lt;String&gt; isEmpty = String::isEmpty;Predicate&lt;String&gt; isNotEmpty = isEmpty.negate(); FunctionsFunction 接口接受一个参数并生成结果。默认方法可用于将多个函数链接在一起（compose, andThen）： 译者注： Function 接口源码如下 12345678910111213141516171819202122232425package java.util.function; import java.util.Objects; @FunctionalInterfacepublic interface Function&lt;T, R&gt; &#123; //将Function对象应用到输入的参数上，然后返回计算结果。 R apply(T t); //将两个Function整合，并返回一个能够执行两个Function对象功能的Function对象。 default &lt;V&gt; Function&lt;V, R&gt; compose(Function&lt;? super V, ? extends T&gt; before) &#123; Objects.requireNonNull(before); return (V v) -&gt; apply(before.apply(v)); &#125; // default &lt;V&gt; Function&lt;T, V&gt; andThen(Function&lt;? super R, ? extends V&gt; after) &#123; Objects.requireNonNull(after); return (T t) -&gt; after.apply(apply(t)); &#125; static &lt;T&gt; Function&lt;T, T&gt; identity() &#123; return t -&gt; t; &#125;&#125; 123Function&lt;String, Integer&gt; toInteger = Integer::valueOf;Function&lt;String, String&gt; backToString = toInteger.andThen(String::valueOf);backToString.apply("123"); // "123" SuppliersSupplier 接口产生给定泛型类型的结果。 与 Function 接口不同，Supplier 接口不接受参数。 12Supplier&lt;Person&gt; personSupplier = Person::new;personSupplier.get(); // new Person ConsumersConsumer 接口表示要对单个输入参数执行的操作。 12Consumer&lt;Person&gt; greeter = (p) -&gt; System.out.println("Hello, " + p.firstName);greeter.accept(new Person("Luke", "Skywalker")); ComparatorsComparator 是老Java中的经典接口， Java 8在此之上添加了多种默认方法： 1234567Comparator&lt;Person&gt; comparator = (p1, p2) -&gt; p1.firstName.compareTo(p2.firstName);Person p1 = new Person("John", "Doe");Person p2 = new Person("Alice", "Wonderland");comparator.compare(p1, p2); // &gt; 0comparator.reversed().compare(p1, p2); // &lt; 0 OptionalsOptionals不是函数式接口，而是用于防止 NullPointerException 的漂亮工具。这是下一节的一个重要概念，让我们快速了解一下Optionals的工作原理。 Optional 是一个简单的容器，其值可能是null或者不是null。在Java 8之前一般某个函数应该返回非空对象但是有时却什么也没有返回，而在Java 8中，你应该返回 Optional 而不是 null。 译者注：示例中每个方法的作用已经添加。 12345678910//of（）：为非null的值创建一个OptionalOptional&lt;String&gt; optional = Optional.of("bam");// isPresent（）： 如果值存在返回true，否则返回falseoptional.isPresent(); // true//get()：如果Optional有值则将其返回，否则抛出NoSuchElementExceptionoptional.get(); // "bam"//orElse（）：如果有值则将其返回，否则返回指定的其它值optional.orElse("fallback"); // "bam"//ifPresent（）：如果Optional实例有值则为其调用consumer，否则不做处理optional.ifPresent((s) -&gt; System.out.println(s.charAt(0))); // "b" 推荐阅读：[Java8]如何正确使用Optional Streams(流)java.util.Stream 表示能应用在一组元素上一次执行的操作序列。Stream 操作分为中间操作或者最终操作两种，最终操作返回一特定类型的计算结果，而中间操作返回Stream本身，这样你就可以将多个操作依次串起来。Stream 的创建需要指定一个数据源，比如java.util.Collection 的子类，List 或者 Set， Map 不支持。Stream 的操作可以串行执行或者并行执行。 首先看看Stream是怎么用，首先创建实例代码的用到的数据List： 123456789List&lt;String&gt; stringCollection = new ArrayList&lt;&gt;();stringCollection.add("ddd2");stringCollection.add("aaa2");stringCollection.add("bbb1");stringCollection.add("aaa1");stringCollection.add("bbb3");stringCollection.add("ccc");stringCollection.add("bbb2");stringCollection.add("ddd1"); Java 8扩展了集合类，可以通过 Collection.stream() 或者 Collection.parallelStream() 来创建一个Stream。下面几节将详细解释常用的Stream操作： Filter(过滤)过滤通过一个predicate接口来过滤并只保留符合条件的元素，该操作属于中间操作，所以我们可以在过滤后的结果来应用其他Stream操作（比如forEach）。forEach需要一个函数来对过滤后的元素依次执行。forEach是一个最终操作，所以我们不能在forEach之后来执行其他Stream操作。 12345// 测试 Filter(过滤)stringList .stream() .filter((s) -&gt; s.startsWith("a")) .forEach(System.out::println);//aaa2 aaa1 forEach 是为 Lambda 而设计的，保持了最紧凑的风格。而且 Lambda 表达式本身是可以重用的，非常方便。 Sorted(排序)排序是一个 中间操作，返回的是排序好后的 Stream。如果你不指定一个自定义的 Comparator 则会使用默认排序。 123456// 测试 Sort (排序)stringList .stream() .sorted() .filter((s) -&gt; s.startsWith("a")) .forEach(System.out::println);// aaa1 aaa2 需要注意的是，排序只创建了一个排列好后的Stream，而不会影响原有的数据源，排序之后原数据stringCollection是不会被修改的： 1System.out.println(stringList);// ddd2, aaa2, bbb1, aaa1, bbb3, ccc, bbb2, ddd1 Map(映射)中间操作 map 会将元素根据指定的 Function 接口来依次将元素转成另外的对象。 下面的示例展示了将字符串转换为大写字符串。你也可以通过map来讲对象转换成其他类型，map返回的Stream类型是根据你map传递进去的函数的返回值决定的。 123456// 测试 Map 操作stringList .stream() .map(String::toUpperCase) .sorted((a, b) -&gt; b.compareTo(a)) .forEach(System.out::println);// "DDD2", "DDD1", "CCC", "BBB3", "BBB2", "AAA2", "AAA1" Match(匹配)Stream提供了多种匹配操作，允许检测指定的Predicate是否匹配整个Stream。所有的匹配操作都是 最终操作 ，并返回一个 boolean 类型的值。 1234567891011121314151617181920// 测试 Match (匹配)操作boolean anyStartsWithA = stringList .stream() .anyMatch((s) -&gt; s.startsWith("a"));System.out.println(anyStartsWithA); // trueboolean allStartsWithA = stringList .stream() .allMatch((s) -&gt; s.startsWith("a"));System.out.println(allStartsWithA); // falseboolean noneStartsWithZ = stringList .stream() .noneMatch((s) -&gt; s.startsWith("z"));System.out.println(noneStartsWithZ); // true Count(计数)计数是一个 最终操作，返回Stream中元素的个数，返回值类型是 long。 1234567//测试 Count (计数)操作 long startsWithB = stringList .stream() .filter((s) -&gt; s.startsWith("b")) .count(); System.out.println(startsWithB); // 3 Reduce(规约)这是一个 最终操作 ，允许通过指定的函数来讲stream中的多个元素规约为一个元素，规约后的结果是通过Optional 接口表示的： 12345678//测试 Reduce (规约)操作Optional&lt;String&gt; reduced = stringList .stream() .sorted() .reduce((s1, s2) -&gt; s1 + "#" + s2);reduced.ifPresent(System.out::println);//aaa1#aaa2#bbb1#bbb2#bbb3#ccc#ddd1#ddd2 译者注： 这个方法的主要作用是把 Stream 元素组合起来。它提供一个起始值（种子），然后依照运算规则（BinaryOperator），和前面 Stream 的第一个、第二个、第 n 个元素组合。从这个意义上说，字符串拼接、数值的 sum、min、max、average 都是特殊的 reduce。例如 Stream 的 sum 就相当于Integer sum = integers.reduce(0, (a, b) -&gt; a+b);也有没有起始值的情况，这时会把 Stream 的前面两个元素组合起来，返回的是 Optional。 123456789101112// 字符串连接，concat = "ABCD"String concat = Stream.of("A", "B", "C", "D").reduce("", String::concat); // 求最小值，minValue = -3.0double minValue = Stream.of(-1.5, 1.0, -3.0, -2.0).reduce(Double.MAX_VALUE, Double::min); // 求和，sumValue = 10, 有起始值int sumValue = Stream.of(1, 2, 3, 4).reduce(0, Integer::sum);// 求和，sumValue = 10, 无起始值sumValue = Stream.of(1, 2, 3, 4).reduce(Integer::sum).get();// 过滤，字符串连接，concat = "ace"concat = Stream.of("a", "B", "c", "D", "e", "F"). filter(x -&gt; x.compareTo("Z") &gt; 0). reduce("", String::concat); 上面代码例如第一个示例的 reduce()，第一个参数（空白字符）即为起始值，第二个参数（String::concat）为 BinaryOperator。这类有起始值的 reduce() 都返回具体的对象。而对于第四个示例没有起始值的 reduce()，由于可能没有足够的元素，返回的是 Optional，请留意这个区别。更多内容查看： IBM：Java 8 中的 Streams API 详解 Parallel Streams(并行流)前面提到过Stream有串行和并行两种，串行Stream上的操作是在一个线程中依次完成，而并行Stream则是在多个线程上同时执行。 下面的例子展示了是如何通过并行Stream来提升性能： 首先我们创建一个没有重复元素的大表： 123456int max = 1000000;List&lt;String&gt; values = new ArrayList&lt;&gt;(max);for (int i = 0; i &lt; max; i++) &#123; UUID uuid = UUID.randomUUID(); values.add(uuid.toString());&#125; 我们分别用串行和并行两种方式对其进行排序，最后看看所用时间的对比。 Sequential Sort(串行排序)123456789//串行排序long t0 = System.nanoTime();long count = values.stream().sorted().count();System.out.println(count);long t1 = System.nanoTime();long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0);System.out.println(String.format("sequential sort took: %d ms", millis)); 121000000sequential sort took: 709 ms//串行排序所用的时间 Parallel Sort(并行排序)12345678910//并行排序long t0 = System.nanoTime();long count = values.parallelStream().sorted().count();System.out.println(count);long t1 = System.nanoTime();long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0);System.out.println(String.format("parallel sort took: %d ms", millis)); 121000000parallel sort took: 475 ms//串行排序所用的时间 上面两个代码几乎是一样的，但是并行版的快了 50% 左右，唯一需要做的改动就是将 stream() 改为parallelStream()。 Maps前面提到过，Map 类型不支持 streams，不过Map提供了一些新的有用的方法来处理一些日常任务。Map接口本身没有可用的 stream（）方法，但是你可以在键，值上创建专门的流或者通过 map.keySet().stream(),map.values().stream()和map.entrySet().stream()。 此外,Maps 支持各种新的和有用的方法来执行常见任务。 1234567Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;();for (int i = 0; i &lt; 10; i++) &#123; map.putIfAbsent(i, "val" + i);&#125;map.forEach((id, val) -&gt; System.out.println(val));//val0 val1 val2 val3 val4 val5 val6 val7 val8 val9 putIfAbsent 阻止我们在null检查时写入额外的代码;forEach接受一个 consumer 来对 map 中的每个元素操作。 此示例显示如何使用函数在 map 上计算代码： 1234567891011map.computeIfPresent(3, (num, val) -&gt; val + num);map.get(3); // val33map.computeIfPresent(9, (num, val) -&gt; null);map.containsKey(9); // falsemap.computeIfAbsent(23, num -&gt; "val" + num);map.containsKey(23); // truemap.computeIfAbsent(3, num -&gt; "bam");map.get(3); // val33 接下来展示如何在Map里删除一个键值全都匹配的项： 1234map.remove(3, "val3");map.get(3); // val33map.remove(3, "val33");map.get(3); // null 另外一个有用的方法： 1map.getOrDefault(42, "not found"); // not found 对Map的元素做合并也变得很容易了： 1234map.merge(9, "val9", (value, newValue) -&gt; value.concat(newValue));map.get(9); // val9map.merge(9, "concat", (value, newValue) -&gt; value.concat(newValue));map.get(9); // val9concat Merge 做的事情是如果键名不存在则插入，否则则对原键对应的值做合并操作并重新插入到map中。 Date API(日期相关API)Java 8在 java.time 包下包含一个全新的日期和时间API。新的Date API与Joda-Time库相似，但它们不一样。以下示例涵盖了此新 API 的最重要部分。译者对这部分内容参考相关书籍做了大部分修改。 译者注(总结)： Clock 类提供了访问当前日期和时间的方法，Clock 是时区敏感的，可以用来取代 System.currentTimeMillis() 来获取当前的微秒数。某一个特定的时间点也可以使用 Instant 类来表示，Instant 类也可以用来创建旧版本的java.util.Date 对象。 在新API中时区使用 ZoneId 来表示。时区可以很方便的使用静态方法of来获取到。 抽象类ZoneId（在java.time包中）表示一个区域标识符。 它有一个名为getAvailableZoneIds的静态方法，它返回所有区域标识符。 jdk1.8中新增了 LocalDate 与 LocalDateTime等类来解决日期处理方法，同时引入了一个新的类DateTimeFormatter 来解决日期格式化问题。可以使用Instant代替 Date，LocalDateTime代替 Calendar，DateTimeFormatter 代替 SimpleDateFormat。 ClockClock 类提供了访问当前日期和时间的方法，Clock 是时区敏感的，可以用来取代 System.currentTimeMillis() 来获取当前的微秒数。某一个特定的时间点也可以使用 Instant 类来表示，Instant 类也可以用来创建旧版本的java.util.Date 对象。 1234567Clock clock = Clock.systemDefaultZone();long millis = clock.millis();System.out.println(millis);//1552379579043Instant instant = clock.instant();System.out.println(instant);Date legacyDate = Date.from(instant); //2019-03-12T08:46:42.588ZSystem.out.println(legacyDate);//Tue Mar 12 16:32:59 CST 2019 Timezones(时区)在新API中时区使用 ZoneId 来表示。时区可以很方便的使用静态方法of来获取到。 抽象类ZoneId（在java.time包中）表示一个区域标识符。 它有一个名为getAvailableZoneIds的静态方法，它返回所有区域标识符。 1234567//输出所有区域标识符System.out.println(ZoneId.getAvailableZoneIds());ZoneId zone1 = ZoneId.of("Europe/Berlin");ZoneId zone2 = ZoneId.of("Brazil/East");System.out.println(zone1.getRules());// ZoneRules[currentStandardOffset=+01:00]System.out.println(zone2.getRules());// ZoneRules[currentStandardOffset=-03:00] LocalTime(本地时间)LocalTime 定义了一个没有时区信息的时间，例如 晚上10点或者 17:30:15。下面的例子使用前面代码创建的时区创建了两个本地时间。之后比较时间并以小时和分钟为单位计算两个时间的时间差： 123456789LocalTime now1 = LocalTime.now(zone1);LocalTime now2 = LocalTime.now(zone2);System.out.println(now1.isBefore(now2)); // falselong hoursBetween = ChronoUnit.HOURS.between(now1, now2);long minutesBetween = ChronoUnit.MINUTES.between(now1, now2);System.out.println(hoursBetween); // -3System.out.println(minutesBetween); // -239 LocalTime 提供了多种工厂方法来简化对象的创建，包括解析时间字符串. 123456789LocalTime late = LocalTime.of(23, 59, 59);System.out.println(late); // 23:59:59DateTimeFormatter germanFormatter = DateTimeFormatter .ofLocalizedTime(FormatStyle.SHORT) .withLocale(Locale.GERMAN);LocalTime leetTime = LocalTime.parse("13:37", germanFormatter);System.out.println(leetTime); // 13:37 LocalDate(本地日期)LocalDate 表示了一个确切的日期，比如 2014-03-11。该对象值是不可变的，用起来和LocalTime基本一致。下面的例子展示了如何给Date对象加减天/月/年。另外要注意的是这些对象是不可变的，操作返回的总是一个新实例。 123456789LocalDate today = LocalDate.now();//获取现在的日期System.out.println("今天的日期: "+today);//2019-03-12LocalDate tomorrow = today.plus(1, ChronoUnit.DAYS);System.out.println("明天的日期: "+tomorrow);//2019-03-13LocalDate yesterday = tomorrow.minusDays(2);System.out.println("昨天的日期: "+yesterday);//2019-03-11LocalDate independenceDay = LocalDate.of(2019, Month.MARCH, 12);DayOfWeek dayOfWeek = independenceDay.getDayOfWeek();System.out.println("今天是周几:"+dayOfWeek);//TUESDAY 从字符串解析一个 LocalDate 类型和解析 LocalTime 一样简单,下面是使用 DateTimeFormatter 解析字符串的例子： 12345678910111213String str1 = "2014==04==12 01时06分09秒"; // 根据需要解析的日期、时间字符串定义解析所用的格式器 DateTimeFormatter fomatter1 = DateTimeFormatter .ofPattern("yyyy==MM==dd HH时mm分ss秒"); LocalDateTime dt1 = LocalDateTime.parse(str1, fomatter1); System.out.println(dt1); // 输出 2014-04-12T01:06:09 String str2 = "2014$$$四月$$$13 20小时"; DateTimeFormatter fomatter2 = DateTimeFormatter .ofPattern("yyy$$$MMM$$$dd HH小时"); LocalDateTime dt2 = LocalDateTime.parse(str2, fomatter2); System.out.println(dt2); // 输出 2014-04-13T20:00 再来看一个使用 DateTimeFormatter 格式化日期的示例 12345LocalDateTime rightNow=LocalDateTime.now();String date=DateTimeFormatter.ISO_LOCAL_DATE_TIME.format(rightNow);System.out.println(date);//2019-03-12T16:26:48.29DateTimeFormatter formatter=DateTimeFormatter.ofPattern("YYYY-MM-dd HH:mm:ss");System.out.println(formatter.format(rightNow));//2019-03-12 16:26:48 LocalDateTime(本地日期时间)LocalDateTime 同时表示了时间和日期，相当于前两节内容合并到一个对象上了。LocalDateTime 和 LocalTime还有 LocalDate 一样，都是不可变的。LocalDateTime 提供了一些能访问具体字段的方法。 12345678910LocalDateTime sylvester = LocalDateTime.of(2014, Month.DECEMBER, 31, 23, 59, 59);DayOfWeek dayOfWeek = sylvester.getDayOfWeek();System.out.println(dayOfWeek); // WEDNESDAYMonth month = sylvester.getMonth();System.out.println(month); // DECEMBERlong minuteOfDay = sylvester.getLong(ChronoField.MINUTE_OF_DAY);System.out.println(minuteOfDay); // 1439 只要附加上时区信息，就可以将其转换为一个时间点Instant对象，Instant时间点对象可以很容易的转换为老式的java.util.Date。 123456Instant instant = sylvester .atZone(ZoneId.systemDefault()) .toInstant();Date legacyDate = Date.from(instant);System.out.println(legacyDate); // Wed Dec 31 23:59:59 CET 2014 格式化LocalDateTime和格式化时间和日期一样的，除了使用预定义好的格式外，我们也可以自己定义格式： 123456DateTimeFormatter formatter = DateTimeFormatter .ofPattern("MMM dd, yyyy - HH:mm");LocalDateTime parsed = LocalDateTime.parse("Nov 03, 2014 - 07:13", formatter);String string = formatter.format(parsed);System.out.println(string); // Nov 03, 2014 - 07:13 和java.text.NumberFormat不一样的是新版的DateTimeFormatter是不可变的，所以它是线程安全的。关于时间日期格式的详细信息在这里。 Annotations(注解)在Java 8中支持多重注解了，先看个例子来理解一下是什么意思。首先定义一个包装类Hints注解用来放置一组具体的Hint注解： 1234567@interface Hints &#123; Hint[] value();&#125;@Repeatable(Hints.class)@interface Hint &#123; String value();&#125; Java 8允许我们把同一个类型的注解使用多次，只需要给该注解标注一下@Repeatable即可。 例 1: 使用包装类当容器来存多个注解（老方法） 12@Hints(&#123;@Hint("hint1"), @Hint("hint2")&#125;)class Person &#123;&#125; 例 2：使用多重注解（新方法） 123@Hint("hint1")@Hint("hint2")class Person &#123;&#125; 第二个例子里java编译器会隐性的帮你定义好@Hints注解，了解这一点有助于你用反射来获取这些信息： 1234567Hint hint = Person.class.getAnnotation(Hint.class);System.out.println(hint); // nullHints hints1 = Person.class.getAnnotation(Hints.class);System.out.println(hints1.value().length); // 2Hint[] hints2 = Person.class.getAnnotationsByType(Hint.class);System.out.println(hints2.length); // 2 即便我们没有在 Person类上定义 @Hints注解，我们还是可以通过 getAnnotation(Hints.class)来获取 @Hints注解，更加方便的方法是使用 getAnnotationsByType 可以直接获取到所有的@Hint注解。另外Java 8的注解还增加到两种新的target上了： 12@Target(&#123;ElementType.TYPE_PARAMETER, ElementType.TYPE_USE&#125;)@interface MyAnnotation &#123;&#125; Whete to go from here?关于Java 8的新特性就写到这了，肯定还有更多的特性等待发掘。JDK 1.8里还有很多很有用的东西，比如Arrays.parallelSort, StampedLock和CompletableFuture等等。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见HTTP/FTP/WebSocket错误代码大全]]></title>
    <url>%2F2016%2F09%2F23%2Fsubject-network-HTTP-Status-codes%2F</url>
    <content type="text"><![CDATA[HTTP1xx消息这一类型的状态码，代表请求已被接受，需要继续处理。这类响应是临时响应，只包含状态行和某些可选的响应头信息，并以空行结束。由于HTTP/1.0协议中没有定义任何1xx状态码，所以除非在某些试验条件下，服务器禁止向此类客户端发送1xx响应。 这些状态码代表的响应都是信息性的，标示客户应该采取的其他行动。 100 - 客户端应当继续发送请求 101 - 切换协议 102 - 处理将被继续执行 2xx成功这一类型的状态码，代表请求已成功被服务器接收、理解、并接受。 200 - （成功）请求已成功，请求所希望的响应头或数据体将随此响应返回。 201 - （已创建）请求成功且服务器已创建了新的资源。。 202 - （已接受）服务器已接受了请求，但尚未对其进行处理。 203 - （非授权信息）服务器已成功处理了请求，但返回了可能来自另一来源的信息。 204 - （无内容）服务器成功处理了请求，但未返回任何内容。 205 - （重置内容）服务器成功处理了请求，但未返回任何内容。 206 - （部分内容）服务器成功处理了部分 GET 请求。 3xx重定向这类状态码代表需要客户端采取进一步的操作才能完成请求。通常，这些状态码用来重定向，后续的请求地址（重定向目标）在本次响应的Location域中指明。按照HTTP/1.0版规范的建议，浏览器不应自动访问超过5次的重定向。对重定向一般是由浏览器来控制重定向的次数，重定向会导致客户端不必要的资源消耗 300 - 多重选择，被请求的资源有一系列可供选择的回馈信息。 301 - 永久移除，被请求的资源已永久移动到新位置。 302 - 临时移动，请求的资源现在临时从不同的URI响应请求。 303 - 查看其他位置，对应当前请求的响应可以在另一个URI上被找到，而且客户端应当采用GET的方式访问那个资源。 304 - 未修改。自从上次请求后，请求的网页未被修改过。服务器返回此响应时，不会返回网页内容。 305 - 使用代理，被请求的资源必须通过指定的代理才能被访问。 306 - 临时重定向，在最新版的规范中，306状态码已经不再被使用。 307 - 临时重定向。 4xx客户端错误这类的状态码代表了客户端看起来可能发生了错误，妨碍了服务器的处理。 400 - 错误的请求。 401 - 访问被拒绝。 402 - 付款要求。 403 - 禁止访问 403.1 - 执行访问被禁止。 403.2 - 读访问被禁止。 403.3 - 写访问被禁止。 403.4 - 要求 SSL。 403.5 - 要求 SSL 128。 403.6 - IP 地址被拒绝。 403.7 - 要求客户端证书。 403.8 - 站点访问被拒绝。 403.9 - 用户数过多。 403.10 - 配置无效。 403.11 - 密码更改。 403.12 - 拒绝访问映射表。 403.13 - 客户端证书被吊销。 403.14 - 拒绝目录列表。 403.15 - 超出客户端访问许可。 403.16 - 客户端证书不受信任或无效。 403.17 - 客户端证书已过期或尚未生效。 403.18 - 在当前的应用程序池中不能执行所请求的 URL。 403.19 - 不能为这个应用程序池中的客户端执行 CGI。 403.20 - Passport 登录失败。 404 - 未找到。 404.0 -（无） – 没有找到文件或目录。 404.1 - 无法在所请求的端口上访问 Web 站点。 404.2 - Web 服务扩展锁定策略阻止本请求。 404.3 - MIME 映射策略阻止本请求。 405 - 用来访问本页面的 HTTP 谓词不被允许（方法不被允许） 406 - 客户端浏览器不接受所请求页面的 MIME 类型。 407 - 要求进行代理身份验证。 408 - 请求超时。 409 - 由于和被请求的资源的当前状态之间存在冲突，请求无法完成。 410 - 被请求的资源在服务器上已经不再可用，而且没有任何已知的转发地址。 411 - 服务器拒绝在没有定义Content-Length头的情况下接受请求。 412 - 前提条件失败。 413 – 请求实体太大。 414 - 请求 URI 太长。 415 – 不支持的媒体类型。 416 – 所请求的范围无法满足。 417 – 执行失败。 418 – 本操作码是在1998年作为IETF的传统愚人节笑话。 421 – 从当前客户端所在的IP地址到服务器的连接数超过了服务器许可的最大范围。 422 – 请求格式正确，但是由于含有语义错误，无法响应。 423 – 当前资源被锁定。 424 – 由于之前的某个请求发生的错误，导致当前请求失败。 425 – 无序的集合。 426 – 客户端应当切换到TLS/1.0。 451 – （由IETF在2015核准后新增加）该访问因法律的要求而被拒绝。 5xx服务器错误这类状态码代表了服务器在处理请求的过程中有错误或者异常状态发生，也有可能是服务器意识到以当前的软硬件资源无法完成对请求的处理。 500 - 内部服务器错误。 501 - 尚未实施，页眉值指定了未实现的配置。 502 - 错误网关，Web 服务器用作网关或代理服务器时收到了无效响应。 503 - 服务不可用，这个错误代码为 IIS 6.0 所专用。 504 - 网关超时，服务器作为网关或代理，未及时从上游服务器接收请求。 505 - HTTP 版本不受支持，服务器不支持请求中所使用的 HTTP 协议版本。 506 - 服务器没有正确配置。 507 - 存储空间不足。服务器无法存储完成请求所必须的内容。这个状况被认为是临时的。 509 - 带宽超过限制。这不是一个官方的状态码，但是仍被广泛使用。 510 - 没有扩展，获取资源所需要的策略并没有被满足。 FTP1xx初步肯定的初步答复，这些状态代码指示一项操作已经成功开始，但客户端希望在继续操作新命令前得到另一个答复。 110 重新启动标记答复。 120 服务已就绪，在 nnn 分钟后开始。 125 数据连接已打开，正在开始传输。 150 文件状态正常，准备打开数据连接。 2xx完成肯定的完成答复，一项操作已经成功完成。客户端可以执行新命令。 200 命令确定。 202 未执行命令，站点上的命令过多。 211 系统状态，或系统帮助答复。 212 目录状态。 213 文件状态。 214 帮助消息。 215 NAME 系统类型，其中，NAME 是 Assigned Numbers 文档中所列的正式系统名称。 220 服务就绪，可以执行新用户的请求。 221 服务关闭控制连接。如果适当，请注销。 225 数据连接打开，没有进行中的传输。 226 关闭数据连接。请求的文件操作已成功（例如，传输文件或放弃文件）。 227 进入被动模式 (h1,h2,h3,h4,p1,p2)。 230 用户已登录，继续进行。 250 请求的文件操作正确，已完成。 257 已创建“PATHNAME”。 3xx中间肯定的中间答复，该命令已成功，但服务器需要更多来自客户端的信息以完成对请求的处理。 331 用户名正确，需要密码。 332 需要登录帐户。 350 请求的文件操作正在等待进一步的信息。 4xx瞬态否定瞬态否定的完成答复，该命令不成功，但错误是暂时的。如果客户端重试命令，可能会执行成功。 421 服务不可用，正在关闭控制连接。如果服务确定它必须关闭，将向任何命令发送这一应答。 425 无法打开数据连接。 426 Connection closed; transfer aborted. 450 未执行请求的文件操作。文件不可用（例如，文件繁忙）。 451 请求的操作异常终止：正在处理本地错误。 452 未执行请求的操作。系统存储空间不够。 5xx永久性否定永久性否定的完成答复，该命令不成功，错误是永久性的。如果客户端重试命令，将再次出现同样的错误。 500 语法错误，命令无法识别。这可能包括诸如命令行太长之类的错误。 501 在参数中有语法错误。 502 未执行命令。 503 错误的命令序列。 504 未执行该参数的命令。 530 未登录。 532 存储文件需要帐户。 550 未执行请求的操作。文件不可用（例如，未找到文件，没有访问权限）。 551 请求的操作异常终止：未知的页面类型。 552 请求的文件操作异常终止：超出存储分配（对于当前目录或数据集）。 553 未执行请求的操作。不允许的文件名。 6xx受保护 600 Series，Replies regarding confidentiality and integrity 631 Integrity protected reply. 632 Confidentiality and integrity protected reply. 633 Confidentiality protected reply. WebSockets状态码WebSockets 的CloseEvent 会在连接关闭时发送给使用 WebSockets 的客户端。它在 WebSocket 对象的 onclose 事件监听器中使用。服务端发送的关闭码，以下为已分配的状态码。 状态码 名称 描述 0–999 - 保留段, 未使用。 1000 CLOSE_NORMAL 正常关闭; 无论为何目的而创建, 该链接都已成功完成任务。 1001 CLOSE_GOING_AWAY 终端离开, 可能因为服务端错误, 也可能因为浏览器正从打开连接的页面跳转离开。 1002 CLOSE_PROTOCOL_ERROR 由于协议错误而中断连接。 1003 CLOSE_UNSUPPORTED 由于接收到不允许的数据类型而断开连接 (如仅接收文本数据的终端接收到了二进制数据)。 1004 - 保留。 其意义可能会在未来定义。 1005 CLOSE_NO_STATUS 保留。 表示没有收到预期的状态码。 1006 CLOSE_ABNORMAL 保留。 用于期望收到状态码时连接非正常关闭 (也就是说, 没有发送关闭帧)。 1007 Unsupported Data 由于收到了格式不符的数据而断开连接 (如文本消息中包含了非 UTF-8 数据)。 1008 Policy Violation 由于收到不符合约定的数据而断开连接。 这是一个通用状态码, 用于不适合使用 1003 和 1009 状态码的场景。 1009 CLOSE_TOO_LARGE 由于收到过大的数据帧而断开连接。 1010 Missing Extension 客户端期望服务器商定一个或多个拓展, 但服务器没有处理, 因此客户端断开连接。 1011 Internal Error 客户端由于遇到没有预料的情况阻止其完成请求, 因此服务端断开连接。 1012 Service Restart 服务器由于重启而断开连接。 [Ref] 1013 Try Again Later 服务器由于临时原因断开连接, 如服务器过载因此断开一部分客户端连接。 [Ref] 1014 - 由 WebSocket 标准保留以便未来使用。 1015 TLS Handshake 保留。 表示连接由于无法完成 TLS 握手而关闭 (例如无法验证服务器证书)。 1016–1999 - 由 WebSocket 标准保留以便未来使用。 2000–2999 - 由 WebSocket 拓展保留使用。 3000–3999 - 可以由库或框架使用。 不应由应用使用。 可以在 IANA 注册, 先到先得。 4000–4999 - 可以由应用使用。 参考资料 HTTP状态码列表 FTP状态码列表 MDN CloseEvent HTTP 404 List of FTP server return codes HTTP概述 Help for HTTP error 403: “Forbidden”]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTPS中的TLS]]></title>
    <url>%2F2016%2F07%2F22%2Fsubject-network-HTTPS%E4%B8%AD%E7%9A%84TLS%2F</url>
    <content type="text"><![CDATA[1. SSL 与 TLSSSL：（Secure Socket Layer） 安全套接层，于 1994 年由网景公司设计，并于 1995 年发布了 3.0 版本TLS：（Transport Layer Security）传输层安全性协议，是 IETF 在 SSL3.0 的基础上设计的协议以下全部使用 TLS 来表示 2. 从网络协议的角度理解 HTTPSHTTP：HyperText Transfer Protocol 超文本传输协议HTTPS：Hypertext Transfer Protocol Secure 超文本传输安全协议TLS：位于 HTTP 和 TCP 之间的协议，其内部有 TLS握手协议、TLS记录协议HTTPS 经由 HTTP 进行通信，但利用 TLS 来保证安全，即 HTTPS = HTTP + TLS 3. 从密码学的角度理解 HTTPSHTTPS 使用 TLS 保证安全，这里的“安全”分两部分，一是传输内容加密、二是服务端的身份认证 3.1. TLS 工作流程此为服务端单向认证，还有客户端/服务端双向认证，流程类似，只不过客户端也有自己的证书，并发送给服务器进行验证 3.2. 密码基础3.2.1. 伪随机数生成器为什么叫伪随机数，因为没有真正意义上的随机数，具体可以参考 Random/TheadLocalRandom它的主要作用在于生成对称密码的秘钥、用于公钥密码生成秘钥对 3.2.2. 消息认证码消息认证码主要用于验证消息的完整性与消息的认证，其中消息的认证指“消息来自正确的发送者” 消息认证码用于验证和认证，而不是加密 发送者与接收者事先共享秘钥 发送者根据发送消息计算 MAC 值 发送者发送消息和 MAC 值 接收者根据接收到的消息计算 MAC 值 接收者根据自己计算的 MAC 值与收到的 MAC 对比 如果对比成功，说明消息完整，并来自与正确的发送者 3.2.3. 数字签名消息认证码的缺点在于无法防止否认，因为共享秘钥被 client、server 两端拥有，server 可以伪造 client 发送给自己的消息（自己给自己发送消息），为了解决这个问题，我们需要它们有各自的秘钥不被第二个知晓（这样也解决了共享秘钥的配送问题） 数字签名和消息认证码都不是为了加密可以将单向散列函数获取散列值的过程理解为使用 md5 摘要算法获取摘要的过程 使用自己的私钥对自己所认可的消息生成一个该消息专属的签名，这就是数字签名，表明我承认该消息来自自己注意：私钥用于加签，公钥用于解签，每个人都可以解签，查看消息的归属人 3.2.4. 公钥密码公钥密码也叫非对称密码，由公钥和私钥组成，它是最开始是为了解决秘钥的配送传输安全问题，即，我们不配送私钥，只配送公钥，私钥由本人保管它与数字签名相反，公钥密码的私钥用于解密、公钥用于加密，每个人都可以用别人的公钥加密，但只有对应的私钥才能解开密文client：明文 + 公钥 = 密文server：密文 + 私钥 = 明文注意：公钥用于加密，私钥用于解密，只有私钥的归属者，才能查看消息的真正内容 3.2.5. 证书证书：全称公钥证书（Public-Key Certificate, PKC）,里面保存着归属者的基本信息，以及证书过期时间、归属者的公钥，并由认证机构（Certification Authority, CA）施加数字签名，表明，某个认证机构认定该公钥的确属于此人 想象这个场景：你想在支付宝页面交易，你需要支付宝的公钥进行加密通信，于是你从百度上搜索关键字“支付宝公钥”，你获得了支什宝的公钥，这个时候，支什宝通过中间人攻击，让你访问到了他们支什宝的页面，最后你在这个支什宝页面完美的使用了支什宝的公钥完成了与支什宝的交易 在上面的场景中，你可以理解支付宝证书就是由支付宝的公钥、和给支付宝颁发证书的企业的数字签名组成任何人都可以给自己或别人的公钥添加自己的数字签名，表明：我拿我的尊严担保，我的公钥/别人的公钥是真的，至于信不信那是另一回事了 3.2.6. 密码小结 密码 作用 组成 消息认证码 确认消息的完整、并对消息的来源认证 共享秘钥+消息的散列值 数字签名 对消息的散列值签名 公钥+私钥+消息的散列值 公钥密码 解决秘钥的配送问题 公钥+私钥+消息 证书 解决公钥的归属问题 公钥密码中的公钥+数字签名 3.3. TLS 使用的密码技术 伪随机数生成器：秘钥生成随机性，更难被猜测 对称密码：对称密码使用的秘钥就是由伪随机数生成，相较于非对称密码，效率更高 消息认证码：保证消息信息的完整性、以及验证消息信息的来源 公钥密码：证书技术使用的就是公钥密码 数字签名：验证证书的签名，确定由真实的某个 CA 颁发 证书：解决公钥的真实归属问题，降低中间人攻击概率 3.4. TLS 总结TLS 是一系列密码工具的框架，作为框架，它也是非常的灵活，体现在每个工具套件它都可以替换，即：客户端与服务端之间协商密码套件，从而更难的被攻破，例如使用不同方式的对称密码，或者公钥密码、数字签名生成方式、单向散列函数技术的替换等 4. RSA 简单示例RSA 是一种公钥密码算法，我们简单的走一遍它的加密解密过程加密算法：密文 = (明文^E) mod N，其中公钥为{E,N}，即”求明文的E次方的对 N 的余数“解密算法：明文 = (密文^D) mod N，其中秘钥为{D,N}，即”求密文的D次方的对 N 的余数“例：我们已知公钥为{5,323}，私钥为{29,323}，明文为300，请写出加密和解密的过程： 加密：密文 = 123 ^ 5 mod 323 = 225解密：明文 = 225 ^ 29 mod 323 = [[(225 ^ 5) mod 323] * [(225 ^ 5) mod 323] * [(225 ^ 5) mod 323] * [(225 ^ 5) mod 323] * [(225 ^ 5) mod 323] * [(225 ^ 4) mod 323]] mod 323 = (4 * 4 * 4 * 4 * 4 * 290) mod 323 = 123 5. 参考 SSL加密发生在哪里：https://security.stackexchange.com/questions/19681/where-does-ssl-encryption-take-place TLS工作流程：https://blog.csdn.net/ustccw/article/details/76691248 《图解密码技术》：https://book.douban.com/subject/26822106/ 豆瓣评分 9.5]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络]]></title>
    <url>%2F2016%2F07%2F22%2Fsubject-network-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[相对与上一个版本的计算机网路面试知识总结，这个版本增加了 “TCP协议如何保证可靠传输”包括超时重传、停止等待协议、滑动窗口、流量控制、拥塞控制等内容并且对一些已有内容做了补充。 一 OSI与TCP/IP各层的结构与功能,都有哪些协议五层协议的体系结构学习计算机网络时我们一般采用折中的办法，也就是中和 OSI 和 TCP/IP 的优点，采用一种只有五层协议的体系结构，这样既简洁又能将概念阐述清楚。 结合互联网的情况，自上而下地，非常简要的介绍一下各层的作用。 1 应用层应用层(application-layer）的任务是通过应用进程间的交互来完成特定网络应用。应用层协议定义的是应用进程（进程：主机中正在运行的程序）间的通信和交互的规则。对于不同的网络应用需要不同的应用层协议。在互联网中应用层协议很多，如域名系统DNS，支持万维网应用的 HTTP协议，支持电子邮件的 SMTP协议等等。我们把应用层交互的数据单元称为报文。 域名系统 域名系统(Domain Name System缩写 DNS，Domain Name被译为域名)是因特网的一项核心服务，它作为可以将域名和IP地址相互映射的一个分布式数据库，能够使人更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。（百度百科）例如：一个公司的 Web 网站可看作是它在网上的门户，而域名就相当于其门牌地址，通常域名都使用该公司的名称或简称。例如上面提到的微软公司的域名，类似的还有：IBM 公司的域名是 www.ibm.com、Oracle 公司的域名是 www.oracle.com、Cisco公司的域名是 www.cisco.com 等。 HTTP协议 超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的 WWW（万维网） 文件都必须遵守这个标准。设计 HTTP 最初的目的是为了提供一种发布和接收 HTML 页面的方法。（百度百科） 2 运输层运输层(transport layer)的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。应用进程利用该服务传送应用层报文。“通用的”是指并不针对某一个特定的网络应用，而是多种应用可以使用同一个运输层服务。由于一台主机可同时运行多个线程，因此运输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面运输层的服务，分用和复用相反，是运输层把收到的信息分别交付上面应用层中的相应进程。 运输层主要使用以下两种协议 传输控制协议 TCP（Transmission Control Protocol）–提供面向连接的，可靠的数据传输服务。 用户数据协议 UDP（User Datagram Protocol）–提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性）。 UDP 的主要特点 UDP 是无连接的； UDP 使用尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的链接状态（这里面有许多参数）； UDP 是面向报文的； UDP 没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如 直播，实时视频会议等）； UDP 支持一对一、一对多、多对一和多对多的交互通信； UDP 的首部开销小，只有8个字节，比TCP的20个字节的首部要短。 TCP 的主要特点 TCP 是面向连接的。（就好像打电话一样，通话前需要先拨号建立连接，通话结束后要挂机释放连接）； 每一条 TCP 连接只能有两个端点，每一条TCP连接只能是点对点的（一对一）； TCP 提供可靠交付的服务。通过TCP连接传送的数据，无差错、不丢失、不重复、并且按序到达； TCP 提供全双工通信。TCP 允许通信双方的应用进程在任何时候都能发送数据。TCP 连接的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据； 面向字节流。TCP 中的“流”（Stream）指的是流入进程或从进程流出的字节序列。“面向字节流”的含义是：虽然应用程序和 TCP 的交互是一次一个数据块（大小不等），但 TCP 把应用程序交下来的数据仅仅看成是一连串的无结构的字节流。 3 网络层在 计算机网络中进行通信的两个计算机之间可能会经过很多个数据链路，也可能还要经过很多通信子网。网络层的任务就是选择合适的网间路由和交换结点， 确保数据及时传送。 在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组和包进行传送。在 TCP/IP 体系结构中，由于网络层使用 IP 协议，因此分组也叫 IP 数据报 ，简称 数据报。 这里要注意：不要把运输层的“用户数据报 UDP ”和网络层的“ IP 数据报”弄混。另外，无论是哪一层的数据单元，都可笼统地用“分组”来表示。 这里强调指出，网络层中的“网络”二字已经不是我们通常谈到的具体网络，而是指计算机网络体系结构模型中第三层的名称. 互联网是由大量的异构（heterogeneous）网络通过路由器（router）相互连接起来的。互联网使用的网络层协议是无连接的网际协议（Intert Protocol）和许多路由选择协议，因此互联网的网络层也叫做网际层或IP层。 4 数据链路层数据链路层(data link layer)通常简称为链路层。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。 在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装程帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如同步信息，地址信息，差错控制等）。 在接收数据时，控制信息使接收端能够知道一个帧从哪个比特开始和到哪个比特结束。这样，数据链路层在收到一个帧后，就可从中提出数据部分，上交给网络层。控制信息还使接收端能够检测到所收到的帧中有误差错。如果发现差错，数据链路层就简单地丢弃这个出了差错的帧，以避免继续在网络中传送下去白白浪费网络资源。如果需要改正数据在链路层传输时出现差错（这就是说，数据链路层不仅要检错，而且还要纠错），那么就要采用可靠性传输协议来纠正出现的差错。这种方法会使链路层的协议复杂些。 5 物理层在物理层上所传送的数据单位是比特。 物理层(physical layer)的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。 使其上面的数据链路层不必考虑网络的具体传输介质是什么。“透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。 在互联网使用的各种协中最重要和最著名的就是 TCP/IP 两个协议。现在人们经常提到的TCP/IP并不一定单指TCP和IP这两个具体的协议，而往往表示互联网所使用的整个TCP/IP协议族。 总结一下上面我们对计算机网络的五层体系结构有了初步的了解，下面附送一张七层体系结构图总结一下。图片来源：https://blog.csdn.net/yaopeng_2005/article/details/7064869 二 TCP 三次握手和四次挥手(面试常客)为了准确无误地把数据送达目标处，TCP协议采用了三次握手策略。 漫画图解： 图片来源：《图解HTTP》 简单示意图： 客户端–发送带有 SYN 标志的数据包–一次握手–服务端 服务端–发送带有 SYN/ACK 标志的数据包–二次握手–客户端 客户端–发送带有带有 ACK 标志的数据包–三次握手–服务端 为什么要三次握手三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。 第一次握手：Client 什么都不能确认；Server 确认了对方发送正常 第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己接收正常，对方发送正常 第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常 所以三次握手就能确认双发收发功能都正常，缺一不可。 为什么要传回 SYN接收端传回发送端所发送的 SYN 是为了告诉发送端，我接收到的信息确实就是你所发送的信号了。 SYN 是 TCP/IP 建立连接时使用的握手信号。在客户机和服务器之间建立正常的 TCP 网络连接时，客户机首先发出一个 SYN 消息，服务器使用 SYN-ACK 应答表示接收到了这个消息，最后客户机再以 ACK(Acknowledgement[汉译：确认字符 ,在数据通信传输中，接收站发给发送站的一种传输控制字符。它表示确认发来的数据已经接受无误。 ]）消息响应。这样在客户机和服务器之间才能建立起可靠的TCP连接，数据才可以在客户机和服务器之间传递。 传了 SYN,为啥还要传 ACK双方通信无误必须是两者互相发送信息都无误。传了 SYN，证明发送方到接收方的通道没有问题，但是接收方到发送方的通道还需要 ACK 信号来进行验证。 断开一个 TCP 连接则需要“四次挥手”： 客户端-发送一个 FIN，用来关闭客户端到服务器的数据传送 服务器-收到这个 FIN，它发回一 个 ACK，确认序号为收到的序号加1 。和 SYN 一样，一个 FIN 将占用一个序号 服务器-关闭与客户端的连接，发送一个FIN给客户端 客户端-发回 ACK 报文确认，并将确认序号设置为收到序号加1 为什么要四次挥手任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后就完全关闭了TCP连接。 举个例子：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话，于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。 上面讲的比较概括，推荐一篇讲的比较细致的文章：https://blog.csdn.net/qzcsu/article/details/72861891 三 TCP、UDP 协议的区别 UDP 在传送数据之前不需要先建立连接，远地主机在收到 UDP 报文后，不需要给出任何确认。虽然 UDP 不提供可靠交付，但在某些情况下 UDP 确是一种最有效的工作方式（一般用于即时通信），比如： QQ 语音、 QQ 视频 、直播等等 TCP 提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。 TCP 不提供广播或多播服务。由于 TCP 要提供可靠的，面向连接的传输服务（TCP的可靠体现在TCP在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源），这一难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多处理机资源。TCP 一般用于文件传输、发送和接收邮件、远程登录等场景。 四 TCP 协议如何保证可靠传输 应用数据被分割成 TCP 认为最适合发送的数据块。 TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 校验和： TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 TCP 的接收端会丢弃重复的数据。 流量控制： TCP 连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制） 拥塞控制： 当网络拥塞时，减少数据的发送。 停止等待协议 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 超时重传： 当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。 停止等待协议 停止等待协议是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组； 在停止等待协议中，若接收方收到重复分组，就丢弃该分组，但同时还要发送确认； 1) 无差错情况: 发送方发送分组,接收方在规定时间内收到,并且回复确认.发送方再次发送。 2) 出现差错情况（超时重传）:停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重转时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为 自动重传请求 ARQ 。另外在停止等待协议中若收到重复分组，就丢弃该分组，但同时还要发送确认。连续 ARQ 协议 可提高信道利用率。发送维持一个发送窗口，凡位于发送窗口内的分组可连续发送出去，而不需要等待对方确认。接收方一般采用累积确认，对按序到达的最后一个分组发送确认，表明到这个分组位置的所有分组都已经正确收到了。 3) 确认丢失和确认迟到 确认丢失：确认消息在传输过程丢失 当A发送M1消息，B收到后，B向A发送了一个M1确认消息，但却在传输过程中丢失。而A并不知道，在超时计时过后，A重传M1消息，B再次收到该消息后采取以下两点措施： 丢弃这个重复的M1消息，不向上层交付。 向A发送确认消息。（不会认为已经发送过了，就不再发送。A能重传，就证明B的确认消息丢失）。 确认迟到 ：确认消息在传输过程中迟到A发送M1消息，B收到并发送确认。在超时时间内没有收到确认消息，A重传M1消息，B仍然收到并继续发送确认消息（B收到了2份M1）。此时A收到了B第二次发送的确认消息。接着发送其他数据。过了一会，A收到了B第一次发送的对M1的确认消息（A也收到了2份确认消息）。处理如下： A收到重复的确认后，直接丢弃。 B收到重复的M1后，也直接丢弃重复的M1。 自动重传请求 ARQ 协议停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重传时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为自动重传请求ARQ。 优点： 简单 缺点： 信道利用率低 连续ARQ协议连续 ARQ 协议可提高信道利用率。发送方维持一个发送窗口，凡位于发送窗口内的分组可以连续发送出去，而不需要等待对方确认。接收方一般采用累计确认，对按序到达的最后一个分组发送确认，表明到这个分组为止的所有分组都已经正确收到了。 优点： 信道利用率高，容易实现，即使确认丢失，也不必重传。 缺点： 不能向发送方反映出接收方已经正确收到的所有分组的信息。 比如：发送方发送了 5条 消息，中间第三条丢失（3号），这时接收方只能对前两个发送确认。发送方无法知道后三个分组的下落，而只好把后三个全部重传一次。这也叫 Go-Back-N（回退 N），表示需要退回来重传已经发送过的 N 个消息。 滑动窗口 TCP 利用滑动窗口实现流量控制的机制。 滑动窗口（Sliding window）是一种流量控制技术。早期的网络通信中，通信双方不会考虑网络的拥挤情况直接发送数据。由于大家不知道网络拥塞状况，同时发送数据，导致中间节点阻塞掉包，谁也发不了数据，所以就有了滑动窗口机制来解决此问题。 TCP 中采用滑动窗口来进行传输控制，滑动窗口的大小意味着接收方还有多大的缓冲区可以用于接收数据。发送方可以通过滑动窗口的大小来确定应该发送多少字节的数据。当滑动窗口为 0 时，发送方一般不能再发送数据报，但有两种情况除外，一种情况是可以发送紧急数据，例如，允许用户终止在远端机上的运行进程。另一种情况是发送方可以发送一个 1 字节的数据报来通知接收方重新声明它希望接收的下一字节及发送方的滑动窗口大小。 流量控制 TCP 利用滑动窗口实现流量控制。 流量控制是为了控制发送方发送速率，保证接收方来得及接收。 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。 拥塞控制在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫拥塞。拥塞控制就是为了防止过多的数据注入到网络中，这样就可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机，所有的路由器，以及与降低网络传输性能有关的所有因素。相反，流量控制往往是点对点通信量的控制，是个端到端的问题。流量控制所要做到的就是抑制发送端发送数据的速率，以便使接收端来得及接收。 为了进行拥塞控制，TCP 发送方要维持一个 拥塞窗口(cwnd) 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。 TCP的拥塞控制采用了四种算法，即 慢开始 、 拥塞避免 、快重传 和 快恢复。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理 AQM），以减少网络拥塞的发生。 慢开始： 慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd初始值为1，每经过一个传播轮次，cwnd加倍。 拥塞避免： 拥塞避免算法的思路是让拥塞窗口cwnd缓慢增大，即每经过一个往返时间RTT就把发送放的cwnd加1. 快重传与快恢复：在 TCP/IP 中，快速重传和恢复（fast retransmit and recovery，FRR）是一种拥塞控制算法，它能快速恢复丢失的数据包。没有 FRR，如果数据包丢失了，TCP 将会使用定时器来要求传输暂停。在暂停的这段时间内，没有新的或复制的数据包被发送。有了 FRR，如果接收机接收到一个不按顺序的数据段，它会立即给发送机发送一个重复确认。如果发送机接收到三个重复确认，它会假定确认件指出的数据段丢失了，并立即重传这些丢失的数据段。有了 FRR，就不会因为重传时要求的暂停被耽误。 当有单独的数据包丢失时，快速重传和恢复（FRR）能最有效地工作。当有多个数据信息包在某一段很短的时间内丢失时，它则不能很有效地工作。 五 在浏览器中输入url地址 -&gt;&gt; 显示主页的过程（面试常客）百度好像最喜欢问这个问题。 打开一个网页，整个过程会使用哪些协议 图解（图片来源：《图解HTTP》）： 总体来说分为以下几个过程: DNS解析 TCP连接 发送HTTP请求 服务器处理请求并返回HTTP报文 浏览器解析渲染页面 连接结束 具体可以参考下面这篇文章： https://segmentfault.com/a/1190000006879700 六 状态码 七 各种协议与HTTP协议之间的关系一般面试官会通过这样的问题来考察你对计算机网络知识体系的理解。 图片来源：《图解HTTP》 八 HTTP长连接、短连接在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个HTML或其他类型的Web页中包含有其他的Web资源（如JavaScript文件、图像文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个HTTP会话。 而从HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头加入这行代码： 1Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。 —— 《HTTP长连接、短连接究竟是什么？》 写在最后计算机网络常见问题回顾 ①TCP三次握手和四次挥手、 ②在浏览器中输入url地址-&gt;&gt;显示主页的过程 ③HTTP和HTTPS的区别 ④TCP、UDP协议的区别 ⑤常见的状态码。 建议非常推荐大家看一下 《图解HTTP》 这本书，这本书页数不多，但是内容很是充实，不管是用来系统的掌握网络方面的一些知识还是说纯粹为了应付面试都有很大帮助。下面的一些文章只是参考。大二学习这门课程的时候，我们使用的教材是 《计算机网络第七版》（谢希仁编著），不推荐大家看这本教材，书非常厚而且知识偏理论，不确定大家能不能心平气和的读完。 参考 https://blog.csdn.net/qq_16209077/article/details/52718250 https://blog.csdn.net/zixiaomuwu/article/details/60965466 https://blog.csdn.net/turn__back/article/details/73743641]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[干货：计算机网络知识总结]]></title>
    <url>%2F2016%2F07%2F22%2Fsubject-network-%E5%B9%B2%E8%B4%A7%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录结构 1. 计算机概述 2. 物理层 3. 数据链路层 4. 网络层 5. 运输层 6. 应用层一计算机概述（1），基本术语 结点 （node）：网络中的结点可以是计算机，集线器，交换机或路由器等。 链路（link ）：从一个结点到另一个结点的一段物理线路。中间没有任何其他交点。 主机（host）：连接在因特网上的计算机. ISP（Internet Service Provider）：因特网服务提供者（提供商）. IXP（Internet eXchange Point）：互联网交换点IXP的主要作用就是允许两个网络直接相连并交换分组，而不需要再通过第三个网络来转发分组。. RFC(Request For Comments)意思是“请求评议”，包含了关于Internet几乎所有的重要的文字资料。 广域网WAN（Wide Area Network）任务是通过长距离运送主机发送的数据 城域网MAN（Metropolitan Area Network）用来讲多个局域网进行互连 局域网LAN（Local Area Network）学校或企业大多拥有多个互连的局域网 个人区域网PAN（Personal Area Network）在个人工作的地方把属于个人使用的电子设备用无线技术连接起来的网络 端系统（end system）：处在因特网边缘的部分即是连接在因特网上的所有的主机. 分组（packet ）：因特网中传送的数据单元。由首部header和数据段组成。分组又称为包，首部可称为包头。 存储转发（store and forward ）:路由器收到一个分组，先存储下来，再检查气首部，查找转发表，按照首部中的目的地址，找到合适的接口转发出去。 带宽（bandwidth）：在计算机网络中，表示在单位时间内从网络中的某一点到另一点所能通过的“最高数据率”。常用来表示网络的通信线路所能传送数据的能力。单位是“比特每秒”，记为b/s。 吞吐量（throughput ）：表示在单位时间内通过某个网络（或信道、接口）的数据量。吞吐量更经常地用于对现实世界中的网络的一种测量，以便知道实际上到底有多少数据量能够通过网络。吞吐量受网络的带宽或网络的额定速率的限制。（2），重要知识点总结 1，计算机网络（简称网络）把许多计算机连接在一起，而互联网把许多网络连接在一起，是网络的网络。 2，小写字母i开头的internet（互联网）是通用名词，它泛指由多个计算机网络相互连接而成的网络。在这些网络之间的通信协议（即通信规则）可以是任意的。 大写字母I开头的Internet（互联网）是专用名词，它指全球最大的，开放的，由众多网络相互连接而成的特定的互联网，并采用TCP/IP协议作为通信规则，其前身为ARPANET。Internet的推荐译名为因特网，现在一般流行称为互联网。 3，路由器是实现分组交换的关键构件，其任务是转发受到的分组，这是网络核心部分最重要的功能。分组交换采用存储转发技术，表示把一个报文（要发送的整块数据）分为几个分组后在进行传送。在发送报文之前，先把较长的报文划分成为一个个更小的等长数据段。在每个数据端的前面加上一些由必要的控制信息组成的首部后，就构成了一个分组。分组有称为包。分组是在互联网中传送的数据单元，正式由于分组的头部包含了诸如目的地址和源地址等重要控制信息，每一个分组才能在互联网中独立的选择传输路径，并正确地交付到分组传输的终点。 4，互联网按工作方式可划分为边缘部分和核心部分。主机在网络的边缘部分，其作用是进行信息处理。由大量网络和连接这些网络的路由西组成边缘部分，其作用是提供连通性和交换。 5，计算机通信是计算机中进程（即运行着的程序）之间的通信。计算机网络采用的通信方式是客户-服务器方式（C/S方式）和对等连接方式（P2P方式）。 6，客户和服务器都是指通信中所涉及的应用进程。客户是服务请求方，服务器是服务提供方。 7，按照作用范围的不同，计算机网络分为广域网WAN，城域网MAN,局域网LAN，个人区域网PAN。 8，计算机网络最常用的性能指标是：速率，带宽，吞吐量，时延（发送时延，处理时延，排队时延），时延带宽积，往返时间和信道利用率。 9，网络协议即协议，是为进行网络中的数据交换而建立的规则。计算机网络的各层以及其协议集合，称为网络的体系结构。 10，五层体系结构由应用层，运输层，网络层（网际层），数据链路层，物理层组成。运输层最主要的协议是TCP和UDP协议，网络层最重要的协议是IP协议。 二物理层（1），基本术语数据（data）：运送消息的实体。信号（signal）：数据的电气的或电磁的表现。或者说信号是适合在传输介质上传输的对象。码元（ code）： 在使用时间域（或简称为时域）的波形来表示数字信号时，代表不同离散数值的基本波形。单工（simplex ）：只能有一个方向的通信而没有反方向的交互。半双工（half duplex ）：通信的双方都可以发送信息，但不能双方同时发送(当然也就不能同时接收)。全双工（full duplex）： 通信的双方可以同时发送和接收信息。奈氏准则：在任何信道中，码元的传输的效率是有上限的，传输速率超过此上限，就会出现严重的码间串扰问题，使接收端对码元的判决（即识别）成为不可能。基带信号（baseband signal）：来自信源的信号。指没有经过调制的数字信号或模拟信号。 带通（频带）信号（bandpass signal）：把基带信号经过载波调制后，把信号的频率范围搬移到较高的频段以便在信道中传输（即仅在一段频率范围内能够通过信道），这里调制过后的信号就是带通信号。 调制（modulation ）：对信号源的信息进行处理后加到载波信号上，使其变为适合在信道传输的形式的过程。信噪比（signal-to-noise ratio ）：指信号的平均功率和噪声的平均功率之比，记为S/N。信噪比（dB）=10*log10（S/N）信道复用（channel multiplexing ）：指多个用户共享同一个信道。（并不一定是同时）比特率（bit rate ）：单位时间（每秒）内传送的比特数。波特率（baud rate）：单位时间载波调制状态改变的次数。针对数据信号对载波的调制速率。复用（multiplexing）：共享信道的方法ADSL（Asymmetric Digital Subscriber Line ）： 非对称数字用户线。光纤同轴混合网（HFC网）:在目前覆盖范围很广的有线电视网的基础上开发的一种居民宽带接入网（2），重要知识点总结 1，物理层的主要任务就是确定与传输媒体接口有关的一些特性，如机械特性，电气特性，功能特性，过程特性。 2，一个数据通信系统可划分为三大部分，即源系统，传输系统，目的系统。源系统包括源点（或源站，信源）和发送器，目的系统包括接收器和终点。 3，通信的目的是传送消息。如话音，文字，图像等都是消息，数据是运送消息的实体。信号则是数据的电器或电磁的表现。 4，根据信号中代表消息的参数的取值方式不同，信号可分为模拟信号（或连续信号）和数字信号（或离散信号）。在使用时间域（简称时域）的波形表示数字信号时，代表不同离散数值的基本波形称为码元。 5，根据双方信息交互的方式，通信可划分为单向通信（或单工通信），双向交替通信（或半双工通信），双向同时通信（全双工通信）。 6，来自信源的信号称为基带信号。信号要在信道上传输就要经过调制。调制有基带调制和带通调制之分。最基本的带通调制方法有调幅，调频和调相。还有更复杂的调制方法，如正交振幅调制。 7，要提高数据在信道上的传递速率，可以使用更好的传输媒体，或使用先进的调制技术。但数据传输速率不可能任意被提高。 8，传输媒体可分为两大类，即导引型传输媒体（双绞线，同轴电缆，光纤）和非导引型传输媒体（无线，红外，大气激光）。 9，为了有效利用光纤资源，在光纤干线和用户之间广泛使用无源光网络PON。无源光网络无需配备电源，其长期运营成本和管理成本都很低。最流行的无源光网络是以太网无源光网络EPON和吉比特无源光网络GPON。 （3），最重要的知识点①，物理层的任务 透明地传送比特流。也可以将物理层的主要任务描述为确定与传输媒体的接口的一些特性，即：机械特性（接口所用接线器的一些物理属性如形状尺寸），电气特性（接口电缆的各条线上出现的电压的范围），功能特性（某条线上出现的某一电平的电压的意义），过程特性（对于不同功能能的各种可能事件的出现顺序）。 拓展： 物理层考虑的是怎样才能在连接各种计算机的传输媒体上传输数据比特流，而不是指具体的传输媒体。现有的计算机网络中的硬件设备和传输媒体的种类非常繁多，而且通信手段也有许多不同的方式。物理层的作用正是尽可能地屏蔽掉这些传输媒体和通信手段的差异，使物理层上面的数据链路层感觉不到这些差异，这样就可以使数据链路层只考虑完成本层的协议和服务，而不必考虑网络的具体传输媒体和通信手段是什么。 ②，几种常用的信道复用技术 ③，几种常用的宽带接入技术，主要是ADSL和FTTx 用户到互联网的宽带接入方法有非对称数字用户线ADSL（用数字技术对现有的模拟电话线进行改造，而不需要重新布线。ASDL的快速版本是甚高速数字用户线VDSL。），光纤同轴混合网HFC（是在目前覆盖范围很广的有线电视网的基础上开发的一种居民宽带接入网）和FTTx（即光纤到······）。 三数据链路层（1），基本术语 链路（link）：一个结点到相邻结点的一段物理链路 数据链路（data link）：把实现控制数据运输的协议的硬件和软件加到链路上就构成了数据链路 循环冗余检验CRC（Cyclic Redundancy Check）：为了保证数据传输的可靠性，CRC是数据链路层广泛使用的一种检错技术 帧（frame）：一个数据链路层的传输单元，由一个数据链路层首部和其携带的封包所组成协议数据单元。 MTU（Maximum Transfer Uint ）：最大传送单元。帧的数据部分的的长度上限。 误码率BER（Bit Error Rate ）：在一段时间内，传输错误的比特占所传输比特总数的比率。 PPP（Point-to-Point Protocol ）：点对点协议。即用户计算机和ISP进行通信时所使用的数据链路层协议。以下是PPP帧的示意图: MAC地址（Media Access Control或者Medium Access Control）：意译为媒体访问控制，或称为物理地址、硬件地址，用来定义网络设备的位置。 在OSI模型中，第三层网络层负责 IP地址，第二层数据链路层则负责 MAC地址。 因此一个主机会有一个MAC地址，而每个网络位置会有一个专属于它的IP地址 。 地址是识别某个系统的重要标识符，“名字指出我们所要寻找的资源，地址指出资源所在的地方，路由告诉我们如何到达该处” 网桥（bridge）：一种用于数据链路层实现中继，连接两个或多个局域网的网络互连设备。 交换机（switch ）：广义的来说，交换机指的是一种通信系统中完成信息交换的设备。这里工作在数据链路层的交换机指的是交换式集线器，其实质是一个多接口的网桥（2），重要知识点总结1，链路是从一个结点到相邻节点的一段物理链路，数据链路则在链路的基础上增加了一些必要的硬件（如网络适配器）和软件（如协议的实现） 2，数据链路层使用的主要是点对点信道和广播信道两种。 3，数据链路层传输的协议数据单元是帧。数据链路层的三个基本问题是：封装成帧，透明传输和差错检测 4，循环冗余检验CRC是一种检错方法，而帧检验序列FCS是添加在数据后面的冗余码 5，点对点协议PPP是数据链路层使用最多的一种协议，它的特点是：简单，只检测差错而不去纠正差错，不使用序号，也不进行流量控制，可同时支持多种网络层协议 6，PPPoE是为宽带上网的主机使用的链路层协议 7，局域网的优点是：具有广播功能，从一个站点可方便地访问全网；便于系统的扩展和逐渐演变；提高了系统的可靠性，可用性和生存性。 8，共向媒体通信资源的方法有二：一是静态划分信道(各种复用技术)，而是动态媒体接入控制，又称为多点接入（随即接入或受控接入） 9，计算机与外接局域网通信需要通过通信适配器（或网络适配器），它又称为网络接口卡或网卡。计算器的硬件地址就在适配器的ROM中。 10，以太网采用的无连接的工作方式，对发送的数据帧不进行编号，也不要求对方发回确认。目的站收到有差错帧就把它丢掉，其他什么也不做 11，以太网采用的协议是具有冲突检测的载波监听多点接入CSMA/CD。协议的特点是：发送前先监听，边发送边监听，一旦发现总线上出现了碰撞，就立即停止发送。然后按照退避算法等待一段随机时间后再次发送。 因此，每一个站点在自己发送数据之后的一小段时间内，存在这遭遇碰撞的可能性。以太网上的各站点平等的争用以太网信道 12，以太网的适配器具有过滤功能，它只接收单播帧，广播帧和多播帧。 13，使用集线器可以在物理层扩展以太网（扩展后的以太网任然是一个网络） （3），最重要的知识点① 数据链路层的点对点信道和广播信道的特点，以及这两种信道所使用的协议（PPP协议以及CSMA/CD协议）的特点② 数据链路层的三个基本问题：封装成帧，透明传输，差错检测③ 以太网的MAC层硬件地址④ 适配器，转发器，集线器，网桥，以太网交换机的作用以及适用场合四网络层（1），基本术语虚电路（Virtual Circuit）：在两个终端设备的逻辑或物理端口之间，通过建立的双向的透明传输通道。虚电路表示这只是一条逻辑上的连接，分组都沿着这条逻辑连接按照存储转发方式传送，而并不是真正建立了一条物理连接。IP（Internet Protocol ）：网际协议 IP 是 TCP/IP体系中两个最主要的协议之一，是TCP/IP体系结构网际层的核心。配套的有ARP，RARP，ICMP，IGMP。 ARP（Address Resolution Protocol）：地址解析协议ICMP（Internet Control Message Protocol ）：网际控制报文协议 （ICMP 允许主机或路由器报告差错情况和提供有关异常情况的报告。）子网掩码（subnet mask ）：它是一种用来指明一个IP地址的哪些位标识的是主机所在的子网以及哪些位标识的是主机的位掩码。子网掩码不能单独存在，它必须结合IP地址一起使用。 CIDR（ Classless Inter-Domain Routing ）：无分类域间路由选择 （特点是消除了传统的 A 类、B 类和 C 类地址以及划分子网的概念，并使用各种长度的“网络前缀”(network-prefix)来代替分类地址中的网络号和子网号）默认路由（default route）：当在路由表中查不到能到达目的地址的路由时，路由器选择的路由。默认路由还可以减小路由表所占用的空间和搜索路由表所用的时间。路由选择算法（Virtual Circuit）：路由选择协议的核心部分。因特网采用自适应的，分层次的路由选择协议。（2），重要知识点总结1，TCP/IP协议中的网络层向上只提供简单灵活的，无连接的，尽最大努力交付的数据报服务。网络层不提供服务质量的承诺，不保证分组交付的时限所传送的分组可能出错，丢失，重复和失序。进程之间通信的可靠性由运输层负责 2，在互联网的交付有两种，一是在本网络直接交付不用经过路由器，另一种是和其他网络的间接交付，至少经过一个路由器，但最后一次一定是直接交付 3，分类的IP地址由网络号字段（指明网络）和主机号字段（指明主机）组成。网络号字段最前面的类别指明IP地址的类别。IP地址市一中分等级的地址结构。IP地址管理机构分配IP地址时只分配网络号，主机号由得到该网络号的单位自行分配。路由器根据目的主机所连接的网络号来转发分组。一个路由器至少连接到两个网络，所以一个路由器至少应当有两个不同的IP地址 4，IP数据报分为首部和数据两部分。首部的前一部分是固定长度，共20字节，是所有IP数据包必须具有的（源地址，目的地址，总长度等重要地段都固定在首部）。一些长度可变的可选字段固定在首部的后面。IP首部中的生存时间给出了IP数据报在互联网中所能经过的最大路由器数。可防止IP数据报在互联网中无限制的兜圈子。 5，地址解析协议ARP把IP地址解析为硬件地址。ARP的高速缓存可以大大减少网络上的通信量。因为这样可以使主机下次再与同样地址的主机通信时，可以直接从高速缓存中找到所需要的硬件地址而不需要再去广播方式发送ARP请求分组 6，无分类域间路由选择CIDR是解决目前IP地址紧缺的一个好办法。CIDR记法把IP地址后面加上斜线“/”，然后写上前缀所所占的位数。前缀（或网络前缀用来指明网络），前缀后面的部分是后缀，用来指明主机。CIDR把前缀都相同的连续的IP地址组成一个“CIDR地址块”，IP地址分配都以CIDR地址块为单位。 7， 网际控制报文协议是IP层的协议.ICMP报文作为IP数据报的数据，加上首部后组成IP数据报发送出去。使用ICMP数据报并不是为了实现可靠传输。ICMP允许主机或路由器报告差错情况和提供有关异常情况的报告。ICMP报文的种类有两种 ICMP差错报告报文和ICMP询问报文。 8，要解决IP地址耗尽的问题，最根本的办法是采用具有更大地址弓箭的新版本IP协议-IPv6。IPv6所带来的变化有①更大的地址空间（采用128位地址）②灵活的首部格式③改进的选项④支持即插即用⑤支持资源的预分配⑥IPv6的首部改为8字节对齐。另外IP数据报的目的地址可以是以下三种基本类型地址之一：单播，多播和任播 9，虚拟专用网络VPN利用公用的互联网作为本机构专用网之间的通信载体。VPN内使用互联网的专用地址。一个VPN至少要有一个路由器具有合法的全球IP地址，这样才能和本系统的另一个VPN通过互联网进行通信。所有通过互联网传送的数据都需要加密 10， MPLS的特点是：①支持面向连接的服务质量②支持流量工程，平衡网络负载③有效的支持虚拟专用网VPN。MPLS在入口节点给每一个IP数据报打上固定长度的“标记”，然后根据标记在第二层（链路层）用硬件进行转发（在标记交换路由器中进行标记交换），因而转发速率大大加快。 （3），最重要知识点① 虚拟互联网络的概念② IP地址和物理地址的关系③ 传统的分类的IP地址（包括子网掩码）和误分类域间路由选择CIDR④ 路由选择协议的工作原理五运输层（1），基本术语进程（process）：指计算机中正在运行的程序实体应用进程互相通信：一台主机的进程和另一台主机中的一个进程交换数据的过程（另外注意通信真正的端点不是主机而是主机中的进程，也就是说端到端的通信是应用进程之间的通信）传输层的复用与分用：复用指发送方不同的进程都可以通过统一个运输层协议传送数据。分用指接收方的运输层在剥去报文的首部后能把这些数据正确的交付到目的应用进程。 TCP（Transmission Control Protocol）：传输控制协议UDP（User Datagram Protocol）：用户数据报协议端口（port）（link）：端口的目的是为了确认对方机器是那个进程在于自己进行交互，比如MSN和QQ的端口不同，如果没有端口就可能出现QQ进程和MSN交互错误。端口又称协议端口号。 停止等待协议（link）：指发送方每发送完一个分组就停止发送，等待对方确认，在收到确认之后在发送下一个分组。流量控制（link）：就是让发送方的发送速率不要太快，既要让接收方来得及接收，也不要使网络发生拥塞。拥塞控制（link）：防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。（2），重要知识点总结1，运输层提供应用进程之间的逻辑通信，也就是说，运输层之间的通信并不是真正在两个运输层之间直接传输数据。运输层向应用层屏蔽了下面网络的细节（如网络拓补，所采用的路由选择协议等），它使应用进程之间看起来好像两个运输层实体之间有一条端到端的逻辑通信信道。 2，网络层为主机提供逻辑通信，而运输层为应用进程之间提供端到端的逻辑通信。 3，运输层的两个重要协议是用户数据报协议UDP和传输控制协议TCP。按照OSI的术语，两个对等运输实体在通信时传送的数据单位叫做运输协议数据单元TPDU（Transport Protocol Data Unit）。但在TCP/IP体系中，则根据所使用的协议是TCP或UDP，分别称之为TCP报文段或UDP用户数据报。 4，UDP在传送数据之前不需要先建立连接，远地主机在收到UDP报文后，不需要给出任何确认。虽然UDP不提供可靠交付，但在某些情况下UDP确是一种最有效的工作方式。 TCP提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。TCP不提供广播或多播服务。由于TCP要提供可靠的，面向连接的传输服务，这一难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多处理机资源。 5，硬件端口是不同硬件设备进行交互的接口，而软件端口是应用层各种协议进程与运输实体进行层间交互的一种地址。UDP和TCP的首部格式中都有源端口和目的端口这两个重要字段。当运输层收到IP层交上来的运输层报文时，就能够 根据其首部中的目的端口号把数据交付应用层的目的应用层。（两个进程之间进行通信不光要知道对方IP地址而且要知道对方的端口号(为了找到对方计算机中的应用进程)） 6，运输层用一个16位端口号标志一个端口。端口号只有本地意义，它只是为了标志计算机应用层中的各个进程在和运输层交互时的层间接口。在互联网的不同计算机中，相同的端口号是没有关联的。协议端口号简称端口。虽然通信的终点是应用进程，但只要把所发送的报文交到目的主机的某个合适端口，剩下的工作（最后交付目的进程）就由TCP和UDP来完成。 7，运输层的端口号分为服务器端使用的端口号（01023指派给熟知端口，102449151是登记端口号）和客户端暂时使用的端口号（49152~65535） 8，UDP的主要特点是①无连接②尽最大努力交付③面向报文④无拥塞控制⑤支持一对一，一对多，多对一和多对多的交互通信⑥首部开销小（只有四个字段：源端口，目的端口，长度和检验和） 9，TCP的主要特点是①面向连接②每一条TCP连接只能是一对一的③提供可靠交付④提供全双工通信⑤面向字节流 10，TCP用主机的IP地址加上主机上的端口号作为TCP连接的端点。这样的端点就叫做套接字（socket）或插口。套接字用（IP地址：端口号）来表示。每一条TCP连接唯一被通信两端的两个端点所确定。 11，停止等待协议是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 12，为了提高传输效率，发送方可以不使用低效率的停止等待协议，而是采用流水线传输。流水线传输就是发送方可连续发送多个分组，不必每发完一个分组就停下来等待对方确认。这样可使信道上一直有数据不间断的在传送。这种传输方式可以明显提高信道利用率。 13，停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重转时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为自动重传请求ARQ。另外在停止等待协议中若收到重复分组，就丢弃该分组，但同时还要发送确认。连续ARQ协议可提高信道利用率。发送维持一个发送窗口，凡位于发送窗口内的分组可连续发送出去，而不需要等待对方确认。接收方一般采用累积确认，对按序到达的最后一个分组发送确认，表明到这个分组位置的所有分组都已经正确收到了。 14，TCP报文段的前20个字节是固定的，后面有4n字节是根据需要增加的选项。因此，TCP首部的最小长度是20字节。 15，TCP使用滑动窗口机制。发送窗口里面的序号表示允许发送的序号。发送窗口后沿的后面部分表示已发送且已收到确认，而发送窗口前沿的前面部分表示不晕与发送。发送窗口后沿的变化情况有两种可能，即不动（没有收到新的确认）和前移（收到了新的确认）。发送窗口的前沿通常是不断向前移动的。一般来说，我们总是希望数据传输更快一些。但如果发送方把数据发送的过快，接收方就可能来不及接收，这就会造成数据的丢失。所谓流量控制就是让发送方的发送速率不要太快，要让接收方来得及接收。 16，在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫拥塞。拥塞控制就是为了防止过多的数据注入到网络中，这样就可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机，所有的路由器，以及与降低网络传输性能有关的所有因素。相反，流量控制往往是点对点通信量的控制，是个端到端的问题。流量控制所要做到的就是抑制发送端发送数据的速率，以便使接收端来得及接收。 17，为了进行拥塞控制，TCP发送方要维持一个拥塞窗口cwnd的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。 18，TCP的拥塞控制采用了四种算法，即慢开始，拥塞避免，快重传和快恢复。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理AQM），以减少网络拥塞的发生。 19，运输连接的三个阶段，即：连接建立，数据传送和连接释放。 20，主动发起TCP连接建立的应用进程叫做客户，而被动等待连接建立的应用进程叫做服务器。TCP连接采用三报文握手机制。服务器要确认用户的连接请求，然后客户要对服务器的确认进行确认。 21，TCP的连接释放采用四报文握手机制。任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送时，则发送连接释放通知，对方确认后就完全关闭了TCP连接 （3），最重要的知识点① 端口和套接字的意义② 无连接UDP的特点③ 面向连接TCP的特点④ 在不可靠的网络上实现可靠传输的工作原理，停止等待协议和ARQ协议① TCP的滑动窗口，流量控制，拥塞控制和连接管理六应用层（1），基本术语 域名系统（DNS）：DNS（Domain Name System，域名系统），万维网上作为域名和IP地址相互映射的一个分布式数据库，能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。 通过域名，最终得到该域名对应的IP地址的过程叫做域名解析（或主机名解析）。DNS协议运行在UDP协议之上，使用端口号53。在RFC文档中RFC 2181对DNS有规范说明，RFC 2136对DNS的动态更新进行说明，RFC 2308对DNS查询的反向缓存进行说明。 文件传输协议（FTP）：FTP 是File TransferProtocol（文件传输协议）的英文简称，而中文简称为“文传协议”。用于Internet上的控制文件的双向传输。同时，它也是一个应用程序（Application）。 基于不同的操作系统有不同的FTP应用程序，而所有这些应用程序都遵守同一种协议以传输文件。在FTP的使用当中，用户经常遇到两个概念：&quot;下载&quot;（Download）和&quot;上传&quot;（Upload）。 &quot;下载&quot;文件就是从远程主机拷贝文件至自己的计算机上；&quot;上传&quot;文件就是将文件从自己的计算机中拷贝至远程主机上。用Internet语言来说，用户可通过客户机程序向（从）远程主机上传（下载）文件。 简单文件传输协议（TFTP）：TFTP（Trivial File Transfer Protocol,简单文件传输协议）是TCP/IP协议族中的一个用来在客户机与服务器之间进行简单文件传输的协议，提供不复杂、开销不大的文件传输服务。端口号为69。 远程终端协议（TELENET）：Telnet协议是TCP/IP协议族中的一员，是Internet远程登陆服务的标准协议和主要方式。它为用户提供了在本地计算机上完成远程主机工作的能力。 在终端使用者的电脑上使用telnet程序，用它连接到服务器。终端使用者可以在telnet程序中输入命令，这些命令会在服务器上运行，就像直接在服务器的控制台上输入一样。 可以在本地就能控制服务器。要开始一个telnet会话，必须输入用户名和密码来登录服务器。Telnet是常用的远程控制Web服务器的方法。 万维网（WWW）：WWW是环球信息网的缩写，（亦作“Web”、“WWW”、“&apos;W3&apos;”，英文全称为“World Wide Web”），中文名字为“万维网”，&quot;环球网&quot;等，常简称为Web。分为Web客户端和Web服务器程序。 WWW可以让Web客户端（常用浏览器）访问浏览Web服务器上的页面。是一个由许多互相链接的超文本组成的系统，通过互联网访问。在这个系统中，每个有用的事物，称为一样“资源”；并且由一个全局“统一资源标识符”（URI）标识；这些资源通过超文本传输协议（Hypertext Transfer Protocol）传送给用户，而后者通过点击链接来获得资源。 万维网联盟（英语：World Wide Web Consortium，简称W3C），又称W3C理事会。1994年10月在麻省理工学院（MIT）计算机科学实验室成立。万维网联盟的创建者是万维网的发明者蒂姆·伯纳斯-李。 万维网并不等同互联网，万维网只是互联网所能提供的服务其中之一，是靠着互联网运行的一项服务。 万维网的大致工作工程： 统一资源定位符（URL）：统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。 超文本传输协议（HTTP）：超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的WWW文件都必须遵守这个标准。 设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法。1960年美国人Ted Nelson构思了一种通过计算机处理文本信息的方法，并称之为超文本（hypertext）,这成为了HTTP超文本传输协议标准架构的发展根基。 代理服务器（Proxy Server）：代理服务器（Proxy Server）是一种网络实体，它又称为万维网高速缓存。 代理服务器把最近的一些请求和响应暂存在本地磁盘中。当新请求到达时，若代理服务器发现这个请求与暂时存放的的请求相同，就返回暂存的响应，而不需要按URL的地址再次去互联网访问该资源。 代理服务器可在客户端或服务器工作，也可以在中间系统工作。 http请求头：http请求头，HTTP客户程序（例如浏览器），向服务器发送请求的时候必须指明请求类型（一般是GET或者POST）。如有必要，客户程序还可以选择发送其他的请求头。 - Accept：浏览器可接受的MIME类型。 - Accept-Charset：浏览器可接受的字符集。 - Accept-Encoding：浏览器能够进行解码的数据编码方式，比如gzip。Servlet能够向支持gzip的浏览器返回经gzip编码的HTML页面。许多情形下这可以减少5到10倍的下载时间。 - Accept-Language：浏览器所希望的语言种类，当服务器能够提供一种以上的语言版本时要用到。 - Authorization：授权信息，通常出现在对服务器发送的WWW-Authenticate头的应答中。 - Connection：表示是否需要持久连接。如果Servlet看到这里的值为“Keep-Alive”，或者看到请求使用的是HTTP 1.1（HTTP 1.1默认进行持久连接），它就可以利用持久连接的优点，当页面包含多个元素时（例如Applet，图片），显著地减少下载所需要的时间。要实现这一点，Servlet需要在应答中发送一个Content-Length头，最简单的实现方法是：先把内容写入ByteArrayOutputStream，然后在正式写出内容之前计算它的大小。 - Content-Length：表示请求消息正文的长度。 - Cookie：这是最重要的请求头信息之一 - From：请求发送者的email地址，由一些特殊的Web客户程序使用，浏览器不会用到它。 - Host：初始URL中的主机和端口。 - If-Modified-Since：只有当所请求的内容在指定的日期之后又经过修改才返回它，否则返回304“Not Modified”应答。 - Pragma：指定“no-cache”值表示服务器必须返回一个刷新后的文档，即使它是代理服务器而且已经有了页面的本地拷贝。 - Referer：包含一个URL，用户从该URL代表的页面出发访问当前请求的页面。 - User-Agent：浏览器类型，如果Servlet返回的内容与浏览器类型有关则该值非常有用。简单邮件传输协议(SMTP)：SMTP（Simple Mail Transfer Protocol）即简单邮件传输协议,它是一组用于由源地址到目的地址传送邮件的规则，由它来控制信件的中转方式。 SMTP协议属于TCP/IP协议簇，它帮助每台计算机在发送或中转信件时找到下一个目的地。 通过SMTP协议所指定的服务器,就可以把E-mail寄到收信人的服务器上了，整个过程只要几分钟。SMTP服务器则是遵循SMTP协议的发送邮件服务器，用来发送或中转发出的电子邮件。搜索引擎：搜索引擎（Search Engine）是指根据一定的策略、运用特定的计算机程序从互联网上搜集信息，在对信息进行组织和处理后，为用户提供检索服务，将用户检索相关的信息展示给用户的系统。 搜索引擎包括全文索引、目录索引、元搜索引擎、垂直搜索引擎、集合式搜索引擎、门户搜索引擎与免费链接列表等。全文索引： 全文索引技术是目前搜索引擎的关键技术。 试想在1M大小的文件中搜索一个词，可能需要几秒，在100M的文件中可能需要几十秒，如果在更大的文件中搜索那么就需要更大的系统开销，这样的开销是不现实的。 所以在这样的矛盾下出现了全文索引技术，有时候有人叫倒排文档技术。目录索引：目录索引（ search index/directory)，顾名思义就是将网站分门别类地存放在相应的目录中，因此用户在查询信息时，可选择关键词搜索，也可按分类目录逐层查找。垂直搜索引擎：垂直搜索引擎是针对某一个行业的专业搜索引擎，是搜索引擎的细分和延伸，是对网页库中的某类专门的信息进行一次整合，定向分字段抽取出需要的数据进行处理后再以某种形式返回给用户。 垂直搜索是相对通用搜索引擎的信息量大、查询不准确、深度不够等提出来的新的搜索引擎服务模式，通过针对某一特定领域、某一特定人群或某一特定需求提供的有一定价值的信息和相关服务。 其特点就是“专、精、深”，且具有行业色彩，相比较通用搜索引擎的海量信息无序化，垂直搜索引擎则显得更加专注、具体和深入。（2），重要知识点总结1，文件传输协议（FTP）使用TCP可靠的运输服务。FTP使用客户服务器方式。一个FTP服务器进程可以同时为多个用户提供服务。在进进行文件传输时，FTP的客户和服务器之间要先建立两个并行的TCP连接:控制连接和数据连接。实际用于传输文件的是数据连接。 2，万维网客户程序与服务器之间进行交互使用的协议时超文本传输协议HTTP。HTTP使用TCP连接进行可靠传输。但HTTP本身是无连接、无状态的。HTTP/1.1协议使用了持续连接（分为非流水线方式和流水线方式） 3，电子邮件把邮件发送到收件人使用的邮件服务器，并放在其中的收件人邮箱中，收件人可随时上网到自己使用的邮件服务器读取，相当于电子邮箱。 4，一个电子邮件系统有三个重要组成构件：用户代理、邮件服务器、邮件协议（包括邮件发送协议，如SMTP，和邮件读取协议，如POP3和IMAP）。用户代理和邮件服务器都要运行这些协议。 （3），最重要知识点总结① 域名系统-从域名解析出IP地址② 访问一个网站大致的过程③ 系统调用和应用编程接口概念]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8新特性总结]]></title>
    <url>%2F2016%2F03%2F17%2Flang-java-What-s-New-in-JDK8-JDK8%E6%96%B0%E7%89%B9%E6%80%A7%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[总结了部分JDK8新特性，另外一些新特性可以通过Oracle的官方文档查看，毕竟是官方文档，各种新特性都会介绍，有兴趣的可以去看。 Oracle官方文档:What’s New in JDK8 Java语言特性 Lambda表达式是一个新的语言特性，已经在JDK8中加入。它是一个可以传递的代码块，你也可以把它们当做方法参数。Lambda表达式允许您更紧凑地创建单虚方法接口（称为功能接口）的实例。 方法引用为已经存在的具名方法提供易于阅读的Lambda表达式 默认方法允许将新功能添加到库的接口，并确保与为这些接口的旧版本编写的代码的二进制兼容性。 改进的类型推断。 方法参数反射(通过反射获得方法参数信息) 流(stream) 新java.util.stream包中的类提供Stream API以支持对元素流的功能样式操作。流(stream)和I/O里的流不是同一个概念 ，使用stream API可以更方便的操作集合。 国际化 待办 待办 Lambda表达式1.什么是Lambda表达式Lambda表达式实质上是一个可传递的代码块，Lambda又称为闭包或者匿名函数，是函数式编程语法，让方法可以像普通参数一样传递 2.Lambda表达式语法-> &#123;执行代码块&#125;```1&lt;br&gt;参数列表可以为空```()-&gt;&#123;&#125; 可以加类型声明比如para1, int para2) -> &#123;return para1 + para2;&#125;```我们可以看到，lambda同样可以有返回值.1&lt;br&gt;在编译器可以推断出类型的时候，可以将类型声明省略，比如```(para1, para2) -&gt; &#123;return para1 + para2;&#125; (lambda有点像动态类型语言语法。lambda在字节码层面是用invokedynamic实现的，而这条指令就是为了让JVM更好的支持运行在其上的动态类型语言) 3.函数式接口在了解Lambda表达式之前，有必要先了解什么是函数式接口1234567**函数式接口指的是有且只有一个抽象(abstract)方法的接口**&lt;br&gt;当需要一个函数式接口的对象时，就可以用Lambda表达式来实现，举个常用的例子:&lt;br&gt;```java Thread thread = new Thread(() -&gt; &#123; System.out.println(&quot;This is JDK8&apos;s Lambda!&quot;); &#125;); 这段代码和函数式接口有啥关系？我们回忆一下，Thread类的构造函数里是不是有一个以Runnable接口为参数的？ 123456789public Thread(Runnable target) &#123;...&#125;/** * Runnable Interface */@FunctionalInterfacepublic interface Runnable &#123; public abstract void run();&#125; 到这里大家可能已经明白了，Lambda表达式相当于一个匿名类或者说是一个匿名方法。上面Thread的例子相当于 123456Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("Anonymous class"); &#125;&#125;); 也就是说，上面的lambda表达式相当于实现了这个run()方法，然后当做参数传入(个人感觉可以这么理解,lambda表达式就是一个函数，只不过它的返回值、参数列表都由编译器帮我们推断，因此可以减少很多代码量)。Lambda也可以这样用 : 1Runnable runnable = () -&gt; &#123;...&#125;; 其实这和上面的用法没有什么本质上的区别。至此大家应该明白什么是函数式接口以及函数式接口和lambda表达式之间的关系了。在JDK8中修改了接口的规范，目的是为了在给接口添加新的功能时保持向前兼容(个人理解)，比如一个已经定义了的函数式接口，某天我们想给它添加新功能，那么就不能保持向前兼容了，因为在旧的接口规范下，添加新功能必定会破坏这个函数式接口(JDK8中接口规范)除了上面说的Runnable接口之外，JDK中已经存在了很多函数式接口比如(当然不止这些): 1- ```java.util.Comparator 1234567891011121314151617181920212223242526272829303132333435363738&lt;br&gt;**关于JDK中的预定义的函数式接口**- JDK在```java.util.function```下预定义了很多函数式接口 - ```Function&lt;T, R&gt; &#123;R apply(T t);&#125;``` 接受一个T对象，然后返回一个R对象，就像普通的函数。 - ```Consumer&lt;T&gt; &#123;void accept(T t);&#125;``` 消费者 接受一个T对象，没有返回值。 - ```Predicate&lt;T&gt; &#123;boolean test(T t);&#125;``` 判断，接受一个T对象，返回一个布尔值。 - ```Supplier&lt;T&gt; &#123;T get();&#125; 提供者(工厂)``` 返回一个T对象。 - 其他的跟上面的相似，大家可以看一下function包下的具体接口。### 4.变量作用域```javapublic class VaraibleHide &#123; @FunctionalInterface interface IInner &#123; void printInt(int x); &#125; public static void main(String[] args) &#123; int x = 20; IInner inner = new IInner() &#123; int x = 10; @Override public void printInt(int x) &#123; System.out.println(x); &#125; &#125;; inner.printInt(30); inner = (s) -&gt; &#123; //Variable used in lambda expression should be final or effectively final //!int x = 10; //!x= 50; error System.out.print(x); &#125;; inner.printInt(30); &#125;&#125;输出 :3020 对于lambda表达式inner 123456789101112的参数列表()获取的变量成为自由变量，它是被lambda表达式捕获的。&lt;br&gt;lambda表达式和内部类一样，对外部自由变量捕获时，外部自由变量必须为final或者是最终变量(effectively final)的，也就是说这个变量初始化后就不能为它赋新值，同时lambda不像内部类/匿名类，lambda表达式与外围嵌套块有着相同的作用域，因此对变量命名的有关规则对lambda同样适用。大家阅读上面的代码对这些概念应该不难理解。&lt;span id="MethodReferences"&gt;&lt;/span&gt;### 5.方法引用**只需要提供方法的名字，具体的调用过程由Lambda和函数式接口来确定，这样的方法调用成为方法引用。**&lt;br&gt;下面的例子会打印list中的每个元素:```javaList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; ++i) &#123; list.add(i); &#125; list.forEach(System.out::println); 其中```(para)->&#123;System.out.println(para);&#125;```12345&lt;br&gt;我们看一下List#forEach方法 ```default void forEach(Consumer&lt;? super T&gt; action)```可以看到它的参数是一个Consumer接口，该接口是一个函数式接口```java@FunctionalInterfacepublic interface Consumer&lt;T&gt; &#123; void accept(T t); 大家能发现这个函数接口的方法和1234567891011121314151617&lt;br&gt;我们自己定义一个方法，看看能不能像标准输出的打印函数一样被调用```javapublic class MethodReference &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; ++i) &#123; list.add(i); &#125; list.forEach(MethodReference::myPrint); &#125; static void myPrint(int i) &#123; System.out.print(i + &quot;, &quot;); &#125;&#125;输出: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 可以看到，我们自己定义的方法也可以当做方法引用。到这里大家多少对方法引用有了一定的了解，我们再来说一下方法引用的形式。 方法引用 类名::静态方法名 类名::实例方法名 类名::new (构造方法引用) 实例名::实例方法名可以看出，方法引用是通过(方法归属名)::(方法名)来调用的。通过上面的例子已经讲解了一个类名::静态方法名的使用方法了，下面再依次介绍其余的几种方法引用的使用方法。 类名::实例方法名先来看一段代码 12String[] strings = new String[10];Arrays.sort(strings, String::compareToIgnoreCase); 上面的String::compareToIgnoreCase等价于(x, y) -&gt; {return x.compareToIgnoreCase(y);}我们看一下Arrays#sort方法public static &lt;T&gt; void sort(T[] a, Comparator&lt;? super T&gt; c),可以看到第二个参数是一个Comparator接口，该接口也是一个函数式接口，其中的抽象方法是int compare(T o1, T o2);，再看一下String#compareToIgnoreCase方法,public int compareToIgnoreCase(String str)，这个方法好像和上面讲方法引用中类名::静态方法名不大一样啊，它的参数列表和函数式接口的参数列表不一样啊，虽然它的返回值一样？是的，确实不一样但是别忘了，String类的这个方法是个实例方法，而不是静态方法，也就是说，这个方法是需要有一个接收者的。所谓接收者就是instance.method(x)中的instance，它是某个类的实例，有的朋友可能已经明白了。上面函数式接口的compare(T o1, T o2)中的第一个参数作为了实例方法的接收者，而第二个参数作为了实例方法的参数。我们再举一个自己实现的例子: 12345678910public class MethodReference &#123; static Random random = new Random(47); public static void main(String[] args) &#123; MethodReference[] methodReferences = new MethodReference[10]; Arrays.sort(methodReferences, MethodReference::myCompare); &#125; int myCompare(MethodReference o) &#123; return random.nextInt(2) - 1; &#125;&#125; 上面的例子可以在IDE里通过编译，大家有兴趣的可以模仿上面的例子自己写一个程序，打印出排序后的结果。构造器引用构造器引用仍然需要与特定的函数式接口配合使用，并不能像下面这样直接使用。IDE会提示String不是一个函数式接口 12//compile error : String is not a functional interfaceString str = String::new; 下面是一个使用构造器引用的例子，可以看出构造器引用可以和这种工厂型的函数式接口一起使用的。 12345678910111213141516 interface IFunctional&lt;T&gt; &#123; T func();&#125;public class ConstructorReference &#123; public ConstructorReference() &#123; &#125; public static void main(String[] args) &#123; Supplier&lt;ConstructorReference&gt; supplier0 = () -&gt; new ConstructorReference(); Supplier&lt;ConstructorReference&gt; supplier1 = ConstructorReference::new; IFunctional&lt;ConstructorReference&gt; functional = () -&gt; new ConstructorReference(); IFunctional&lt;ConstructorReference&gt; functional1 = ConstructorReference::new; &#125;&#125; 下面是一个JDK官方的例子 12345678910111213141516public static &lt;T, SOURCE extends Collection&lt;T&gt;, DEST extends Collection&lt;T&gt;&gt; DEST transferElements( SOURCE sourceCollection, Supplier&lt;DEST&gt; collectionFactory) &#123; DEST result = collectionFactory.get(); for (T t : sourceCollection) &#123; result.add(t); &#125; return result; &#125; ... Set&lt;Person&gt; rosterSet = transferElements( roster, HashSet::new); 实例::实例方法其实开始那个例子就是一个实例::实例方法的引用 12345List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; ++i) &#123; list.add(i); &#125; list.forEach(System.out::println); 其中System.out就是一个实例，println是一个实例方法。相信不用再给大家做解释了。 总结Lambda表达式是JDK8引入Java的函数式编程语法，使用Lambda需要直接或者间接的与函数式接口配合，在开发中使用Lambda可以减少代码量，但是并不是说必须要使用Lambda(虽然它是一个很酷的东西)。有些情况下使用Lambda会使代码的可读性急剧下降，并且也节省不了多少代码，所以在实际开发中还是需要仔细斟酌是否要使用Lambda。和Lambda相似的还有JDK10中加入的var类型推断，同样对于这个特性需要斟酌使用。 JDK8接口规范在JDK8中引入了lambda表达式，出现了函数式接口的概念，为了在扩展接口时保持向前兼容性(JDK8之前扩展接口会使得实现了该接口的类必须实现添加的方法，否则会报错。为了保持兼容性而做出妥协的特性还有泛型，泛型也是为了保持兼容性而失去了在一些别的语言泛型拥有的功能)，Java接口规范发生了一些改变。1.JDK8以前的接口规范 JDK8以前接口可以定义的变量和方法 所有变量(Field)不论是否显式 的声明为static final```，它实际上都是```public static final```的。123456 - 所有方法(Method)不论是否&lt;i&gt;显示&lt;/i&gt; 的声明为```public abstract```，它实际上都是```public abstract```的。```javapublic interface AInterfaceBeforeJDK8 &#123; int FIELD = 0; void simpleMethod();&#125; 以上接口信息反编译以后可以看到字节码信息里Filed是public static final的，而方法是public abstract的，即是你没有显示的去声明它。 12345678910&#123; public static final int FIELD; descriptor: I flags: (0x0019) ACC_PUBLIC, ACC_STATIC, ACC_FINAL ConstantValue: int 0 public abstract void simpleMethod(); descriptor: ()V flags: (0x0401) ACC_PUBLIC, ACC_ABSTRACT&#125; 2.JDK8之后的接口规范 JDK8之后接口可以定义的变量和方法 变量(Field)仍然必须是 public static final```的1234567891011121314151617 - 方法(Method)除了可以是public abstract之外，还可以是public static或者是default(相当于仅public修饰的实例方法)的。从以上改变不难看出，修改接口的规范主要是为了能在扩展接口时保持向前兼容。&lt;br&gt;下面是一个JDK8之后的接口例子```javapublic interface AInterfaceInJDK8 &#123; int simpleFiled = 0; static int staticField = 1; public static void main(String[] args) &#123; &#125; static void staticMethod()&#123;&#125; default void defaultMethod()&#123;&#125; void simpleMethod() throws IOException;&#125; 进行反编译(去除了一些没用信息) 123456789101112131415161718192021&#123; public static final int simpleFiled; flags: (0x0019) ACC_PUBLIC, ACC_STATIC, ACC_FINAL public static final int staticField; flags: (0x0019) ACC_PUBLIC, ACC_STATIC, ACC_FINAL public static void main(java.lang.String[]); flags: (0x0009) ACC_PUBLIC, ACC_STATIC public static void staticMethod(); flags: (0x0009) ACC_PUBLIC, ACC_STATIC public void defaultMethod(); flags: (0x0001) ACC_PUBLIC public abstract void simpleMethod() throws java.io.IOException; flags: (0x0401) ACC_PUBLIC, ACC_ABSTRACT Exceptions: throws java.io.IOException&#125; 可以看到 default关键字修饰的方法是像实例方法(就是普通类中定义的普通方法)一样定义的，所以我们来定义一个只有default方法的接口并且实现一下这个接口试一试。 1234567891011121314interface Default &#123; default int defaultMethod() &#123; return 4396; &#125;&#125;public class DefaultMethod implements Default &#123; public static void main(String[] args) &#123; DefaultMethod defaultMethod = new DefaultMethod(); System.out.println(defaultMethod.defaultMethod()); //compile error : Non-static method 'defaultMethod()' cannot be referenced from a static context //! DefaultMethod.defaultMethod(); &#125;&#125; 可以看到default方法确实像实例方法一样，必须有实例对象才能调用，并且子类在实现接口时，可以不用实现default方法，也可以选择覆盖该方法。这有点像子类继承父类实例方法。接口静态方法就像是类静态方法，唯一的区别是接口静态方法只能通过接口名调用，而类静态方法既可以通过类名调用也可以通过实例调用 123456789interface Static &#123; static int staticMethod() &#123; return 4396; &#125;&#125; ... main(String...args) //!compile error: Static method may be invoked on containing interface class only //!aInstanceOfStatic.staticMethod(); ... 另一个问题是多继承问题，大家知道Java中类是不支持多继承的，但是接口是多继承和多实现(implements后跟多个接口)的，那么如果一个接口继承另一个接口，两个接口都有同名的default方法会怎么样呢？答案是会像类继承一样覆写(@Override)，以下代码在IDE中可以顺利编译 12345678910111213141516171819interface Default &#123; default int defaultMethod() &#123; return 4396; &#125;&#125;interface Default2 extends Default &#123; @Override default int defaultMethod() &#123; return 9527; &#125;&#125;public class DefaultMethod implements Default,Default2 &#123; public static void main(String[] args) &#123; DefaultMethod defaultMethod = new DefaultMethod(); System.out.println(defaultMethod.defaultMethod()); &#125;&#125;输出 : 9527 出现上面的情况时，会优先找继承树上近的方法，类似于“短路优先”。那么如果一个类实现了两个没有继承关系的接口，且这两个接口有同名方法的话会怎么样呢？IDE会要求你重写这个冲突的方法，让你自己选择去执行哪个方法，因为IDE它还没智能到你不告诉它，它就知道你想执行哪个方法。可以通过接口名.super```指针来访问接口中定义的实例(default)方法。12345678910111213141516171819202122232425262728293031```javainterface Default &#123; default int defaultMethod() &#123; return 4396; &#125;&#125;interface Default2 &#123; default int defaultMethod() &#123; return 9527; &#125;&#125;//如果不重写//compile error : defaults.DefaultMethod inherits unrelated defaults for defaultMethod() from types defaults.Default and defaults.Default2public class DefaultMethod implements Default,Default2 &#123;@Override public int defaultMethod() &#123; System.out.println(Default.super.defaultMethod()); System.out.println(Default2.super.defaultMethod()); return 996; &#125; public static void main(String[] args) &#123; DefaultMethod defaultMethod = new DefaultMethod(); System.out.println(defaultMethod.defaultMethod()); &#125;&#125;运行输出 : 43969527996 改进的类型推断1.什么是类型推断类型推断就像它的字面意思一样，编译器根据你显示声明的已知的信息 推断出你没有显示声明的类型，这就是类型推断。看过《Java编程思想 第四版》的朋友可能还记得里面讲解泛型一章的时候，里面很多例子是下面这样的: 1Map&lt;String, Object&gt; map = new Map&lt;String, Object&gt;(); 而我们平常写的都是这样的: 1Map&lt;String, Object&gt; map = new Map&lt;&gt;(); 这就是类型推断，《Java编程思想 第四版》这本书出书的时候最新的JDK只有1.6(JDK7推出的类型推断)，在Java编程思想里Bruce Eckel大叔还提到过这个问题(可能JDK的官方人员看了Bruce Eckel大叔的Thinking in Java才加的类型推断，☺)，在JDK7中推出了上面这样的类型推断，可以减少一些无用的代码。(Java编程思想到现在还只有第四版，是不是因为Bruce Eckel大叔觉得Java新推出的语言特性“然并卵”呢？/滑稽)在JDK7中，类型推断只有上面例子的那样的能力，即只有在使用赋值语句时才能自动推断出泛型参数信息(即&lt;&gt;里的信息)，下面的官方文档里的例子在JDK7里会编译错误 1234List&lt;String&gt; stringList = new ArrayList&lt;&gt;();stringList.add("A");//error : addAll(java.util.Collection&lt;? extends java.lang.String&gt;)in List cannot be applied to (java.util.List&lt;java.lang.Object&gt;)stringList.addAll(Arrays.asList()); 但是上面的代码在JDK8里可以通过，也就说，JDK8里，类型推断不仅可以用于赋值语句，而且可以根据代码中上下文里的信息推断出更多的信息，因此我们需要些的代码会更少。加强的类型推断还有一个就是用于Lambda表达式了。大家其实不必细究类型推断，在日常使用中IDE会自动判断，当IDE自己无法推断出足够的信息时，就需要我们额外做一下工作，比如在&lt;&gt;里添加更多的类型信息，相信随着Java的进化，这些便利的功能会越来越强大。 通过反射获得方法的参数信息JDK8之前 .class文件是不会存储方法参数信息的，因此也就无法通过反射获取该信息(想想反射获取类信息的入口是什么？当然就是Class类了)。即是是在JDK11里也不会默认生成这些信息，可以通过在javac加上-parameters参数来让javac生成这些信息(javac就是java编译器，可以把java文件编译成.class文件)。生成额外的信息(运行时非必须信息)会消耗内存并且有可能公布敏感信息(某些方法参数比如password，JDK文档里这么说的)，并且确实很多信息javac并不会为我们生成，比如LocalVariableTable，javac就不会默认生成，需要你加上 -g:vars来强制让编译器生成，同样的，方法参数信息也需要加上-parameters来让javac为你在.class文件中生成这些信息，否则运行时反射是无法获取到这些信息的。在讲解Java语言层面的方法之前，先看一下javac加上该参数和不加生成的信息有什么区别(不感兴趣想直接看运行代码的可以跳过这段)。下面是随便写的一个类。 12345public class ByteCodeParameters &#123; public String simpleMethod(String canUGetMyName, Object yesICan) &#123; return "9527"; &#125;&#125; 先来不加参数编译和反编译一下这个类javac ByteCodeParameters.java , javap -v ByteCodeParameters: 1234567891011//只截取了部分信息public java.lang.String simpleMethod(java.lang.String, java.lang.Object); descriptor: (Ljava/lang/String;Ljava/lang/Object;)Ljava/lang/String; flags: (0x0001) ACC_PUBLIC Code: stack=1, locals=3, args_size=3 0: ldc #2 // String 9527 2: areturn LineNumberTable: line 5: 0//这个方法的描述到这里就结束了 接下来我们加上参数javac -parameters ByteCodeParameters.java 再来看反编译的信息: 12345678910111213public java.lang.String simpleMethod(java.lang.String, java.lang.Object); descriptor: (Ljava/lang/String;Ljava/lang/Object;)Ljava/lang/String; flags: (0x0001) ACC_PUBLIC Code: stack=1, locals=3, args_size=3 0: ldc #2 // String 9527 2: areturn LineNumberTable: line 8: 0 MethodParameters: Name Flags canUGetMyName yesICan 可以看到.class文件里多了一个MethodParameters信息，这就是参数的名字，可以看到默认是不保存的。下面看一下在Intelj Idea里运行的这个例子，我们试一下通过反射获取方法名 : 1234567891011121314151617public class ByteCodeParameters &#123; public String simpleMethod(String canUGetMyName, Object yesICan) &#123; return "9527"; &#125; public static void main(String[] args) throws NoSuchMethodException &#123; Class&lt;?&gt; clazz = ByteCodeParameters.class; Method simple = clazz.getDeclaredMethod("simpleMethod", String.class, Object.class); Parameter[] parameters = simple.getParameters(); for (Parameter p : parameters) &#123; System.out.println(p.getName()); &#125; &#125;&#125;输出 :arg0arg1 ？？？说好的方法名呢？？？？别急，哈哈。前面说了，默认是不生成参数名信息的，因此我们需要做一些配置，我们找到IDEA的settings里的Java Compiler选项，在Additional command line parameters:一行加上-parameters(Eclipse 也是找到Java Compiler选中Stoer information about method parameters)，或者自己编译一个.class文件放在IDEA的out下，然后再来运行 : 123输出 :canUGetMyNameyesICan 这样我们就通过反射获取到参数信息了。想要了解更多的同学可以自己研究一下 [官方文档](https://docs.oracle.com/javase/tutorial/reflect/member/methodparameterreflection.html) 总结与补充在JDK8之后，可以通过-parameters参数来让编译器生成参数信息然后在运行时通过反射获取方法参数信息，其实在SpringFramework里面也有一个LocalVariableTableParameterNameDiscoverer对象可以获取方法参数名信息，有兴趣的同学可以自行百度(这个类在打印日志时可能会比较有用吧，个人感觉)。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos常用命令]]></title>
    <url>%2F2015%2F08%2F26%2Fos-linux-CentOS%2F</url>
    <content type="text"><![CDATA[登录username:rootpassword:安装时设置的密码 其它终端登录 $ ssh root@192.168.0.23 系统配置情况命令系统12345678910cat /etc/redhat-release # CentOS 查看系统信息uname -a # 查看内核/操作系统/CPU信息head -n 1 /etc/issue # 查看操作系统版本cat /proc/cpuinfo # 查看CPU信息hostname # 查看计算机名lspci -tv # 列出所有PCI设备lsusb -tv # 列出所有USB设备lsmod # 列出加载的内核模块env # 查看环境变量dmidecode | grep "Product Nmae" #查看服务器型号 资源1234567891011121314free -m # 查看内存使用量和交换区使用量df -h # 查看各分区使用情况du -sh &lt;目录名&gt; # 查看指定目录的大小grep MemTotal /proc/meminfo # 查看内存总量grep MemFree /proc/meminfo # 查看空闲内存量uptime # 查看系统运行时间、用户数、负载cat /proc/loadavg # 查看系统负载# 查看内存的插槽数，已经使用多少插槽。每条内存多大，已使用内存多大dmidecode|grep -P -A5 "Memory\s+Device"|grep Size|grep -v Range # 查看内存支持的最大内存容量dmidecode|grep -P 'Maximum\s+Capacity'# 查看内存的频率dmidecode|grep -A16 "Memory Device"dmidecode|grep -A16 "Memory Device"|grep 'Speed' 磁盘和分区12345mount | column -t # 查看挂接的分区状态fdisk -l # 查看所有分区swapon -s # 查看所有交换分区hdparm -i /dev/hda # 查看磁盘参数(仅适用于IDE设备)dmesg | grep IDE # 查看启动时IDE设备检测状况 网络123456ifconfig # 查看所有网络接口的属性iptables -L # 查看防火墙设置route -n # 查看路由表netstat -lntp # 查看所有监听端口netstat -antp # 查看所有已经建立的连接netstat -s # 查看网络统计信息 进程查看12ps -ef # 查看所有进程top # 实时显示进程状态 系统时间UTC: 整个地球分为二十四时区，每个时区都有自己的本地时间。在国际无线电通信场合，为了统一起见，使用一个统一的时间，称为通用协调时(UTC, Universal Time Coordinated)。GMT: 格林威治标准时间 (Greenwich Mean Time)指位于英国伦敦郊区的皇家格林尼治天文台的标准时间，因为本初子午线被定义在通过那里的经线。(UTC与GMT时间基本相同)CST: 中国标准时间 (China Standard Time)。GMT + 8 = UTC + 8 = CSTDST: 夏令时(Daylight Saving Time) 指在夏天太阳升起的比较早时，将时钟拨快一小时，以提早日光的使用。(中国不使用)硬件时钟: RTC(Real-Time Clock)或CMOS时钟，一般在主板上靠电池供电，服务器断电后也会继续运行。仅保存日期时间数值，无法保存时区和夏令时设置。系统时钟: 一般在服务器启动时复制RTC时间，之后独立运行，保存了时间、时区和夏令时设置。 12345678910111213timedatectl # 等同于 timedatectl statustimedatectl set-time "YYYY-MM-DD HH:MM:SS" # 设置时间timedatectl list-timezones # 列出所有时区timedatectl set-timezone Asia/Shanghai # 设置时区timedatectl set-ntp yes # 是否NTP服务器同步, yes或者no# 将硬件时钟调整为与本地时钟一致timedatectl set-local-rtc 1hwclock --systohc --localtime # 与上面命令效果一致 # 硬件时间设置成 UTCtimedatectl set-local-rtc 1hwclock --systohc --utc //与上面命令效果一致 用户123456w # 查看活动用户id &lt;用户名&gt; # 查看指定用户信息last # 查看用户登录日志cut -d: -f1 /etc/passwd # 查看系统所有用户cut -d: -f1 /etc/group # 查看系统所有组crontab -l # 查看当前用户的计划任务 常用命令123456789101112131415161718192021222324252627282930313233343536373839404142# 查看网络接口统计数据的，两种发放 ip link # 或者下面方法ip -s link yum install net-tools # net-tools包提供了ifconfig命令 ifconfig -a # 查看IP地址 ip addr # 查看IP地址 route -n # 使用最快的速度查找主机的路由 cat /proc/version # 查看系统信息 uname -a # 方法二 uname -r #方法三 getconf LONG_BIT # 查看系统是64位还是32位 uname -a # 查看内核/操作系统/CPU信息 head -n 1 /etc/issue #查看操作系统版本 cat /proc/cpuinfo #查看CPU信息 hostname #查看计算机名 lspci -tv #列出所有PCI设备 lsusb -tv #列出所有USB设备 lsmod #列出加载的内核模块 env #查看环境变量 arch # 显示机器的处理器架构(1) uname -m # 显示机器的处理器架构(2) uname -r # 显示正在使用的内核版本 dmidecode -q # 显示硬件系统部件 hdparm -i /dev/hda # 罗列一个磁盘的架构特性 hdparm -tT /dev/sda # 在磁盘上执行测试性读取操作 cat /proc/interrupts # 显示中断 cat /proc/meminfo # 校验内存使用 cat /proc/swaps # 显示哪些swap被使用 cat /proc/version # 显示内核的版本 cat /proc/net/dev # 显示网络适配器及统计 cat /proc/mounts # 显示已加载的文件系统 lspci -tv # 罗列 PCI 设备 lsusb -tv # 显示 USB 设备 date # 显示系统日期 date 041217002007.00 # 设置日期和时间 – 月日时分年.秒 cal 2007 # 显示2007年的日历表 clock -w # 将时间修改保存到 BIOS 系统的关机、重启以及登出12345678shutdown -h now # 关闭系统(1) init 0 # 关闭系统(2) telinit 0 # 关闭系统(3) shutdown -h hours:minutes &amp; #按预定时间关闭系统 shutdown -c #取消按预定时间关闭系统 shutdown -r now # 重启 (1) reboot #重启 (2) logout # 注销 查看网络配置的命令123456ifconfig # 查看所有网络接口的属性 iptables -L # 查看防火墙设置 route -n # 查看路由表 netstat -lntp # 查看所有监听端口 netstat -antp # 查看所有已经建立的连接 netstat -s # 查看网络统计信息 查看linux进程123ps -aux | grep node # 查看`node`进程 ps -ef # 查看所有进程 top # 实时显示进程状态 杀进程123killall -9 websocket # 干掉`websocket`服务进程 ps aux | grep mysql # 查看mysql进程 kill -9 35562 # 根据进程号杀 查看用户的命令123456w # 查看活动用户 id &lt;用户名&gt; # 查看指定用户信息 last # 查看用户登录日志 cut -d: -f1 /etc/passwd # 查看系统所有用户 cut -d: -f1 /etc/group # 查看系统所有组 crontab -l # 查看当前用户的计划任务 log日志查看1234cat /var/log/messages # 查询日志的全部内容head -5 /var/log/messages # 查询日志的前5行tail -5 /var/log/messages # 查询日志的最新5行sed -n '5,10p' /var/log/messages # 查询日志的5到10行 查看系统服务的命令12chkconfig –list # 列出所有系统服务 chkconfig –list | grep on # 列出所有启动的系统服务 安装程序的命令rpm -qa 查看所有安装的软件包 获取帮助的命令man &lt;命令&gt; 获得命令帮助 安装软件方法12345678910# 安装下载工具wget$ yum install wget# 解压$ wget https://nodejs.org/dist/v4.4.4/node-v4.4.4-linux-x64.tar.xz https://nodejs.org/dist/v4.4.5/node-v4.4.5-linux-x64.tar.xz# 测试安装# 没有用到`gzip`压缩去掉`z`参数$ sudo tar --strip-components 1 -xzvf node-v* -C /usr/local yum错误yum错误：Cannot retrieve repository metadata (repomd.xml) for repository解决方法 12cd /etc/yum.repos.d/ls 找到yum.repos.d这个目录，里面有个文件，以.repo 结尾的，例如zl.repo删除然后#yum clean all 安装源http://dl.fedoraproject.org/pub/http://rpms.remirepo.net/enterprise/ 下载www.centos.org http://mirror.neu.edu.cn/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.isohttp://centos.ustc.edu.cn/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.isohttp://mirrors.163.com/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.isohttp://mirrors.hust.edu.cn/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.isohttp://mirrors.zju.edu.cn/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.isohttp://mirrors.cqu.edu.cn/CentOS/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.isohttp://mirrors.cug.edu.cn/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.isohttp://mirrors.neusoft.edu.cn/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.isohttp://mirrors.skyshe.cn/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.isohttp://mirrors.aliyun.com/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.isohttp://mirrors.nwsuaf.edu.cn/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.isohttp://mirror.bit.edu.cn/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.isohttp://mirror.lzu.edu.cn/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.isohttp://ftp.sjtu.edu.cn/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.isohttp://mirrors.yun-idc.com/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.isohttp://mirrors.pubyun.com/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.iso]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7安装配置vsftp搭建FTP]]></title>
    <url>%2F2015%2F08%2F26%2Fos-linux-CentOS7%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEvsftp%E6%90%AD%E5%BB%BAFTP%2F</url>
    <content type="text"><![CDATA[安装配置vsftpd做FTP服务，我们的项目应用使用git管理进行迭代，公共文件软件存储使用开源网盘Seafile来管理，基本够用。想不到FTP的使用的场景，感觉它好像老去了，虽然现在基本没有用到这个工具，刚好公司公司刷一个硬件需要使用FTP来配置下载文件，于是研究使用了一下，记录了一下使用过程。😀 安装在安装前查看是否已安装vsftpd 12345678910# 查看是否已安装 方法一[root@localhost ~]# rpm -q vsftpdvsftpd-3.0.2-21.el7.x86_64# 查看是否已安装 方法二[root@localhost ~]# vsftpd -vvsftpd: version 3.0.2# 安装 vsftpd[root@localhost ~]# yum -y install vsftpd 查看位置12[root@localhost ~]# whereis vsftpdvsftpd: /usr/sbin/vsftpd /etc/vsftpd /usr/share/man/man8/vsftpd.8.gz 启动vsftpd服务1systemctl start vsftpd.service 关闭firewall和SELinux12345678910111213setenforce 0 # 设置SELinux 成为permissive模式 （关闭SELinux）setenforce 1 # 设置SELinux 成为enforcing模式 （开启SELinux）# 或者修改配置vi /etc/selinux/config# SELINUX=enforcing# 注释掉# SELINUXTYPE=targeted# 注释掉SELINUX=disabled# 增加:wq! #保存退出setenforce 0 或者设置SELinux 123getsebool -a | grep ftpsetsebool -P ftpd_full_access on 1234systemctl stop firewalld.service#停止firewallsystemctl disable firewalld.service#禁止firewall开机启动 如果你不愿意关闭防火墙，需要防火墙添加FTP服务。 12firewall-cmd --permanent --zone=public --add-service=ftpfirewall-cmd --reload 修改配置文件配置文件/etc/vsftpd/vsftpd.conf 123456789anonymous_enable=NO # 不允许匿名访问，禁用匿名登录chroot_local_user=YES # 启用限定用户在其主目录下use_localtime=YES # 使用本地时(自行添加)chroot_list_enable=YESlocal_enable=YES # 允许使用本地帐户进行FTP用户登录验证allow_writeable_chroot=YES # 如果启用了限定用户在其主目录下需要添加这个配置，解决报错 500 OOPS: vsftpd: refusing to run with writable root inside chroot()xferlog_enable=YES # 启用上传和下载的日志功能，默认开启。local_umask=022 # 设置本地用户默认文件掩码022# FTP上本地的文件权限，默认是077，不过vsftpd安装后的配置文件里默认是022 虚拟用户高级参数 12345678910111213当virtual_use_local_privs=YES 时，虚拟用户和本地用户有相同的权限；当virtual_use_local_privs=NO 时，虚拟用户和匿名用户有相同的权限，默认是NO。当virtual_use_local_privs=YES，write_enable=YES时，虚拟用户具有写权限（上传、下载、删除、重命名）。当virtual_use_local_privs=NO，write_enable=YES，anon_world_readable_only=YES，anon_upload_enable=YES时，虚拟用户不能浏览目录，只能上传文件，无其他权限。当virtual_use_local_privs=NO，write_enable=YES，anon_world_readable_only=NO，anon_upload_enable=NO时，虚拟用户只能下载文件，无其他权限。当virtual_use_local_privs=NO，write_enable=YES，anon_world_readable_only=NO，anon_upload_enable=YES时，虚拟用户只能上传和下载文件，无其他权限。当virtual_use_local_privs=NO，write_enable=YES，anon_world_readable_only=NO，anon_mkdir_write_enable=YES时，虚拟用户只能下载文件和创建文件夹，无其他权限。当virtual_use_local_privs=NO，write_enable=YES，anon_world_readable_only=NO，anon_other_write_enable=YES时，虚拟用户只能下载、删除和重命名文件，无其他权限。 匿名登录安装完默认情况下是开启匿名登录的，对应的是 /var/ftp 目录，这时只要服务启动了，就可以直接连上FTP了。默认用户名是ftp，密码是空的。如果你在配置里面配置了anonymous_enable=NO，匿名就无法登录。 12345678910111213141516$ ftp 192.168.188.114Connected to 192.168.188.114.220 (vsFTPd 3.0.2)Name (192.168.188.114:kennywang): ftp331 Please specify the password.Password:230 Login successful.Remote system type is UNIX.Using binary mode to transfer files.ftp&gt; ls229 Entering Extended Passive Mode (|||47867|).150 Here comes the directory listing.-rw-r--r-- 1 0 0 12 Jan 18 06:31 README.mddrwxr-xr-x 2 0 0 6 Nov 05 19:43 pub226 Directory send OK. 多用户配置多用户配置需要自己手工添加配置，下面内容到vsftpd.conf末尾 123456789101112131415161718192021222324# # use_localtime=YES # 使用本地时(自行添加)listen_port=21chroot_local_user=YES # 启用限定用户在其主目录下idle_session_timeout=300data_connection_timeout=120 # 数据连接超时时间guest_enable=YES # 设定启用虚拟用户功能guest_username=ftpuser # 指定虚拟用户的宿主用户 ftpuser（就是我们后面会新建这个用户）# guest_username=www# 如果ftp目录是指向网站根目录，用来上传网站程序，# 可以指定虚拟用户的宿主用户为nginx运行账户www，可以避免很多权限设置问题 user_config_dir=/etc/vsftpd/vuser_conf # 虚拟用户配置文件目录virtual_use_local_privs=YES # NO时，虚拟用户和匿名用户有相同的权限，默认是NOpasv_min_port=10060 # 被动模式最小端口号10060pasv_max_port=10090 # 被动模式最大端口号10090accept_timeout=5connect_timeout=1 创建宿主用户新建系统用户ftpuser，用户目录为/home/vsftpd, 用户登录终端设为/bin/false(即使之不能登录系统) 12345678910111213141516# 方法一# 创建用户 ftpuser 指定 `/home/vsftpd` 目录useradd -g root -M -d /home/vsftpd -s /sbin/nologin ftpuser# 设置用户 ftpuser 的密码passwd ftpuser# 把 /home/vsftpd 的所有权给ftpuser.rootchown -R ftpuser.root /home/vsftpd# 方法二useradd ftpuser -d /home/vsftpd -s /bin/falsechown ftpuser:ftpuser /home/vsftpd -R # 如果虚拟用户的宿主用户为www，需要这样设置# www目录是你应用的目录chown www:www /home/www -R 删除用户 userdel ftpuser 建立虚拟用户文件12345678910touch /etc/vsftpd/vuser_passwd# 编辑虚拟用户名单文件：（# 第一行账号，第二行密码，注意：不能使用root做用户名，系统保留）vi /etc/vsftpd/vuser_passwd # 编辑内容，下面是 vuser_passwd 内容wcj123456hss123456#保存退出 生成虚拟用户数据文件12db_load -T -t hash -f /etc/vsftpd/vuser_passwd /etc/vsftpd/vuser_passwd.dbchmod 600 /etc/vsftpd/vuser_passwd.db 创建用户配置123mkdir /etc/vsftpd/vuser_conf # 建立虚拟用户个人vsftp的配置文件cd /etc/vsftpd/vuser_conf # 进入目录touch hss wcj # 这里创建两个虚拟用户配置文件 每一个文件配置文件都差不多，只是参数local_root不一样。 123456local_root=/home/vsftpd/hss # 用户 hss 配置目录，这个地方不一样write_enable=YES # 允许本地用户对FTP服务器文件具有写权限anon_world_readable_only=NOanon_upload_enable=YES # 允许匿名用户上传文件(须将全局的write_enable=YES,默认YES)anon_mkdir_write_enable=YES # 允许匿名用户创建目录anon_other_write_enable=YES # 允许匿名用户删除和重命名权限(自行添加) 创建用户目录每个用户目录文件夹是有root用户创建的，也就是上面local_root配置目录，其权限应设置为755。因为权限的问题在该文件夹内无法直接上传文件。而如果设置为777则无法访问，这是由于vsftpd的安全性设置。解决上传问题的方法是在local_root文件夹内新建一个upload的文件夹，权限设置为777，可将文件上传到该文件夹。 12345678910111213141516mkdir -p /home/vsftpd/hss # 每个用户对于一个目录，创建两个目录“hss”、“wcj”# 下面是目录结构/home/vsftpd ├── hss │ ├── filename.md │ └── upload └── wcj └── filename.md# 赋予其权限chmod -R 777 /var/vsftpd/hss/upload/# 在/var/ftp下新建一个目录来实现匿名用户上传mkdir /var/ftp/upload vsftpd中几种用户的区分： 本地用户：用户在FTP服务器拥有账号，且该账号为本地用户的账号，可以通过自己的账号和口令进行授权登录，登录目录为自己的home目录$HOME虚拟用户：用户在FTP服务器上拥有账号，但该账号只能用于文件传输服务。登录目录为某一特定的目录，通常可以上传和下载匿名用户：用户在FTP服务器上没有账号，登录目录为/var/ftp 最后重启vsftpd服务器1systemctl restart vsftpd.service 服务运维123systemctl restart vsftpd.service # 重启服务systemctl start vsftpd.service # 启动服务systemctl status vsftpd.service # 服务状态查看 FTP命令12345678910111213141516171819202122232425262728293031323334ftp&gt; ascii # 设定以ASCII方式传送文件(缺省值) ftp&gt; bell # 每完成一次文件传送,报警提示. ftp&gt; binary # 设定以二进制方式传送文件. ftp&gt; bye # 终止主机FTP进程,并退出FTP管理方式. ftp&gt; case # 当为ON时,用MGET命令拷贝的文件名到本地机器中,全部转换为小写字母. ftp&gt; cd # 同UNIX的CD命令. ftp&gt; cdup # 返回上一级目录. ftp&gt; chmod # 改变远端主机的文件权限. ftp&gt; close # 终止远端的FTP进程,返回到FTP命令状态, 所有的宏定义都被删除. ftp&gt; delete # 删除远端主机中的文件. ftp&gt; dir [remote-directory] [local-file] # 列出当前远端主机目录中的文件.如果有本地文件,就将结果写至本地文件. ftp&gt; get [remote-file] [local-file] # 从远端主机中传送至本地主机中. ftp&gt; help [command] # 输出命令的解释. ftp&gt; lcd # 改变当前本地主机的工作目录,如果缺省,就转到当前用户的HOME目录. ftp&gt; ls [remote-directory] [local-file] # 同DIR. ftp&gt; macdef # 定义宏命令. ftp&gt; mdelete [remote-files] # 删除一批文件. ftp&gt; mget [remote-files] # 从远端主机接收一批文件至本地主机. ftp&gt; mkdir directory-name # 在远端主机中建立目录. ftp&gt; mput local-files # 将本地主机中一批文件传送至远端主机. ftp&gt; open host [port] # 重新建立一个新的连接. ftp&gt; prompt # 交互提示模式. ftp&gt; put local-file [remote-file] # 将本地一个文件传送至远端主机中. ftp&gt; pwd # 列出当前远端主机目录. ftp&gt; quit # 同BYE. ftp&gt; recv remote-file [local-file] # 同GET. ftp&gt; rename [from] [to] # 改变远端主机中的文件名. ftp&gt; rmdir directory-name # 删除远端主机中的目录. ftp&gt; send local-file [remote-file] # 同PUT. ftp&gt; status # 显示当前FTP的状态. ftp&gt; system # 显示远端主机系统类型. ftp&gt; user user-name [password] [account] # 重新以别的用户名登录远端主机. ftp&gt; ? [command] # 同HELP. [command]指定需要帮助的命令名称。如果没有指定 command，ftp 将显示全部命令的列表。ftp&gt; ! # 从 ftp 子系统退出到外壳。 关闭FTP连接123byeexitquit 下载文件12ftp&gt; get readme.txt # 下载 readme.txt 文件ftp&gt; mget *.txt # 下载 上传文件12ftp&gt; put /path/readme.txt # 上传 readme.txt 文件ftp&gt; mput *.txt # 可以上传多个文件 状态码 230 - 登录成功 200 - 命令执行成功 150 - 文件状态正常，开启数据连接端口 250 - 目录切换操作完成 226 - 关闭数据连接端口，请求的文件操作成功 参考资料 Vsftpd虚拟用户的配置 CentOS7安装和配置FTP]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7系统更换软件安装源]]></title>
    <url>%2F2015%2F08%2F26%2Fos-linux-CentOS7%E6%9B%B4%E6%8D%A2yum%E8%BD%AF%E4%BB%B6%E9%95%9C%E5%83%8F%E6%BA%90%2F</url>
    <content type="text"><![CDATA[阿里云Linux安装镜像源地址：http://mirrors.aliyun.com/ 。 第一步：备份你的原镜像文件，以免出错后可以恢复。 123cp /etc/yum.repos.d/CentOS-Base.repo&#123;,.backup&#125;# 或者mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 第二步：下载新的CentOS-Base.repo 到/etc/yum.repos.d/ 如果 wget 没有安装，运行下面命令安装 wget 软件。 12yum update --skip-brokenyum -y install wget 安装完成更新下载源 123456# CentOS 5wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-5.repo# CentOS 6wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo# CentOS 7wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 第三步：运行yum makecache生成缓存 12yum clean allyum makecache yum 安装报错 1File "/usr/bin/yum", line 30 except KeyboardInterrupt, e: 解决：修改文件/usr/bin/yum、/usr/libexec/urlgrabber-ext-down头中相应python为#!/usr/bin/python2.7 163官方说明：http://mirrors.163.com/http://mirrors.163.com/.help/centos.html 中国开源镜像站点 阿里云开源镜像站：http://mirrors.aliyun.com/ 网易开源镜像站：http://mirrors.163.com/ 搜狐开源镜像站：http://mirrors.sohu.com/ 北京交通大学：http://mirror.bjtu.edu.cn/cn/ &lt;教育网荐&gt; 兰州大学：http://mirror.lzu.edu.cn/ &lt;西北高校FTP搜索引擎&gt; 厦门大学：http://mirrors.xmu.edu.cn/ 上海交通大学：http://ftp.sjtu.edu.cn/ 清华大学：http://mirrors.tuna.tsinghua.edu.cn/ http://mirrors.6.tuna.tsinghua.edu.cn/ http://mirrors.4.tuna.tsinghua.edu.cn/ 天津大学：http://mirror.tju.edu.cn/ 中国科学技术大学：http://mirrors.ustc.edu.cn/ http://mirrors4.ustc.edu.cn/ &lt;教育网、电信&gt; http://mirrors6.ustc.edu.cn/ 西南大学：http://linux.swu.edu.cn/swudownload/ 泰安移动：http://mirrors.ta139.com/ 东北大学：http://mirror.neu.edu.cn/ 浙江大学：http://mirrors.zju.edu.cn/ 东软信息学院：http://mirrors.neusoft.edu.cn/]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7安装维护Gitlab]]></title>
    <url>%2F2015%2F08%2F26%2Fos-linux-CentOS7%E5%AE%89%E8%A3%85%E7%BB%B4%E6%8A%A4Gitlab%2F</url>
    <content type="text"><![CDATA[官方安装下面是官网复制过来的官方安装方法，最简单的安装，在我大天朝，只能望天兴叹，你可翻墙安装或者略过这里，看下面的。 安装并配置必要的依赖项 If you install Postfix to send email please select ‘Internet Site’ during setup. Instead of using Postfix you can also use Sendmail or configure a custom SMTP server and configure it as an SMTP server. On Centos 6 and 7, the commands below will also open HTTP and SSH access in the system firewall. 1234sudo yum install curl openssh-server openssh-clients postfix croniesudo service postfix startsudo chkconfig postfix onsudo lokkit -s http -s ssh 添加gitlab服务器包和安装包 12curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bashsudo yum install gitlab-ce If you are not comfortable installing the repository through a piped script, you can find the entire script here and select and download the package manually and install usinggitlab/gitlab-ce 123curl -LJO https://packages.gitlab.com/gitlab/gitlab-ce/packages/el/6/gitlab-ce-XXX.rpm/downloadcurl -LJO https://packages.gitlab.com/gitlab/gitlab-ce/packages/el/7/gitlab-ce-10.2.2-ce.0.el7.x86_64.rpm/downloadrpm -i gitlab-ce-XXX.rpm 配置并启动GitLab 1sudo gitlab-ctl reconfigure 浏览器打开并登录 On your first visit, you’ll be redirected to a password reset screen to provide the password for the initial administrator account. Enter your desired password and you’ll be redirected back to the login screen. The default account’s username is root. Provide the password you created earlier and login. After login you can change the username if you wish. 第三方镜像安装 Gitlab Community Edition 镜像使用帮助 在阿里云上通过Omnibus一键安装包安装Gitlab 编辑源新建 /etc/yum.repos.d/gitlab-ce.repo，内容为 使用清华大学 TUNA 镜像源 打开网址将内容复制到gitlab-ce.repo文件中，编辑路径vim /etc/yum.repos.d/gitlab-ce.repo 1234567[gitlab-ce]name=gitlab-cebaseurl=http://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el6repo_gpgcheck=0gpgcheck=0enabled=1gpgkey=https://packages.gitlab.com/gpg.key 更新本地YUM缓存1sudo yum makecache 安装社区版12sudo yum install gitlab-ce #(自动安装最新版)sudo yum install gitlab-ce-8.15.2-ce.0.el6 #(安装指定版本) 更改配置123vim /etc/gitlab/gitlab.rb# 找到 external_url 'http://000.00.00.00:8081'# 修改成你的地址 配置并启动GitLab123456# 打开`/etc/gitlab/gitlab.rb`,# 将`external_url = 'http://git.example.com'`修改为自己的IP地址：`http://xxx.xx.xxx.xx`，# 然后执行下面的命令，对GitLab进行编译。sudo gitlab-ctl reconfigure# 清除缓存sudo gitlab-rake cache:clear RAILS_ENV=production 登录GitLab12Username: root Password: 5iveL!fe Docker安装Docker 安装 Gitlab 教程 卸载1sudo gitlab-ctl uninstall 运维12345678910111213141516171819202122232425262728# 修改默认的配置文件sudo vim /etc/gitlab/gitlab.rb# 查看版本sudo cat /opt/gitlab/embedded/service/gitlab-rails/VERSION# echo "vm.overcommit_memory=1" &gt;&gt; /etc/sysctl.conf# sysctl -p# echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled# 检查gitlabgitlab-rake gitlab:check SANITIZE=true --tracegitlab-rake gitlab:checkgitlab-rake gitlab:check SANITIZE=true# 查看日志gitlab-ctl tail# 数据库关系升级gitlab-rake db:migrate# 清理缓存gitlab-rake cache:clear# 更新gitlab包yum update gitlab-ce# 升级gitlabyum install gitlab-ce# 升级数据命令gitlab-ctl pg-upgrade 服务管理12345678910gitlab-ctl start # 启动所有 gitlab 组件：gitlab-ctl stop # 停止所有 gitlab 组件：gitlab-ctl stop postgresql # 停止所有 gitlab postgresql 组件：# 停止相关数据连接服务gitlab-ctl stop unicorngitlab-ctl stop sidekiqgitlab-ctl restart # 重启所有 gitlab 组件：gitlab-ctl restart gitlab-workhorse # 重启所有 gitlab gitlab-workhorse 组件：gitlab-ctl status # 查看服务状态gitlab-ctl reconfigure # 生成配置启动服务 日志查看12345678sudo gitlab-ctl tail # 查看日志sudo gitlab-ctl tail redis # 检查redis的日志sudo gitlab-ctl tail postgresql # 检查postgresql的日志sudo gitlab-ctl tail gitlab-workhorse # 检查gitlab-workhorse的日志sudo gitlab-ctl tail logrotate # 检查logrotate的日志sudo gitlab-ctl tail nginx # 检查nginx的日志sudo gitlab-ctl tail sidekiq # 检查sidekiq的日志sudo gitlab-ctl tail unicorn # 检查unicorn的日志 重置管理员密码Gitlab管理员密码忘记，怎么重置密码，Gitlab 修改root用户密码，How to reset your root password。 使用rails工具打开终端 1sudo gitlab-rails console production 查询用户的email，用户名，密码等信息，id:1 表示root账号 1user = User.where(id: 1).first 重新设置密码 12user.password = '新密码'user.password_confirmation = '新密码' 保存密码 1user.save! 完整的操作ruby脚本 1234user = User.where(id: 1).firstuser.password = '新密码'user.password_confirmation = '新密码'user.save! 备份恢复使用Gitlab一键安装包安装Gitlab非常简单, 同样的备份恢复与迁移也非常简单,用一条命令即可创建完整的Gitlab备份: 修改备份文件默认目录修改/etc/gitlab/gitlab.rb来修改默认存放备份文件的目录: 1gitlab_rails['backup_path'] = '/mnt/backups' 创建备份1gitlab-rake gitlab:backup:create 以上命令将在 /var/opt/gitlab/backups 目录下创建一个名称类似为xxxxxxxx_gitlab_backup.tar的压缩包, 这个压缩包就是Gitlab整个的完整部分, 其中开头的xxxxxx是备份创建的时间戳。 修改后使用gitlab-ctl reconfigure命令重载配置文件。 开始备份这里放你的备份文件文件夹，和仓库源文件。 12/var/opt/gitlab/backups # 备份文件文件夹/var/opt/gitlab/git-data/repositories # git仓库源文件 自动备份通过crontab使用备份命令实现自动备份 1234crontab -e# 每天2点备份gitlab数据0 2 * * * /usr/bin/gitlab-rake gitlab:backup:create0 2 * * * /opt/gitlab/bin/gitlab-rake gitlab:backup:create 上面两行保存之后，重新载入配置 123service crond reload# orsystemctl reload crond.service 备份保留七天设置只保存最近7天的备份，编辑 /etc/gitlab/gitlab.rb 配置文件，找到如下代码，删除注释 # 保存 12# /etc/gitlab/gitlab.rb 配置文件 修改下面这一行gitlab_rails['backup_keep_time'] = 604800 重新加载gitlab配置文件 1sudo gitlab-ctl reconfigure 开始恢复迁移如同备份与恢复的步骤一样, 只需要将老服务器 /var/opt/gitlab/backups 目录下的备份文件拷贝到新服务器上的 /var/opt/gitlab/backups 即可(如果你没修改过默认备份目录的话)。 然后执行恢复命令。如果修改了，首先进入备份 gitlab 的目录，这个目录是配置文件中的 gitlab_rails[&#39;backup_path&#39;] ，默认为 /var/opt/gitlab/backups 。 然后停止 unicorn 和 sidekiq ，保证数据库没有新的连接，不会有写数据情况。 123456789101112131415# 停止相关数据连接服务gitlab-ctl stop unicorn# ok: down: unicorn: 0s, normally upgitlab-ctl stop sidekiq# ok: down: sidekiq: 0s, normally up# 从xxxxx编号备份中恢复# 然后恢复数据，1406691018为备份文件的时间戳gitlab-rake gitlab:backup:restore BACKUP=1406691018# 新版本 1483533591_2017_01_04_gitlab_backup.targitlab-rake gitlab:backup:restore BACKUP=1483533591_2017_01_04_gitlab_backup.tar# 启动Gitlabsudo gitlab-ctl start 判断是执行实际操作的gitlab相关用户：git，没有得到足够的权限。依次执行命令： 12345678910# 恢复过程中没有权限mkdir /var/opt/gitlab/backupschown git /var/opt/gitlab/backupschmod 700 /var/opt/gitlab/backups# 恢复成功页面报没有权限的错误sudo chown -R git:git /var/opt/gitlab/git-data/repositoriessudo chmod -R ug+rwX,o-rwx /var/opt/gitlab/git-data/repositoriessudo chmod -R ug-s /var/opt/gitlab/git-data/repositoriessudo find /var/opt/gitlab/git-data/repositories -type d -print0 | sudo xargs -0 chmod g+s 如果备份文件报没有权限，通过ls -al查看权限是不是git，而不是root，通过下面方式给git用户权限 1sudo chown -R git:git 1483533591_2017_01_04_gitlab_backup.tar 连接数据库123456# 登陆gitlab的安装服务查看配置文件cat /var/opt/gitlab/gitlab-rails/etc/database.yml vim /var/opt/gitlab/postgresql/data/postgresql.conf# listen_addresses = '192.168.1.125' # 修改监听地址为ip# 或者改为 "*" 修改 pg_hba.conf 配置 123vim /var/opt/gitlab/postgresql/data/pg_hba.conf# 将下面这一行添加到配置的最后面# host all all 0.0.0.0/0 trust 如果不希望允许所有IP远程访问，则可以将上述配置项中的0.0.0.0设定为特定的IP值。 重启 postgresql 数据库 1gitlab-ctl restart postgresql 查看 /etc/passwd 文件里边 gitlab 对应的系统用户 123[root@localhost ~]$ cat /etc/passwd...gitlab-psql:x:493:490::/var/opt/gitlab/postgresql:/bin/sh # gitlab的postgresql用户 一些常规目录123456# 配置目录/etc/gitlab/gitlab.rb# 生成好的nginx配置/var/opt/gitlab/nginx/conf/gitlab-http.conf# 备份目录/var/opt/gitlab/backups 使用HTTPS直接将nginx配置复制到你自己的nginx配置中，停掉gitlab的nginx 1cp /var/opt/gitlab/nginx/conf/gitlab-http.conf /usr/local/nginx/conf/vhost/ 将你的SSL证书配置复制进去 1234567server &#123; listen 443 ssl; server_name g.doman.cn; ssl_certificate /etc/letsencrypt/live/*****/certificate.crt; ssl_certificate_key /etc/letsencrypt/live/*****/private.key; # .....&#125; 编辑vi /usr/local/nginx/conf/nginx.conf你的nginx配置，引用你复制过来的配置。 1234http &#123; # ..... include vhost/gitlab-http.conf;&#125; 同时要把/var/opt/gitlab/nginx/conf/nginx.conf中的一些变量复制到自己的nginx配置中nginx.conf 123456789101112131415161718192021222324252627282930313233343536373839http &#123; # ..... log_format gitlab_access '$remote_addr - $remote_user [$time_local] "$request_method $filtered_request_uri $server_protocol" $status $body_bytes_sent "$filtered_http_referer" "$http_user_agent"'; log_format gitlab_mattermost_access '$remote_addr - $remote_user [$time_local] "$request_method $filtered_request_uri $server_protocol" $status $body_bytes_sent "$filtered_http_referer" "$http_user_agent"'; proxy_cache_path proxy_cache keys_zone=gitlab:10m max_size=1g levels=1:2; proxy_cache gitlab; map $http_upgrade $connection_upgrade &#123; default upgrade; '' close; &#125; # Remove private_token from the request URI # In: /foo?private_token=unfiltered&amp;authenticity_token=unfiltered&amp;rss_token=unfiltered&amp;... # Out: /foo?private_token=[FILTERED]&amp;authenticity_token=unfiltered&amp;rss_token=unfiltered&amp;... map $request_uri $temp_request_uri_1 &#123; default $request_uri; ~(?i)^(?&lt;start&gt;.*)(?&lt;temp&gt;[\?&amp;]private[\-_]token)=[^&amp;]*(?&lt;rest&gt;.*)$ "$start$temp=[FILTERED]$rest"; &#125; # Remove authenticity_token from the request URI # In: /foo?private_token=[FILTERED]&amp;authenticity_token=unfiltered&amp;rss_token=unfiltered&amp;... # Out: /foo?private_token=[FILTERED]&amp;authenticity_token=[FILTERED]&amp;rss_token=unfiltered&amp;... map $temp_request_uri_1 $temp_request_uri_2 &#123; default $temp_request_uri_1; ~(?i)^(?&lt;start&gt;.*)(?&lt;temp&gt;[\?&amp;]authenticity[\-_]token)=[^&amp;]*(?&lt;rest&gt;.*)$ "$start$temp=[FILTERED]$rest"; &#125; # Remove rss_token from the request URI # In: /foo?private_token=[FILTERED]&amp;authenticity_token=[FILTERED]&amp;rss_token=unfiltered&amp;... # Out: /foo?private_token=[FILTERED]&amp;authenticity_token=[FILTERED]&amp;rss_token=[FILTERED]&amp;... map $temp_request_uri_2 $filtered_request_uri &#123; default $temp_request_uri_2; ~(?i)^(?&lt;start&gt;.*)(?&lt;temp&gt;[\?&amp;]rss[\-_]token)=[^&amp;]*(?&lt;rest&gt;.*)$ "$start$temp=[FILTERED]$rest"; &#125; # A version of the referer without the query string map $http_referer $filtered_http_referer &#123; default $http_referer; ~^(?&lt;temp&gt;.*)\? $temp; &#125;&#125; 暴力升级暴力升级前先备份，然后停止所有服务运行，记得备份的良好习惯 123gitlab-ctl stop # 停止所有 gitlab 组件：# 更新gitlab包yum update gitlab-ce 直接编辑源 /etc/yum.repos.d/gitlab-ce.repo，安装 GitLab 社区版 123yum list gitlab-ce # 查看版本sudo yum install gitlab-ce #(自动安装最新版)sudo yum install gitlab-ce-8.15.2-ce.0.el6 #(安装指定版本) 注意：10.7 版本升级到 11.x 版本需要先升级到 10.8 版本 12# 安装指定版本 10.8 的版本sudo yum install gitlab-ce-10.8.0-ce.0.el6 安装完成记得将所有服务启起来哦 123gitlab-ctl start # 启动所有数据库# postgresql 数据库如果启动不了，通过重启启动gitlab-ctl restart postgresql 安装过如果报错，查看提示根据提示操作，版本跨度太大会报错哦。 12345678910111213141516171819202122gitlab preinstall: Automatically backing up only the GitLab SQL database (excluding everything else!)Dumping database ...Dumping PostgreSQL database gitlabhq_production ... pg_dump: [archiver (db)] connection to database &quot;gitlabhq_production&quot; failed: could not connect to server: 没有那个文件或目录 Is the server running locally and accepting connections on Unix domain socket &quot;/var/opt/gitlab/postgresql/.s.PGSQL.5432&quot;?Backup failed[FAILED]gitlab preinstall:gitlab preinstall: Backup failed! If you want to skip this backup, run the following command andgitlab preinstall: try again:gitlab preinstall:gitlab preinstall: sudo touch /etc/gitlab/skip-auto-migrationsgitlab preinstall:error: %pre(gitlab-ce-8.15.2-ce.0.el6.x86_64) scriptlet failed, exit status 1Error in PREIN scriptlet in rpm package gitlab-ce-8.15.2-ce.0.el6.x86_64error: install: %pre scriptlet failed (2), skipping gitlab-ce-8.15.2-ce.0.el6gitlab-ce-8.11.5-ce.0.el6.x86_64 was supposed to be removed but is not! Verifying : gitlab-ce-8.11.5-ce.0.el6.x86_64 1/2 Verifying : gitlab-ce-8.15.2-ce.0.el6.x86_64 2/2Failed: gitlab-ce.x86_64 0:8.11.5-ce.0.el6 看上面一堆错误，瞬间就懵逼了，看到一条救星命令让我尝试运行 sudo touch /etc/gitlab/skip-auto-migrations 于是我二逼的重新yum install gitlab-ce运行了，结果真的安装成功了，😄。 1234# 重新安装命令yum reinstall gitlab-ce# oryum install gitlab-ce 12345678910111213141516171819202122232425262728293031...gitlab: Thank you for installing GitLab!gitlab: To configure and start GitLab, RUN THE FOLLOWING COMMAND:sudo gitlab-ctl reconfiguregitlab: GitLab should be reachable at http://114.55.148.71:8081gitlab: Otherwise configure GitLab for your system by editing /etc/gitlab/gitlab.rb filegitlab: And running reconfigure again.gitlab:gitlab: For a comprehensive list of configuration options please see the Omnibus GitLab readmegitlab: https://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/README.mdgitlab:gitlab: GitLab now ships with a newer version of PostgreSQL (9.6.1), and will be usedgitlab: as the default in the next major relase. To upgrade, RUN THE FOLLOWING COMMANDS:sudo gitlab-ctl pg-upgradegitlab: For more details, please see:gitlab: https://docs.gitlab.com/omnibus/settings/database.html#upgrade-packaged-postgresql-servergitlab: 清理 : gitlab-ce-8.11.5-ce.0.el6.x86_64 2/2Found /etc/gitlab/skip-auto-migrations, exiting... Verifying : gitlab-ce-8.15.2-ce.0.el6.x86_64 1/2 Verifying : gitlab-ce-8.11.5-ce.0.el6.x86_64 2/2更新完毕: gitlab-ce.x86_64 0:8.15.2-ce.0.el6完毕！ 重启配置，可以解决大部分502错误。 1gitlab-ctl reconfigure 优化内存使用修改配置文件 /etc/gitlab/gitlab.rb 1234567891011# 减少 postgresql 数据库缓存postgresql['shared_buffers'] = "256MB"# 减少sidekiq的并发数sidekiq['concurrency'] = 1# worker进程数postgresql['max_worker_processes'] = 4unicorn['worker_processes'] = 2 ## worker进程数unicorn['worker_memory_limit_min'] = "400 * 1 &lt;&lt; 20" ##worker最小内存unicorn['worker_memory_limit_max'] = "650 * 1 &lt;&lt; 20" ##worker最大内存 错误处理解决80端口被占用nginx配置解决 80 端口被占用 1234567891011121314151617181920212223242526upstream gitlab &#123; server 114.55.111.111:8081 ;&#125;server &#123; # 侦听的80端口 listen 80; server_name git.diggg.cn; location / &#123; proxy_pass http://gitlab; #在这里设置一个代理，和upstream的名字一样 #以下是一些反向代理的配置可删除 proxy_redirect off; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; client_max_body_size 10m; #允许客户端请求的最大单文件字节数 client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数 proxy_connect_timeout 300; #nginx跟后端服务器连接超时时间(代理连接超时) proxy_send_timeout 300; #后端服务器数据回传时间(代理发送超时) proxy_read_timeout 300; #连接成功后，后端服务器响应时间(代理接收超时) proxy_buffer_size 4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 4 32k; #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置 proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2） proxy_temp_file_write_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传 &#125;&#125; nginx配置检查和立即生效 1234# 检查配置/usr/local/nginx/sbin/nginx -tc conf/nginx.conf# nginx 重新加载配置/usr/local/nginx/sbin/nginx -s reload 头像无法正常显示原因：gravatar被墙解决办法：编辑 /etc/gitlab/gitlab.rb，将 1# gitlab_rails['gravatar_plain_url'] = 'http://gravatar.duoshuo.com/avatar/%&#123;hash&#125;?s=%&#123;size&#125;&amp;d=identicon' 修改为： 1gitlab_rails[&apos;gravatar_plain_url&apos;] = &apos;http://gravatar.duoshuo.com/avatar/%&#123;hash&#125;?s=%&#123;size&#125;&amp;d=identicon&apos; 然后在命令行执行： 12sudo gitlab-ctl reconfigure sudo gitlab-rake cache:clear RAILS_ENV=production internal API unreachable这个错误是一个自己制造的坑，我克隆和提交都没有办法搞，但是网站能正常运行，尝试了非常多的方法，最终我的问题是22端口没有隐射出去，好尴尬。 1GitLab: Failed to authorize your Git request: internal API unreachable 解决办法：https://gitlab.com/gitlab-org/gitlab-ce/issues/33702通过防火墙规则 127.0.0.1 proxy_temp 目录没有权限1[crit] 14788#0: *215 open() "/usr/local/nginx/proxy_temp/5/01/0000000015" failed (13: Permission denied) while reading upstream 以下方式解决 12345chown -R root:root /usr/local/nginx/proxy_temp# 编辑 nginx.confsudo vi /usr/local/nginx/conf/nginx.conf# 在第一行添加user root; webhooks 错误错误显示不允许发送本地请求 1Url is blocked: Requests to the local network are not allowed 解决方法，在设置中设置允许本地连接即可 admin =&gt; Settings =&gt; Outbound requests 服务无法启动123456789101112131415[root@localhost gitlab]# gitlab-ctl statusfail: alertmanager: runsv not runningfail: gitaly: runsv not runningfail: gitlab-monitor: runsv not runningfail: gitlab-workhorse: runsv not runningfail: logrotate: runsv not runningfail: nginx: runsv not runningfail: node-exporter: runsv not runningfail: postgres-exporter: runsv not runningfail: postgresql: runsv not runningfail: prometheus: runsv not runningfail: redis: runsv not runningfail: redis-exporter: runsv not runningfail: sidekiq: runsv not runningfail: unicorn: runsv not running Omnibus gitlab do not restart on CentOS7开机自动启动服务 1234[root@localhost ~]# systemctl status gitlab-runsvdir.service -l● gitlab-runsvdir.service - GitLab Runit supervision process Loaded: loaded (/usr/lib/systemd/system/gitlab-runsvdir.service; enabled; vendor preset: disabled) Active: inactive (dead) 如果 gitlab-runsvdir.service 服务没有响应，你可能要看一下内存是否满了，需要释放内存，老的版本需要 2G 内存，新版本需要至少 4G 内存。 其它错误1Error executing action `run` on resource 'bash[migrate gitlab-rails database]' 上面错误是数据库没有启动，我不知道如何启动，我重启了服务器，然后好球了。😆https://gitlab.com/gitlab-org/gitlab-ce/issues/2052#note_1667899 1NameError: uninitialized constant Devise::Async 12Processing by RootController#index as HTMLCompleted 401 Unauthorized in 17ms (ActiveRecord: 2.7ms) 12/var/log/gitlab/nginx/gitlab_access.log &lt;==114.55.148.71 - - [04/Jan/2017:17:20:24 +0800] &quot;GET /favicon.ico HTTP/1.0&quot; 502 2662 &quot;http://git.xxxxx.cn/&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36&quot; 参考资料 gitlab/gitlab-ce 官网下载 官网安装说明 开源版本和企业版本对比 官方升级Gitlab教程 官方Centos安装Gitlab教程 Gitlab升级记录 修改gitlab使用现有nginx服务及502问题解决 我所遇到的GitLab 502问题的解决]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署Seafile搭建自己的网盘]]></title>
    <url>%2F2015%2F08%2F26%2Fos-linux-%E9%83%A8%E7%BD%B2Seafile%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E7%BD%91%E7%9B%98%2F</url>
    <content type="text"><![CDATA[Seafile 是一个开源的文件云存储平台，解决文件集中存储、同步、多平台访问的问，允许用户创建“群组”，在群组内共享和同步文件，方便了团队协同工作。 安装依赖 下载 安装 启动 Seafile 启动 Seahub 备份 恢复 服务管理 开机启动 参考资料 安装依赖1234567# 在 CentOS 7 下# (MariaDB 是 MySQL 的分支)yum install mariadb-server# 如果以来有问题 # 尝试在后面加上参数 --skip-brokenyum install python-setuptools python-imaging python-ldap MySQL-python python-memcached python-urllib3 yum 安装 mysqldb-python 后面seafile安装报错，需要通过 python 的工具pip来安装MySQL-python pip install MySQL-python 下载在这里下载seafile-server_6.0.7_x86-64.tar.gz，你可以选择你需要的版本 123456789# 查看系统版本cat /proc/versionwget http://download-cn.seafile.com/seafile-server_6.0.7_x86-64.tar.gz# 解压tar -zxvf seafile-server_6.0.7_x86-64.tar.gz# 解压放到一个目录cd seafile-server-* 安装我的数据库使用MySQL，你需要先在数据库中建立一个 MySQL 用户 seafile。 1234567# 登录MySQL创建一个用户mysql -uroot -p# 创建用户设置密码mysql&gt; Create USER 'seafile'@'%' IDENTIFIED BY '123456';# 执行下面语句立即生效mysql&gt; flush privileges; 运行./setup-seafile-mysql.sh安装脚本并回答预设问题 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122./setup-seafile-mysql.sh Checking python on this machine ... Checking python module: setuptools ... Done. Checking python module: python-imaging ... Done. Checking python module: python-mysqldb ... Done.-----------------------------------------------------------------This script will guide you to setup your seafile server using MySQL.Make sure you have read seafile server manual at https://github.com/haiwen/seafile/wikiPress ENTER to continue-----------------------------------------------------------------What is the name of the server? It will be displayed on the client.3 - 15 letters or digits[ server name ] &lt;填写 seafile 服务器的名字&gt;What is the ip or domain of the server?For example: www.mycompany.com, 192.168.1.101[ This server's ip or domain ] &lt;seafile 服务器的 IP 地址或者域名&gt;Where do you want to put your seafile data?Please use a volume with enough free space[ default "/home/www/jinpans/seafile-data" ]Which port do you want to use for the seafile fileserver?[ default "8082" ] [ seafile fileserver 使用的 TCP 端口 ]-------------------------------------------------------Please choose a way to initialize seafile databases:-------------------------------------------------------[1] Create new ccnet/seafile/seahub databases 你需要提供根密码. 脚本程序会创建数据库和用户。[2] Use existing ccnet/seafile/seahub databases ccnet/seafile/seahub 数据库应该已经被你（或者其他人）提前创建。[ 1 or 2 ] &lt;选择一种创建 Seafile 数据库的方式&gt;What is the host of mysql server?[ default "localhost" ]What is the port of mysql server?[ default "3306" ]What is the password of the mysql root user?[ root password ] &lt;输入root密码&gt;verifying password of user root ... doneEnter the name for mysql user of seafile. It would be created if not exists.[ default "seafile" ] &lt;默认seafile的MySQL用户名，可以使用默认&gt;Enter the password for mysql user "seafile":[ password for seafile ] &lt;输入seafile密码&gt;verifying password of user seafile ... doneEnter the database name for ccnet-server:[ default "ccnet-db" ]Enter the database name for seafile-server:[ default "seafile-db" ]Enter the database name for seahub:[ default "seahub-db" ]---------------------------------This is your configuration--------------------------------- server name: seafile 服务器的名字 server ip/domain: 192.168.1.101 seafile data dir: /home/www/jinpans/seafile-data fileserver port: 8082 database: create new ccnet database: ccnet-db seafile database: seafile-db seahub database: seahub-db database user: seafile---------------------------------Press ENTER to continue, or Ctrl-C to abort---------------------------------Generating ccnet configuration ...doneSuccessly create configuration dir /home/www/jinpans/ccnet.Generating seafile configuration ...Done.doneGenerating seahub configuration ...----------------------------------------Now creating seahub database tables ...----------------------------------------creating seafile-server-latest symbolic link ... done-----------------------------------------------------------------Your seafile server configuration has been finished successfully.-----------------------------------------------------------------run seafile server: ./seafile.sh &#123; start | stop | restart &#125;run seahub server: ./seahub.sh &#123; start &lt;port&gt; | stop | restart &lt;port&gt; &#125;-----------------------------------------------------------------If you are behind a firewall, remember to allow input/output of these tcp ports:-----------------------------------------------------------------port of seafile fileserver: 8082port of seahub: 8000When problems occur, Refer to https://github.com/haiwen/seafile/wiki 上面算是结束了，然后在 seafile-server_6.0.7 目录下面，运行如下命令 启动 Seafile1./seafile.sh start # 启动 Seafile 服务 启动 Seahub1234567891011121314151617181920212223./seahub.sh start &lt;port&gt; # 启动 Seahub 网站 （默认运行在8000端口上）# 你第一次启动 seahub 时，seahub.sh 脚本会提示你创建一个 seafile 管理员帐号。LC_ALL is not set in ENV, set to en_US.UTF-8Starting seahub at port 8000 ...----------------------------------------It's the first time you start the seafile server. Now let's create the admin account----------------------------------------What is the email for the admin account?[ admin email ] &lt;这里输入邮箱地址&gt;What is the password for the admin account?[ admin password ] &lt;这里输入密码&gt;Enter the password again:[ admin password again ] &lt;这里确认输入密码&gt;----------------------------------------Successfully created seafile admin----------------------------------------Seahub is startedDone. 然后你可以打开它了： 1http://192.168.1.111:8000/ 备份备份数据库，假设你的数据库名分别为 ccnet-db , seafile-db 和 seahubdb，下面命令分别备份这三个数据库，同时要注意备份需要先建立好文件夹。 123mysqldump -h localhost -uroot -p --opt ccnet-db &gt; ./backup/databases/ccnet-db.sql.`date +"%Y-%m-%d-%H-%M-%S"`mysqldump -h localhost -uroot -p --opt seafile-db &gt; ./backup/databases/seafile-db.sql.`date +"%Y-%m-%d-%H-%M-%S"`mysqldump -h localhost -uroot -p --opt seahub-db &gt; ./backup/databases/seahub-db.sql.`date +"%Y-%m-%d-%H-%M-%S"` 备份文件 1234nohup cp -R ./seafile-data ./backup/data/seafile-data-`date +"%Y-%m-%d-%H-%M-%S"` 2&gt;&amp;1 &amp;nohup cp -R ./seahub-data ./backup/data/seahub-data-`date +"%Y-%m-%d-%H-%M-%S"` 2&gt;&amp;1 &amp;[1] 17294 恢复123mysql -uroot -p ccnet-db &lt; ccnet-db.sql.2017-11-29-11-36-06 mysql -uroot -p seafile-db &lt; seafile-db.sql.2017-11-29-11-36-06 mysql -uroot -p seahub-db &lt; seahub-db.sql.2017-11-29-11-36-06 服务管理12345678./seahub.sh stop # 停止 Seafile 进程./seafile.sh stop # 停止 Seahub./seafile.sh start # 启动 Seafile 服务./seahub.sh start 8001 # 启动 Seahub 网站 （运行在8001端口上）./seafile.sh restart # 停止当前的 Seafile 进程，然后重启 Seafile./seahub.sh restart # 停止当前的 Seahub 进程，并在 8000 端口重新启动 Seahub 开机启动创建 systemd 服务文件seafile.service，记得将服务文件中的${seafile_dir}替换成你的安装目录 1sudo vim /etc/systemd/system/seafile.service 文件内容如下： 123456789101112131415[Unit]Description=Seafile# add mysql.service or postgresql.service depending on your database to the line belowAfter=network.target[Service]Type=oneshotExecStart=$&#123;seafile_dir&#125;/seafile-server-latest/seafile.sh startExecStop=$&#123;seafile_dir&#125;/seafile-server-latest/seafile.sh stopRemainAfterExit=yesUser=seafileGroup=seafile[Install]WantedBy=multi-user.target 创建 systemd 服务文件 seahub.service，记得将服务文件中的${seafile_dir}替换成你的安装目录 1sudo vim /etc/systemd/system/seahub.service 文件内容如下： 123456789101112131415[Unit]Description=Seafile hubAfter=network.target seafile.service[Service]# change start to start-fastcgi if you want to run fastcgiExecStart=$&#123;seafile_dir&#125;/seafile-server-latest/seahub.sh startExecStop=$&#123;seafile_dir&#125;/seafile-server-latest/seahub.sh stopUser=seafileGroup=seafileType=oneshotRemainAfterExit=yes[Install]WantedBy=multi-user.target [Install]WantedBy=multi-user.target 12sudo systemctl enable seafile.servicesudo systemctl enable seahub.service 参考资料 Seafile服务器手册中文版 Seafile for Github Seafile官网]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2F2015%2F08%2F26%2Fos-linux-%E5%B8%B8%E7%94%A8%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[其它命令sudo chmod 755 -R node 修改目录权限sudo lsof -nP -iTCP -sTCP:LISTEN 查看本地服务ps -ef | grep websocket 查看websocket进程ps aux | grep mysql 查看mysql进程sudo kill 443 杀掉进程 给目录权限sudo chmod -R 777 目录 搜索find path -option [-print] [-exec -ok command] { }\; pathname: find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录。 -print: find命令将匹配的文件输出到标准输出。 -exec: find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为&#39;command&#39; { } \;，注意{ }和\;之间的空格。 -ok： 和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。 1234567891011$ find ~ -name "*.txt" -print #在$HOME中查.txt文件并显示$ find . -name "*.txt" -print$ find . -name "[A-Z]*" -print #查以大写字母开头的文件$ find /etc -name "host*" -print #查以host开头的文件# 查以两个小写字母和两个数字开头的txt文件$ find . -name "[a-z][a-z][0–9][0–9].txt" -print $ ind . -perm 755 -print$ ind . -perm -007 -exec ls -l &#123;&#125; \; #查所有用户都可读写执行的文件同-perm 777$ ind . -type d -print$ ind . ! -type d -print $ ind . -type l -print ls 类似于dos下的dir命令 ls最常用的参数有三个： -a -l -F。 ls –a Linux上的文件以.开头的文件被系统视为隐藏文件，仅用ls命令是看不到他们的，而用ls -a除了显示一般文件名外，连隐藏文件也会显示出来。ls –l该参数显示更详细的文件信息。ls –F使用这个参数表示在文件的后面多添加表示文件类型的符号，例如*表示可执行，/表示目录，@表示连结文件，这都是因为使用了-F这个参数。但是现在基本上所有的Linux发行版本的ls都已经内建了-F参数，也就是说，不用输入这个参数，我们也能看到各种分辨符号。 cd 用于切换用户当前工作目录 cd aaa 进入aaa目录cd 命令后不指定目录，会切换到当前用户的home 目录cd ~ 作用同cd后不指定目录，切换到当前用户的home 目录cd - 命令后跟一个减号，则会退回到切换前的目录cd .. 返回到当前目录下的上一级目录 pwd 用于显示用户当前工作目录 mkdir 和 rmdir midir:创建目录 / rmdir:删除目录 两个命令都支持-p参数，对于mkdir命令若指定路径的父目录不存在则一并创建，对于rmdir命令则删除指定路径的所有层次目录，如果文件夹里有内容，则不能用rmdir命令 如下： 12mkdir -p 1/2/3rmdir -p 1/2/3 mkdir循环创建目录How do I make multiple directories at once in a directory? 1234567for char in &#123;A..Z&#125;; do mkdir $chardonefor num in &#123;1..100&#125;; do mkdir $numdone cp 复制命令 复制一个文件到另一目录：cp 1.txt ../test2 复制一个文件到本目录并改名：cp 1.txt 2.txt 复制一个文件夹a并改名为b，-r或-R 选项表明递归操作：cp -r a b 同时拷贝多个文件，我们只需要将多个文件用空格隔开。cp file_1.txt file_2.txt file_3.txt /home/pungki/office mv 移动命令 将一个文件移动到另一个目录：mv 1.txt ../test1将一个文件在本目录改名：mv 1.txt 2.txt将一个文件一定到另一个目录并改名：mv 1.txt ../test1/2.txt rm命令 命令用于删除文件，与dos下的del/erase命令相似，rm命令常用的参数有三个：-i，-r，-f。 –i ：系统在删除文件之前会先询问确认，用户回车之后，文件才会真的被删除。需要注意，linux下删除的文件是不能恢复的，删除之前一定要谨慎确认。 –r：该参数支持目录删除，功能和rmdir命令相似。 –f：和-i参数相反，-f表示强制删除 find find 查找目录和文件 12345678910111213141516171819202122232425262728293031-name filename #查找名为filename的文件-perm #按执行权限来查找-user username #按文件属主来查找-group groupname #按组来查找-mtime -n +n #按文件更改时间来查找文件，-n指n天以内，+n指n天以前-atime -n +n #按文件访问时间来查GIN: 0px"&gt;-ctime -n +n #按文件创建时间来查找文件，-n指n天以内，+n指n天以前-nogroup #查无有效属组的文件，即文件的属组在/etc/groups中不存在-nouser #查无有效属主的文件，即文件的属主在/etc/passwd中不存-newer f1 !f2 #找文件，-n指n天以内，+n指n天以前 -ctime -n +n #按文件创建时间来查找文件，-n指n天以内，+n指n天以前 -nogroup #查无有效属组的文件，即文件的属组在/etc/groups中不存在-nouser #查无有效属主的文件，即文件的属主在/etc/passwd中不存-newer f1 !f2 #查更改时间比f1新但比f2旧的文件-type b/d/c/p #查是块设备、目录、字符设备、管道、符号链接、普通文件-size n[c] #查长度为n块[或n字节]的文件-depth #使查找在进入子目录前先行查找完本目录-fstype #查更改时间比f1新但比f2旧的文件-type b/d/c/p #查是块设备、目录、字符设备、管道、符号链接、普通文件-size n[c] #查长度为n块[或n字节]的文件-depth #使查找在进入子目录前先行查找完本目录-fstype #查位于某一类型文件系统中的文件，这些文件系统类型通常可 在/etc/fstab中找到-mount #查文件时不跨越文件系统mount点-follow #如果遇到符号链接文件，就跟踪链接所指的文件-cpio %; #查位于某一类型文件系统中的文件，这些文件系统类型通常可 在/etc/fstab中找到-mount #查文件时不跨越文件系统mount点-follow #如果遇到符号链接文件，就跟踪链接所指的文件-cpio #对匹配的文件使用cpio命令，将他们备份到磁带设备中-prune #忽略某个目录 例子： ~ 代表的是$home目录， 12345678910111213141516171819202122232425262728293031323334353637383940414243$ find . -name '*.DS_Store' -type f -delete # 删除所有.DS_Store文件$ find ~ -name "*.txt" -print # 在$HOME中查.txt文件并显示$ find . -size +1000000c -print # 查长度大于1Mb的文件$ find . -size 100c -print # 查长度为100c的文件$ find . -size +10 -print # 查长度超过期作废10块的文件（1块=512字节）$ find -name april* # 在当前目录下查找以april开始的文件$ find -name april* fprint file # 在当前目录下查找以april开始的文件，并把结果输出到file中$ find -name ap* -o -name may* # 查找以ap或may开头的文件$ find /mnt -name tom.txt -ftype vfat # 在/mnt下查找名称为tom.txt且文件系统类型为vfat的文件$ find /mnt -name t.txt ! -ftype vfat # 在/mnt下查找名称为tom.txt且文件系统类型不为vfat的文件$ find /tmp -name wa* -type l # 在/tmp下查找名为wa开头且类型为符号链接的文件$ find ~ -mtime -2 # 在/home下查最近两天内改动过的文件$ find ~ -atime -1 # 查1天之内被存取过的文件$ find ~ -mmin +60 # 在/home下查60分钟前改动过的文件$ find ~ -amin +30 # 查最近30分钟前被存取过的文件$ find ~ -newer tmp.txt # 在/home下查更新时间比tmp.txt近的文件或目录$ find ~ -anewer tmp.txt # 在/home下查存取时间比tmp.txt近的文件或目录$ find ~ -used -2 # 列出文件或目录被改动过之后，在2日内被存取过的文件或目录$ find ~ -user cnscn # 列出/home目录内属于用户cnscn的文件或目录$ find ~ -uid +501 # 列出/home目录内用户的识别码大于501的文件或目录$ find ~ -group cnscn # 列出/home内组为cnscn的文件或目录$ find ~ -gid 501 # 列出/home内组id为501的文件或目录$ find ~ -nouser # 列出/home内不属于本地用户的文件或目录$ find ~ -nogroup # 列出/home内不属于本地组的文件或目录$ find ~ -name tmp.txt -maxdepth 4 # 列出/home内的tmp.txt 查时深度最多为3层$ find ~ -name tmp.txt -mindepth 3 # 从第2层开始查$ find ~ -empty # 查找大小为0的文件或空目录$ find ~ -size +512k # 查大于512k的文件$ find ~ -size -512k # 查小于512k的文件$ find ~ -links +2 # 查硬连接数大于2的文件或目录$ find ~ -perm 0700 # 查权限为700的文件或目录$ find ~ -perm 755 -print | more # 查找权限为755的文件$ find /tmp -name tmp.txt -exec cat &#123;&#125; \;$ find /tmp -name tmp.txt ok rm &#123;&#125; \;$ find / -amin -10 # 查找在系统中最后10分钟访问的文件$ find / -atime -2 # 查找在系统中最后48小时访问的文件$ find / -empty # 查找在系统中为空的文件或者文件夹$ find / -group cat # 查找在系统中属于 groupcat的文件$ find / -mmin -5 # 查找在系统中最后5分钟里修改过的文件$ find / -mtime -1 # 查找在系统中最后24小时里修改过的文件$ find / -nouser # 查找在系统中属于作废用户的文件$ find / -user fred # 查找在系统中属于FRED这个用户的文件 du、df命令 du命令可以显示目前的目录所占用的磁盘空间，df命令可以显示目前磁盘剩余空间。 如果du命令不加任何参数，那么返回的是整个磁盘的使用情况，如果后面加了目录的话，就是这个目录在磁盘上的使用情况。 du -hs 指定目录 查看指定目录的总大小 du -hs ./* 查看当前目录下的所有文件夹和文件的大小 这两个命令都支持-k，-m和-h参数，-k和-m类似，都表示显示单位，一个是k字节一个是兆字节，-h则表示human-readable，即友好可读的显示方式。 cat命令 cat命令的功能是显示或连结一般的ascii文本文件。cat是concatenate的简写，类似于dos下面的type命令。用法如下： cat file1 显示file1文件内容 cat file1 file2 依次显示file1,file2的内容 cat file1 file2 &gt; file3 把file1, file2的内容结合起来，再“重定向（&gt;）”到file3文件中。 &gt;是右重定向符，表示将左边命令结果当成右边命令的输入，注意：如果右侧文件是一个已存在文件，其原有内容将会被清空，而变成左侧命令输出内容。如果希望以追加方式写入，请改用”&gt;&gt;”重定向符。 如果”&gt;”左边没有指定文件，如： cat &gt;file1，将会等用户输入，输入完毕后再按[Ctrl]+[c]或[Ctrl]+[d]，就会将用户的输入内容写入file1。 echo命令 echo命令的使用频率不少于ls和cat，尤其是在shell脚本编写中。语法：echo [-ne][字符串]功能：echo会将输入的字符串送往标准输出，输出的字符串间以空白字符隔开， 并在最后加上换行符。 参数： -n 显示字串时在最后自动换行 -e 支持以下格式的转义字符， -E 不支持以下格式的转义字符 /a 发出警告声； /b 删除前一个字符； /c 最后不加上换行符号； /f 换行但光标仍旧停留在原来的位置； /n 换行且光标移至行首； /r 光标移至行首，但不换行； /t 插入tab； /v 与/f相同； // 插入/字符； /nnn 插入nnn（八进制）所代表的ASCII字符； 示例： 123456789101112131415kenny@jstest:~/hgd&gt; echo "123" "456"123 456kenny@jstest:~/hgd&gt; echo "123/n456"123/n456kenny@jstest:~/hgd&gt; echo -e "123/n456"123456kenny@jstest:~/hgd&gt; echo -E "123/n456"123/n456kenny@jstest:~/hgd&gt; echo -E "123///456"123//456kenny@jstest:~/hgd&gt; echo -e "123///456"123/456kenny@jstest:~/hgd&gt; echo -e "123/100456"123@456 注意事项：在Linux使用的bash下，单引号’’和双引号是有区别的，单引号忽略所有的转义，双引号不会忽略以下特殊字符：Dollar signs ($)，Back quotes (`)，Backslashes (/)，Excalmatory mark(!) 示例如下： 1234567891011121314151617kenny@jstest:~&gt; echo &quot;`TEST`&quot;-bash: TEST: command not foundkenny@jstest:~&gt; echo &apos;`TEST`&apos;`TEST`kenny@jstest:~&gt; echo &quot;$TEST&quot; kenny@jstest:~&gt; echo &apos;$TEST&apos;$TESTkenny@jstest:~&gt; echo &quot;//TEST&quot;/TESTkenny@jstest:~&gt; echo &apos;//TEST&apos;//TESTkenny@jstest:~&gt; echo &quot;Hello!&quot;echo &quot;Hello&quot;Hellokenny@jstest:~&gt; echo &apos;Hello!&apos;Hello! more，less，clear more，less命令 1这两个命令用于查看文件，如果一个文件太长，显示内容超出一个屏幕，用cat命令只能看到最后的内容，用more和less两个命令可以分页查看。more指令可以使超过一页的文件内容分页暂停显示，用户按键后才继续显示下一页。而less除了有more的功能以外，还可以用方向键往上或往下的滚动文件，更方便浏览阅读。 less的常用动作命令： 回车键 向下移动一行； y 向上移动一行； 空格键 向下滚动一屏； b 向上滚动一屏； d 向下滚动半屏； h less的帮助； u 向上洋动半屏； w 可以指定显示哪行开始显示，是从指定数字的下一行显示；比如指定的是6，那就从第7行显示； g 跳到第一行； G 跳到最后一行； p n% 跳到n%，比如 10%，也就是说比整个文件内容的10%处开始显示； /pattern 搜索pattern ，比如 /MAIL表示在文件中搜索MAIL单词； v 调用vi编辑器； q 退出less !command 调用SHELL，可以运行命令；比如!ls 显示当前列当前目录下的所有文件； clear命令 clear命令是用来清除当前屏幕显示的，不需任何参数，和dos下的cls命令功能相同。 head，tail head和tail命令都用于查看文本文件，区别在于： head显示文件的头n行，tail显示文件的尾n行，缺省情况n都为10行。可以通过-n方式指定行数，如：head -100 file和tail -100 file分别表示显示文件头100行和尾100行内容。 tail -f命令可以实时查看文件新增内容。 wc命令该命令用于统计指定文件中的字节数、字数、行数。该命令各选项含义如下： -l 统计行数 -w 统计字数 -c 统计字节数 这些选项可以组合使用。输出列的顺序和数目不受选项的顺序和数目的影响。总是按下述顺序显示并且每项最多一列。行数、字数、字节数、文件名如果命令行中没有文件名，则输出中不出现文件名。 例如： 123456oracle@hjtest:~&gt; wc 1.txt 2.txt 460 1679 16353 1.txt 300 1095 10665 2.txt 760 2774 27018 总用量oracle@hjtest:~&gt; wc -l 1.txt460 1.txt 缺省参数为-lcw，即wc file1 file2命令的执行结果与上面一样。 grep 命令 grep是（global search regular expression(RE) and print out the line的缩写，用于从文件面搜索包含指定模式的行并打印出来，它是一种强大的文本搜索工具，支持使用正则表达式搜索文本。grep的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被””引用，模板后的所有字符串被看作文件名。搜索结果送到屏幕，不影响原文件内容。 grep可用于shell脚本，因为grep通过返回一个状态值来说明搜索的状态，如果模板搜索成功，则返回0，如果搜索不成功，则返回1，如果搜索的文件不存在，则返回2。我们利用这些返回值就可进行一些自动化的文本处理工作。示例： 12345678910$ ls -l | grep &apos;^a&apos;通过管道过滤ls -l输出的内容，只显示以a开头的行。$ grep &apos;test&apos; d*显示所有以d开头的文件中包含test的行。$ grep &apos;test&apos; aa bb cc显示在aa，bb，cc文件中匹配test的行。$ grep &apos;[a-z]/&#123;5/&#125;&apos; aa显示所有包含每个字符串至少有5个连续小写字符的字符串的行。$ grep &apos;w/(es/)t.*/1&apos; aa如果west被匹配，则es就被存储到内存中，并标记为1，然后搜索任意个字符（.*），这些字符后面紧跟着另外一个es（/1），找到就显示该行。如果用egrep或grep -E，就不用&quot;/&quot;号进行转义，直接写成&apos;w(es)t.*/1&apos;就可以了。 man，logout命令 man命令，man是manual的缩写，相当于Unix/Linux的联机Help，每个系统命令和调用都有非常详细的说明，绝大多数都是英文。如：man ls即是查看ls命令的使用说明，一般还有另一种方法用来查看帮助，如：ls –help，这种方式绝大多数命令都支持。 logout命令，该命令用于退出系统，与login命令对应。 管道和xargs管道利用Linux所提供的管道符“|”将两个命令隔开，管道符左边命令的输出就会作为管道符右边命令的输入。连续使用管道意味着第一个命令的输出会作为第二个命令的输入，第二个命令的输出又会作为第三个命令的输入，依此类推。注意：管道左边命令的输入作为管道右边命令的输入(命令的输入是一定的)，不是参数，并不是所有命令都支持管道例子：ls | grep a 查看当前目录下名称包含a的文件或文件夹 xargs大多数 Linux 命令都会产生输出：文件列表、字符串列表等。但如果要使用其他某个命令并将前一个命令的输出作为参数该怎么办？例如，file 命令显示文件类型（可执行文件、ascii 文本等）；你能处理输出，使其仅显示文件名，目前你希望将这些名称传递给 ls -l 命令以查看时间戳记。xargs 命令就是用来完成此项工作的。注意：find命令把匹配到的文件传递给xargs命令，而xargs命令每次只获取一部分文件而不是全部，不像-exec选项那样。这样它可以先处理最先获取的一部分文件，然后是下一批，并如此继续下去 例子： 在整个系统中查找内存信息转储文件(core dump) ，然后把结果保存到/tmp/core.log 文件中： 1$ find / -name &quot;core&quot; -print | xargs echo &quot;&quot; &gt;/tmp/core.log 当一个目录下文件太多时，直接用rm * 命令会包参数过长,用如下方法可以全部删除 1$ls | xargs rm basename 和 dirnamebasename用于查看文件不含路径的名字，dirname则用于查看文件路径，使用效果我们测试一下便知： 12345678&gt; basename /home/hj/1.txt1.txt&gt; dirname /home/hj/1.txt/home/hj&gt; basename 1.txt1.txt&gt; dirname 1.txt.]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux操作系统]]></title>
    <url>%2F2015%2F07%2F23%2Fos-linux-%E5%90%8E%E7%AB%AF%E7%A8%8B%E5%BA%8F%E5%91%98%E5%BF%85%E5%A4%87%E7%9A%84Linux%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[学习Linux之前，我们先来简单的认识一下操作系统。 一 从认识操作系统开始1.1 操作系统简介我通过以下四点介绍什么操作系统： 操作系统（Operation System，简称OS）是管理计算机硬件与软件资源的程序，是计算机系统的内核与基石； 操作系统本质上是运行在计算机上的软件程序 ； 为用户提供一个与系统交互的操作界面 ； 操作系统分内核与外壳（我们可以把外壳理解成围绕着内核的应用程序，而内核就是能操作硬件的程序）。 1.2 操作系统简单分类 Windows: 目前最流行的个人桌面操作系统 ，不做多的介绍，大家都清楚。 Unix： 最早的多用户、多任务操作系统 .按照操作系统的分类，属于分时操作系统。Unix 大多被用在服务器、工作站，现在也有用在个人计算机上。它在创建互联网、计算机网络或客户端/服务器模型方面发挥着非常重要的作用。 Linux: Linux是一套免费使用和自由传播的类Unix操作系统.Linux存在着许多不同的Linux版本，但它们都使用了 Linux内核 。Linux可安装在各种计算机硬件设备中，比如手机、平板电脑、路由器、视频游戏控制台、台式计算机、大型机和超级计算机。严格来讲，Linux这个词本身只表示Linux内核，但实际上人们已经习惯了用Linux来形容整个基于Linux内核，并且使用GNU 工程各种工具和数据库的操作系统。 二 初探Linux2.1 Linux简介我们上面已经介绍到了Linux，我们这里只强调三点。 类Unix系统： Linux是一种自由、开放源码的类似Unix的操作系统 Linux内核： 严格来说，Linux这个词本身只表示Linux内核 Linux之父： 一个编程领域的传奇式人物。他是Linux内核的最早作者，随后发起了这个开源项目，担任Linux内核的首要架构师与项目协调者，是当今世界最著名的电脑程序员、黑客之一。他还发起了Git这个开源项目，并为主要的开发者。 2.2 Linux诞生简介 1991年，芬兰的业余计算机爱好者Linus Torvalds编写了一款类似Minix的系统（基于微内核架构的类Unix操作系统）被ftp管理员命名为Linux 加入到自由软件基金的GNU计划中; Linux以一只可爱的企鹅作为标志，象征着敢作敢为、热爱生活。 2.3 Linux的分类Linux根据原生程度，分为两种： 内核版本： Linux不是一个操作系统，严格来讲，Linux只是一个操作系统中的内核。内核是什么？内核建立了计算机软件与硬件之间通讯的平台，内核提供系统服务，比如文件管理、虚拟内存、设备I/O等； 发行版本： 一些组织或公司在内核版基础上进行二次开发而重新发行的版本。Linux发行版本有很多种（ubuntu和CentOS用的都很多，初学建议选择CentOS），如下图所示： 三 Linux文件系统概览3.1 Linux文件系统简介在Linux操作系统中，所有被操作系统管理的资源，例如网络接口卡、磁盘驱动器、打印机、输入输出设备、普通文件或是目录都被看作是一个文件。 也就是说在LINUX系统中有一个重要的概念：一切都是文件。其实这是UNIX哲学的一个体现，而Linux是重写UNIX而来，所以这个概念也就传承了下来。在UNIX系统中，把一切资源都看作是文件，包括硬件设备。UNIX系统把每个硬件都看成是一个文件，通常称为设备文件，这样用户就可以用读写文件的方式实现对硬件的访问。 3.2 文件类型与目录结构Linux支持5种文件类型 ： Linux的目录结构如下： Linux文件系统的结构层次鲜明，就像一棵倒立的树，最顶层是其根目录： 常见目录说明： /bin： 存放二进制可执行文件(ls、cat、mkdir等)，常用命令一般都在这里； /etc： 存放系统管理和配置文件； /home： 存放所有用户文件的根目录，是用户主目录的基点，比如用户user的主目录就是/home/user，可以用~user表示； /usr ： 用于存放系统应用程序； /opt： 额外安装的可选应用程序包所放置的位置。一般情况下，我们可以把tomcat等都安装到这里； /proc： 虚拟文件系统目录，是系统内存的映射。可直接访问这个目录来获取系统信息； /root： 超级用户（系统管理员）的主目录（特权阶级^o^）； /sbin: 存放二进制可执行文件，只有root才能访问。这里存放的是系统管理员使用的系统级别的管理命令和程序。如ifconfig等； /dev： 用于存放设备文件； /mnt： 系统管理员安装临时文件系统的安装点，系统提供这个目录是让用户临时挂载其他的文件系统； /boot： 存放用于系统引导时使用的各种文件； /lib ： 存放着和系统运行相关的库文件 ； /tmp： 用于存放各种临时文件，是公用的临时文件存储点； /var： 用于存放运行时需要改变数据的文件，也是某些大文件的溢出区，比方说各种服务的日志文件（系统启动日志等。）等； /lost+found： 这个目录平时是空的，系统非正常关机而留下“无家可归”的文件（windows下叫什么.chk）就在这里。 四 Linux基本命令下面只是给出了一些比较常用的命令。推荐一个Linux命令快查网站，非常不错，大家如果遗忘某些命令或者对某些命令不理解都可以在这里得到解决。 Linux命令大全：http://man.linuxde.net/ 4.1 目录切换命令 cd usr： 切换到该目录下usr目录 cd ..（或cd../）： 切换到上一层目录 cd /： 切换到系统根目录 cd ~： 切换到用户主目录 cd -： 切换到上一个操作所在目录 4.2 目录的操作命令(增删改查) mkdir 目录名称： 增加目录 ls或者ll（ll是ls -l的别名，ll命令可以看到该目录下的所有目录和文件的详细信息）：查看目录信息 find 目录 参数： 寻找目录（查） 示例： 列出当前目录及子目录下所有文件和文件夹: find . 在/home目录下查找以.txt结尾的文件名:find /home -name &quot;*.txt&quot; 同上，但忽略大小写: find /home -iname &quot;*.txt&quot; 当前目录及子目录下查找所有以.txt和.pdf结尾的文件:find . \( -name &quot;*.txt&quot; -o -name &quot;*.pdf&quot; \)或find . -name &quot;*.txt&quot; -o -name &quot;*.pdf&quot; mv 目录名称 新目录名称： 修改目录的名称（改） 注意：mv的语法不仅可以对目录进行重命名而且也可以对各种文件，压缩包等进行 重命名的操作。mv命令用来对文件或目录重新命名，或者将文件从一个目录移到另一个目录中。后面会介绍到mv命令的另一个用法。 mv 目录名称 目录的新位置： 移动目录的位置—剪切（改） 注意：mv语法不仅可以对目录进行剪切操作，对文件和压缩包等都可执行剪切操作。另外mv与cp的结果不同，mv好像文件“搬家”，文件个数并未增加。而cp对文件进行复制，文件个数增加了。 cp -r 目录名称 目录拷贝的目标位置： 拷贝目录（改），-r代表递归拷贝 注意：cp命令不仅可以拷贝目录还可以拷贝文件，压缩包等，拷贝文件和压缩包时不 用写-r递归 rm [-rf] 目录: 删除目录（删） 注意：rm不仅可以删除目录，也可以删除其他文件或压缩包，为了增强大家的记忆， 无论删除任何目录或文件，都直接使用rm -rf 目录/文件/压缩包 4.3 文件的操作命令(增删改查) touch 文件名称: 文件的创建（增） cat/more/less/tail 文件名称 文件的查看（查） cat： 查看显示文件内容 more： 可以显示百分比，回车可以向下一行， 空格可以向下一页，q可以退出查看 less： 可以使用键盘上的PgUp和PgDn向上 和向下翻页，q结束查看 tail-10 ： 查看文件的后10行，Ctrl+C结束 注意：命令 tail -f 文件 可以对某个文件进行动态监控，例如tomcat的日志文件， 会随着程序的运行，日志会变化，可以使用tail -f catalina-2016-11-11.log 监控 文 件的变化 vim 文件： 修改文件的内容（改） vim编辑器是Linux中的强大组件，是vi编辑器的加强版，vim编辑器的命令和快捷方式有很多，但此处不一一阐述，大家也无需研究的很透彻，使用vim编辑修改文件的方式基本会使用就可以了。 在实际开发中，使用vim编辑器主要作用就是修改配置文件，下面是一般步骤： vim 文件——&gt;进入文件—–&gt;命令模式——&gt;按i进入编辑模式—–&gt;编辑文件 ——-&gt;按Esc进入底行模式—–&gt;输入：wq/q! （输入wq代表写入内容并退出，即保存；输入q!代表强制退出不保存。） rm -rf 文件： 删除文件（删） 同目录删除：熟记 rm -rf 文件 即可 4.4 压缩文件的操作命令1）打包并压缩文件： Linux中的打包文件一般是以.tar结尾的，压缩的命令一般是以.gz结尾的。 而一般情况下打包和压缩是一起进行的，打包并压缩后的文件的后缀名一般.tar.gz。命令：tar -zcvf 打包压缩后的文件名 要打包压缩的文件其中： z：调用gzip压缩命令进行压缩 c：打包文件 v：显示运行过程 f：指定文件名 比如：加入test目录下有三个文件分别是：aaa.txt bbb.txt ccc.txt，如果我们要打包test目录并指定压缩后的压缩包名称为test.tar.gz可以使用命令：tar -zcvf test.tar.gz aaa.txt bbb.txt ccc.txt或：tar -zcvf test.tar.gz /test/ 2）解压压缩包： 命令：tar [-xvf] 压缩文件 其中：x：代表解压 示例： 1 将/test下的test.tar.gz解压到当前目录下可以使用命令：tar -xvf test.tar.gz 2 将/test下的test.tar.gz解压到根目录/usr下:tar -xvf xxx.tar.gz -C /usr（- C代表指定解压的位置） 4.5 Linux的权限命令 操作系统中每个文件都拥有特定的权限、所属用户和所属组。权限是操作系统用来限制资源访问的机制，在Linux中权限一般分为读(readable)、写(writable)和执行(excutable)，分为三组。分别对应文件的属主(owner)，属组(group)和其他用户(other)，通过这样的机制来限制哪些用户、哪些组可以对特定的文件进行什么样的操作。通过 ls -l 命令我们可以 查看某个目录下的文件或目录的权限 示例：在随意某个目录下ls -l 第一列的内容的信息解释如下： 下面将详细讲解文件的类型、Linux中权限以及文件有所有者、所在组、其它组具体是什么？ 文件的类型： d： 代表目录 -： 代表文件 l： 代表软链接（可以认为是window中的快捷方式） Linux中权限分为以下几种： r：代表权限是可读，r也可以用数字4表示 w：代表权限是可写，w也可以用数字2表示 x：代表权限是可执行，x也可以用数字1表示 文件和目录权限的区别： 对文件和目录而言，读写执行表示不同的意义。 对于文件： 权限名称 可执行操作 r 可以使用cat查看文件的内容 w 可以修改文件的内容 x 可以将其运行为二进制文件 对于目录： 权限名称 可执行操作 r 可以查看目录下列表 w 可以创建和删除目录下文件 x 可以使用cd进入目录 需要注意的是超级用户可以无视普通用户的权限，即使文件目录权限是000，依旧可以访问。在linux中的每个用户必须属于一个组，不能独立于组外。在linux中每个文件有所有者、所在组、其它组的概念。 所有者 一般为文件的创建者，谁创建了该文件，就天然的成为该文件的所有者，用ls ‐ahl命令可以看到文件的所有者 也可以使用chown 用户名 文件名来修改文件的所有者 。 文件所在组 当某个用户创建了一个文件后，这个文件的所在组就是该用户所在的组 用ls ‐ahl命令可以看到文件的所有组 也可以使用chgrp 组名 文件名来修改文件所在的组。 其它组 除开文件的所有者和所在组的用户外，系统的其它用户都是文件的其它组 我们再来看看如何修改文件/目录的权限。 修改文件/目录的权限的命令：chmod 示例：修改/test下的aaa.txt的权限为属主有全部权限，属主所在的组有读写权限，其他用户只有读的权限 chmod u=rwx,g=rw,o=r aaa.txt 上述示例还可以使用数字表示： chmod 764 aaa.txt 补充一个比较常用的东西: 假如我们装了一个zookeeper，我们每次开机到要求其自动启动该怎么办？ 新建一个脚本zookeeper 为新建的脚本zookeeper添加可执行权限，命令是:chmod +x zookeeper 把zookeeper这个脚本添加到开机启动项里面，命令是：chkconfig --add zookeeper 如果想看看是否添加成功，命令是：chkconfig --list 4.6 Linux 用户管理Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。 用户的账号一方面可以帮助系统管理员对使用系统的用户进行跟踪，并控制他们对系统资源的访问；另一方面也可以帮助用户组织文件，并为用户提供安全性保护。 Linux用户管理相关命令: useradd 选项 用户名:添加用户账号 userdel 选项 用户名:删除用户帐号 usermod 选项 用户名:修改帐号 passwd 用户名:更改或创建用户的密码 passwd -S 用户名 :显示用户账号密码信息 passwd -d 用户名: 清除用户密码 useradd命令用于Linux中创建的新的系统用户。useradd可用来建立用户帐号。帐号建好之后，再用passwd设定帐号的密码．而可用userdel删除帐号。使用useradd指令所建立的帐号，实际上是保存在/etc/passwd文本文件中。 passwd命令用于设置用户的认证信息，包括用户密码、密码过期时间等。系统管理者则能用它管理系统用户的密码。只有管理者可以指定用户名称，一般用户只能变更自己的密码。 4.7 Linux系统用户组的管理每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同，如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。 用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。 Linux系统用户组的管理相关命令: groupadd 选项 用户组 :增加一个新的用户组 groupdel 用户组:要删除一个已有的用户组 groupmod 选项 用户组 : 修改用户组的属性 4.8 其他常用命令 pwd： 显示当前所在位置 grep 要搜索的字符串 要搜索的文件 --color： 搜索命令，–color代表高亮显示 ps -ef/ps -aux： 这两个命令都是查看当前系统正在运行进程，两者的区别是展示格式不同。如果想要查看特定的进程可以使用这样的格式：ps aux|grep redis （查看包括redis字符串的进程），也可使用 pgrep redis -a。 注意：如果直接用ps（（Process Status））命令，会显示所有进程的状态，通常结合grep命令查看某进程的状态。 kill -9 进程的pid： 杀死进程（-9 表示强制终止。） 先用ps查找进程，然后用kill杀掉 网络通信命令： 查看当前系统的网卡信息：ifconfig 查看与某台机器的连接情况：ping 查看当前系统的端口使用：netstat -an net-tools 和 iproute2 ： net-tools起源于BSD的TCP/IP工具箱，后来成为老版本Linux内核中配置网络功能的工具。但自2001年起，Linux社区已经对其停止维护。同时，一些Linux发行版比如Arch Linux和CentOS/RHEL 7则已经完全抛弃了net-tools，只支持iproute2。linux ip命令类似于ifconfig，但功能更强大，旨在替代它。更多详情请阅读如何在Linux中使用IP命令和示例 shutdown： shutdown -h now： 指定现在立即关机；shutdown +5 &quot;System will shutdown after 5 minutes&quot;：指定5分钟后关机，同时送出警告信息给登入用户。 reboot： reboot： 重开机。reboot -w： 做个重开机的模拟（只有纪录并不会真的重开机）。]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Git的理解]]></title>
    <url>%2F2015%2F04%2F09%2Futils-git-Git%2F</url>
    <content type="text"><![CDATA[版本控制什么是版本控制版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。 除了项目源代码，你可以对任何类型的文件进行版本控制。 为什么要版本控制有了它你就可以将某个文件回溯到之前的状态，甚至将整个项目都回退到过去某个时间点的状态，你可以比较文件的变化细节，查出最后是谁修改了哪个地方，从而找出导致怪异问题出现的原因，又是谁在何时报告了某个功能缺陷等等。 本地版本控制系统许多人习惯用复制整个项目目录的方式来保存不同的版本，或许还会改名加上备份时间以示区别。 这么做唯一的好处就是简单，但是特别容易犯错。 有时候会混淆所在的工作目录，一不小心会写错文件或者覆盖意想外的文件。 为了解决这个问题，人们很久以前就开发了许多种本地版本控制系统，大多都是采用某种简单的数据库来记录文件的历次更新差异。 集中化的版本控制系统接下来人们又遇到一个问题，如何让在不同系统上的开发者协同工作？ 于是，集中化的版本控制系统（Centralized Version Control Systems，简称 CVCS）应运而生。 集中化的版本控制系统都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。 这么做虽然解决了本地版本控制系统无法让在不同系统上的开发者协同工作的诟病，但也还是存在下面的问题： 单点故障： 中央服务器宕机，则其他人无法使用；如果中心数据库磁盘损坏有没有进行备份，你将丢失所有数据。本地版本控制系统也存在类似问题，只要整个项目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。 必须联网才能工作： 受网络状况、带宽影响。 分布式版本控制系统于是分布式版本控制系统（Distributed Version Control System，简称 DVCS）面世了。 Git 就是一个典型的分布式版本控制系统。 这类系统，客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。 这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。 因为每一次的克隆操作，实际上都是一次对代码仓库的完整备份。 分布式版本控制系统可以不用联网就可以工作，因为每个人的电脑上都是完整的版本库，当你修改了某个文件后，你只需要将自己的修改推送给别人就可以了。但是，在实际使用分布式版本控制系统的时候，很少会直接进行推送修改，而是使用一台充当“中央服务器”的东西。这个服务器的作用仅仅是用来方便“交换”大家的修改，没有它大家也一样干活，只是交换修改不方便而已。 分布式版本控制系统的优势不单是不必联网这么简单，后面我们还会看到 Git 极其强大的分支管理等功能。 认识 GitGit 简史Linux 内核项目组当时使用分布式版本控制系统 BitKeeper 来管理和维护代码。但是，后来开发 BitKeeper 的商业公司同 Linux 内核开源社区的合作关系结束，他们收回了 Linux 内核社区免费使用 BitKeeper 的权力。 Linux 开源社区（特别是 Linux 的缔造者 Linus Torvalds）基于使用 BitKeeper 时的经验教训，开发出自己的版本系统，而且对新的版本控制系统做了很多改进。 Git 与其他版本管理系统的主要区别 Git 在保存和对待各种信息的时候与其它版本控制系统有很大差异，尽管操作起来的命令形式非常相近，理解这些差异将有助于防止你使用中的困惑。 下面我们主要说一个关于 Git 其他版本管理系统的主要差别：对待数据的方式。 Git采用的是直接记录快照的方式，而非差异比较。我后面会详细介绍这两种方式的差别。 大部分版本控制系统（CVS、Subversion、Perforce、Bazaar 等等）都是以文件变更列表的方式存储信息，这类系统将它们保存的信息看作是一组基本文件和每个文件随时间逐步累积的差异。 具体原理如下图所示，理解起来其实很简单，每个我们对提交更新一个文件之后，系统记录都会记录这个文件做了哪些更新，以增量符号Δ(Delta)表示。 我们怎样才能得到一个文件的最终版本呢？ 很简单，高中数学的基本知识，我们只需要将这些原文件和这些增加进行相加就行了。 这种方式有什么问题呢？ 比如我们的增量特别特别多的话，如果我们要得到最终的文件是不是会耗费时间和性能。 Git 不按照以上方式对待或保存数据。 反之，Git 更像是把数据看作是对小型文件系统的一组快照。 每次你提交更新，或在 Git 中保存项目状态时，它主要对当时的全部文件制作一个快照并保存这个快照的索引。 为了高效，如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件。 Git 对待数据更像是一个 快照流。 Git 的三种状态Git 有三种状态，你的文件可能处于其中之一： 已提交（committed）：数据已经安全的保存在本地数据库中。 已修改（modified）：已修改表示修改了文件，但还没保存到数据库中。 已暂存（staged）：表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。 由此引入 Git 项目的三个工作区域的概念：Git 仓库(.git directoty) **、工作目录(Working Directory)** 以及 暂存区域(Staging Area) 。 基本的 Git 工作流程如下： 在工作目录中修改文件。 暂存文件，将文件的快照放入暂存区域。 提交更新，找到暂存区域的文件，将快照永久性存储到 Git 仓库目录。 Git 使用快速入门获取 Git 仓库有两种取得 Git 项目仓库的方法。 在现有目录中初始化仓库: 进入项目目录运行 git init 命令,该命令将创建一个名为 .git 的子目录。 从一个服务器克隆一个现有的 Git 仓库: git clone [url] 自定义本地仓库的名字: git clone [url] directoryname 记录每次更新到仓库 检测当前文件状态 : git status 提出更改（把它们添加到暂存区）：git add filename (针对特定文件)、git add *(所有文件)、git add *.txt（支持通配符，所有 .txt 文件） 忽略文件：.gitignore 文件 提交更新: git commit -m &quot;代码提交信息&quot; （每次准备提交前，先用 git status 看下，是不是都已暂存起来了， 然后再运行提交命令 git commit） 跳过使用暂存区域更新的方式 : git commit -a -m &quot;代码提交信息&quot;。 git commit 加上 -a 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤。 移除文件 ：git rm filename （从暂存区域移除，然后提交。） 对文件重命名 ：git mv README.md README(这个命令相当于mv README.md README、git rm README.md、git add README 这三条命令的集合) 推送改动到远程仓库 如果你还没有克隆现有仓库，并欲将你的仓库连接到某个远程服务器，你可以使用如下命令添加：·git remote add origin &lt;server&gt; ,比如我们要让本地的一个仓库和 Github 上创建的一个仓库关联可以这样git remote add origin https://github.com/Snailclimb/test.git 将这些改动提交到远端仓库：git push origin master (可以把 master 换成你想要推送的任何分支) 如此你就能够将你的改动推送到所添加的服务器上去了。 远程仓库的移除与重命名 将 test 重命名位 test1：git remote rename test test1 移除远程仓库 test1:git remote rm test1 查看提交历史在提交了若干更新，又或者克隆了某个项目之后，你也许想回顾下提交历史。 完成这个任务最简单而又有效的工具是 git log 命令。git log 会按提交时间列出所有的更新，最近的更新排在最上面。 可以添加一些参数来查看自己希望看到的内容： 只看某个人的提交记录： 1git log --author=bob 撤销操作有时候我们提交完了才发现漏掉了几个文件没有添加，或者提交信息写错了。 此时，可以运行带有 --amend 选项的提交命令尝试重新提交： 1git commit --amend 取消暂存的文件 1git reset filename 撤消对文件的修改: 1git checkout -- filename 假如你想丢弃你在本地的所有改动与提交，可以到服务器上获取最新的版本历史，并将你本地主分支指向它： 12git fetch origingit reset --hard origin/master 分支分支是用来将特性开发绝缘开来的。在你创建仓库的时候，master 是“默认的”分支。在其他分支上进行开发，完成后再将它们合并到主分支上。 我们通常在开发新功能、修复一个紧急 bug 等等时候会选择创建分支。单分支开发好还是多分支开发好，还是要看具体场景来说。 创建一个名字叫做 test 的分支 1git branch test 切换当前分支到 test（当你切换分支的时候，Git 会重置你的工作目录，使其看起来像回到了你在那个分支上最后一次提交的样子。 Git 会自动添加、删除、修改文件以确保此时你的工作目录和这个分支最后一次提交时的样子一模一样） 1git checkout test 你也可以直接这样创建分支并切换过去(上面两条命令的合写) 1git checkout -b feature_x 切换到主分支 1git checkout master 合并分支(可能会有冲突) 1git merge test 把新建的分支删掉 1git branch -d feature_x 将分支推送到远端仓库（推送成功后其他人可见）： 1git push origin]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用 Git 命令清单]]></title>
    <url>%2F2015%2F04%2F09%2Futils-git-i-git%2F</url>
    <content type="text"><![CDATA[仓库# 在当前目录新建一个Git代码库 $ git init # 新建一个目录，将其初始化为Git代码库 $ git init [project-name] # 下载一个项目和它的整个代码历史 $ git clone [url]配置# 显示当前的Git配置 $ git config --list # 编辑Git配置文件 $ git config -e [--global] # 设置提交代码时的用户信息 $ git config [--global] user.name &quot;[name]&quot; $ git config [--global] user.email &quot;[email address]&quot;增加/删除文件# 添加指定文件到暂存区 $ git add [file1] [file2] ... # 添加指定目录到暂存区，包括子目录 $ git add [dir] # 添加当前目录的所有文件到暂存区 $ git add . # 添加每个变化前，都会要求确认 # 对于同一个文件的多处变化，可以实现分次提交 $ git add -p # 删除工作区文件，并且将这次删除放入暂存区 $ git rm [file1] [file2] ... # 停止追踪指定文件，但该文件会保留在工作区 $ git rm --cached [file] # 改名文件，并且将这个改名放入暂存区 $ git mv [file-original] [file-renamed]代码提交# 提交暂存区到仓库区 $ git commit -m [message] # 提交暂存区的指定文件到仓库区 $ git commit [file1] [file2] ... -m [message] # 提交工作区自上次commit之后的变化，直接到仓库区 $ git commit -a # 提交时显示所有diff信息 $ git commit -v # 使用一次新的commit，替代上一次提交 # 如果代码没有任何新变化，则用来改写上一次commit的提交信息 $ git commit --amend -m [message] # 重做上一次commit，并包括指定文件的新变化 $ git commit --amend [file1] [file2] ...分支# 列出所有本地分支 $ git branch # 列出所有远程分支 $ git branch -r # 列出所有本地分支和远程分支 $ git branch -a # 新建一个分支，但依然停留在当前分支 $ git branch [branch-name] # 新建一个分支，并切换到该分支 $ git checkout -b [branch] # 新建一个分支，指向指定commit $ git branch [branch] [commit] # 新建一个分支，与指定的远程分支建立追踪关系 $ git branch --track [branch] [remote-branch] # 切换到指定分支，并更新工作区 $ git checkout [branch-name] # 切换到上一个分支 $ git checkout - # 建立追踪关系，在现有分支与指定的远程分支之间 $ git branch --set-upstream [branch] [remote-branch] # 合并指定分支到当前分支 $ git merge [branch] # 选择一个commit，合并进当前分支 $ git cherry-pick [commit] # 删除分支 $ git branch -d [branch-name] # 删除远程分支 $ git push origin --delete [branch-name] $ git branch -dr [remote/branch]标签# 列出所有tag $ git tag # 新建一个tag在当前commit $ git tag [tag] # 新建一个tag在指定commit $ git tag [tag] [commit] # 删除本地tag $ git tag -d [tag] # 删除远程tag $ git push origin :refs/tags/[tagName] # 查看tag信息 $ git show [tag] # 提交指定tag $ git push [remote] [tag] # 提交所有tag $ git push [remote] --tags # 新建一个分支，指向某个tag $ git checkout -b [branch] [tag]查看信息# 显示有变更的文件 $ git status # 显示当前分支的版本历史 $ git log # 显示commit历史，以及每次commit发生变更的文件 $ git log --stat # 搜索提交历史，根据关键词 $ git log -S [keyword] # 显示某个commit之后的所有变动，每个commit占据一行 $ git log [tag] HEAD --pretty=format:%s # 显示某个commit之后的所有变动，其&quot;提交说明&quot;必须符合搜索条件 $ git log [tag] HEAD --grep feature # 显示某个文件的版本历史，包括文件改名 $ git log --follow [file] $ git whatchanged [file] # 显示指定文件相关的每一次diff $ git log -p [file] # 显示过去5次提交 $ git log -5 --pretty --oneline # 显示所有提交过的用户，按提交次数排序 $ git shortlog -sn # 显示指定文件是什么人在什么时间修改过 $ git blame [file] # 显示暂存区和工作区的差异 $ git diff # 显示暂存区和上一个commit的差异 $ git diff --cached [file] # 显示工作区与当前分支最新commit之间的差异 $ git diff HEAD # 显示两次提交之间的差异 $ git diff [first-branch]...[second-branch] # 显示今天你写了多少行代码 $ git diff --shortstat &quot;@{0 day ago}&quot; # 显示某次提交的元数据和内容变化 $ git show [commit] # 显示某次提交发生变化的文件 $ git show --name-only [commit] # 显示某次提交时，某个文件的内容 $ git show [commit]:[filename] # 显示当前分支的最近几次提交 $ git reflog远程同步# 下载远程仓库的所有变动 $ git fetch [remote] # 显示所有远程仓库 $ git remote -v # 显示某个远程仓库的信息 $ git remote show [remote] # 增加一个新的远程仓库，并命名 $ git remote add [shortname] [url] # 取回远程仓库的变化，并与本地分支合并 $ git pull [remote] [branch] # 上传本地指定分支到远程仓库 $ git push [remote] [branch] # 强行推送当前分支到远程仓库，即使有冲突 $ git push [remote] --force # 推送所有分支到远程仓库 $ git push [remote] --all撤销# 恢复暂存区的指定文件到工作区 $ git checkout [file] # 恢复某个commit的指定文件到暂存区和工作区 $ git checkout [commit] [file] # 恢复暂存区的所有文件到工作区 $ git checkout . # 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 $ git reset [file] # 重置暂存区与工作区，与上一次commit保持一致 $ git reset --hard # 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 $ git reset [commit] # 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 $ git reset --hard [commit] # 重置当前HEAD为指定commit，但保持暂存区和工作区不变 $ git reset --keep [commit] # 新建一个commit，用来撤销指定commit # 后者的所有变化都将被前者抵消，并且应用到当前分支 $ git revert [commit]暂时将未提交的变化移除，稍后再移入$ git stash $ git stash pop其他# 生成一个可供发布的压缩包 $ git archive]]></content>
      <categories>
        <category>utils</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[39-40.NoSQL入门]]></title>
    <url>%2F2014%2F07%2F29%2Flang-python-Python-100-Days-39-40-NoSQL%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[NoSQL入门NoSQL概述如今，大多数的计算机系统（包括服务器、PC、移动设备等）都会产生庞大的数据量。其实，早在2012年的时候，全世界每天产生的数据量就达到了2.5EB（艾字节，$$1EB\approx10^{18}B$$）。这些数据有很大一部分是由关系型数据库来存储和管理的。 早在1970年，E.F.Codd发表了论述关系型数据库的著名论文“A relational model of data for large shared data banks”，这篇文章奠定了关系型数据库的基础并在接下来的数十年时间内产生了深远的影响。实践证明，关系型数据库是实现数据持久化最为重要的方式，它也是大多数应用在选择持久化方案时的首选技术。 NoSQL是一项全新的数据库革命性运动，虽然它的历史可以追溯到1998年，但是NoSQL真正深入人心并得到广泛的应用是在进入大数据时候以后，业界普遍认为NoSQL是更适合大数据存储的技术方案，这才使得NoSQL的发展达到了前所未有的高度。2012年《纽约时报》的一篇专栏中写到，大数据时代已经降临，在商业、经济及其他领域中，决策将不再基于经验和直觉而是基于数据和分析而作出。事实上，在天文学、气象学、基因组学、生物学、社会学、互联网搜索引擎、金融、医疗、社交网络、电子商务等诸多领域，由于数据过于密集和庞大，在数据的分析和处理上也遇到了前所未有的限制和阻碍，这一切都使得对大数据处理技术的研究被提升到了新的高度，也使得各种NoSQL的技术方案进入到了公众的视野。 NoSQL数据库按照其存储类型可以大致分为以下几类： 类型 部分代表 特点 列族数据库 HBaseCassandraHypertable 顾名思义是按列存储数据的。最大的特点是方便存储结构化和半结构化数据，方便做数据压缩，对针对某一列或者某几列的查询有非常大的I/O优势，适合于批量数据处理和即时查询。 文档数据库 MongoDBCouchDBElasticSearch 文档数据库一般用类JSON格式存储数据，存储的内容是文档型的。这样也就有机会对某些字段建立索引，实现关系数据库的某些功能，但不提供对参照完整性和分布事务的支持。 KV数据库 DynamoDBRedisLevelDB 可以通过key快速查询到其value，有基于内存和基于磁盘两种实现方案。 图数据库 Neo4JFlockDBJanusGraph 使用图结构进行语义查询的数据库，它使用节点、边和属性来表示和存储数据。图数据库从设计上，就可以简单快速的检索难以在关系系统中建模的复杂层次结构。 对象数据库 db4oVersant 通过类似面向对象语言的语法操作数据库，通过对象的方式存取数据。 说明：想了解更多的NoSQL数据库，可以访问http://nosql-database.org/。 Redis概述Redis是一种基于键值对的NoSQL数据库，它提供了对多种数据类型（字符串、哈希、列表、集合、有序集合、位图等）的支持，能够满足很多应用场景的需求。Redis将数据放在内存中，因此读写性能是非常惊人的。与此同时，Redis也提供了持久化机制，能够将内存中的数据保存到硬盘上，在发生意外状况时数据也不会丢掉。此外，Redis还支持键过期、地理信息运算、发布订阅、事务、管道、Lua脚本扩展等功能，总而言之，Redis的功能和性能都非常强大，如果项目中要实现高速缓存和消息队列这样的服务，直接交给Redis就可以了。目前，国内外很多著名的企业和商业项目都使用了Redis，包括：Twitter、Github、StackOverflow、新浪微博、百度、优酷土豆、美团、小米、唯品会等。 Redis简介2008年，一个名为Salvatore Sanfilippo的程序员为他开发的LLOOGG项目定制了专属的数据库（因为之前他无论怎样优化MySQL，系统性能已经无法再提升了），这项工作的成果就是Redis的初始版本。后来他将Redis的代码放到了全球最大的代码托管平台Github，从那以后，Redis引发了大量开发者的好评和关注，继而有数百人参与了Redis的开发和维护，这使得Redis的功能越来越强大和性能越来越好。 Redis是REmote DIctionary Server的缩写，它是一个用ANSI C编写的高性能的key-value存储系统，与其他的key-value存储系统相比，Redis有以下一些特点（也是优点）： Redis的读写性能极高，并且有丰富的特性（发布/订阅、事务、通知等）。 Redis支持数据的持久化（RDB和AOF两种方式），可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 Redis支持多种数据类型，包括：string、hash、list、set，zset、bitmap、hyperloglog等。 Redis支持主从复制（实现读写分析）以及哨兵模式（监控master是否宕机并自动调整配置）。 Redis支持分布式集群，可以很容易的通过水平扩展来提升系统的整体性能。 Redis基于TCP提供的可靠传输服务进行通信，很多编程语言都提供了Redis客户端支持。 Redis的应用场景 高速缓存 - 将不常变化但又经常被访问的热点数据放到Redis数据库中，可以大大降低关系型数据库的压力，从而提升系统的响应性能。 排行榜 - 很多网站都有排行榜功能，利用Redis中的列表和有序集合可以非常方便的构造各种排行榜系统。 商品秒杀/投票点赞 - Redis提供了对计数操作的支持，网站上常见的秒杀、点赞等功能都可以利用Redis的计数器通过+1或-1的操作来实现，从而避免了使用关系型数据的update操作。 分布式锁 - 利用Redis可以跨多台服务器实现分布式锁（类似于线程锁，但是能够被多台机器上的多个线程或进程共享）的功能，用于实现一个阻塞式操作。 消息队列 - 消息队列和高速缓存一样，是一个大型网站不可缺少的基础服务，可以实现业务解耦和非实时业务削峰等特性，这些我们都会在后面的项目中为大家展示。 Redis的安装和配置可以使用Linux系统的包管理工具（如yum）来安装Redis，也可以通过在Redis的官方网站下载Redis的源代码，解压缩解归档之后通过make工具对源代码进行构建并安装，在更新这篇文档时，Redis官方提供的最新稳定版本是Redis 5.0.4。 12345wget http://download.redis.io/releases/redis-5.0.4.tar.gzgunzip redis-5.0.4.tar.gztar -xvf redis-5.0.4.tarcd redis-5.0.4make &amp;&amp; make install 在redis源代码目录下有一个名为redis.conf的配置文件，我们可以先查看一下该文件。 1vim redis.conf 配置将Redis服务绑定到指定的IP地址和端口。 配置底层有多少个数据库。 配置Redis的持久化机制 - RDB。 配置Redis的持久化机制 - AOF。 配置访问Redis服务器的验证口令。 配置Redis的主从复制，通过主从复制可以实现读写分离。 配置慢查询。 上面这些内容就是Redis的基本配置，如果你对上面的内容感到困惑也没有关系，先把Redis用起来再回头去推敲这些内容就行了。如果想找一些参考书，《Redis开发与运维》是一本不错的入门读物，而《Redis实战》是不错的进阶读物。 Redis的服务器和客户端接下来启动Redis服务器，下面的方式将以默认的配置启动Redis服务。 1redis-server 如果希望修改Redis的配置（如端口、认证口令、持久化方式等），可以通过下面两种方式。 方式一：通过参数指定认证口令和AOF持久化方式。 1redis-server --requirepass 1qaz2wsx --appendonly yes 方式二：通过指定的配置文件来修改Redis的配置。 1redis-server /root/redis-5.0.4/redis.conf 下面我们使用第一种方式来启动Redis并将其置于后台运行，将Redis产生的输出重定向到名为redis.log的文件中。 1redis-server --requirepass 1qaz2wsx &gt; redis.log &amp; 可以通过ps或者netstat来检查Redis服务器是否启动成功。 12ps -ef | grep redis-servernetstat -nap | grep redis-server 接下来，我们尝试用Redis客户端去连接服务器。 123456redis-cli127.0.0.1:6379&gt; auth 1qaz2wsxOK127.0.0.1:6379&gt; pingPONG127.0.0.1:6379&gt; Redis有着非常丰富的数据类型，也有很多的命令来操作这些数据，具体的内容可以查看Redis命令参考，在这个网站上，除了Redis的命令参考，还有Redis的详细文档，其中包括了通知、事务、主从复制、持久化、哨兵、集群等内容。 说明：上面的插图来自付磊和张益军先生编著的《Redis开发与运维》一书。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114127.0.0.1:6379&gt; set username adminOK127.0.0.1:6379&gt; get username"admin"127.0.0.1:6379&gt; set password "123456" ex 300OK127.0.0.1:6379&gt; get password"123456"127.0.0.1:6379&gt; ttl username(integer) -1127.0.0.1:6379&gt; ttl password(integer) 286127.0.0.1:6379&gt; hset stu1 name hao(integer) 0127.0.0.1:6379&gt; hset stu1 age 38(integer) 1127.0.0.1:6379&gt; hset stu1 gender male(integer) 1127.0.0.1:6379&gt; hgetall stu11) "name"2) "hao"3) "age"4) "38"5) "gender"6) "male"127.0.0.1:6379&gt; hvals stu11) "hao"2) "38"3) "male"127.0.0.1:6379&gt; hmset stu2 name wang age 18 gender female tel 13566778899OK127.0.0.1:6379&gt; hgetall stu21) "name"2) "wang"3) "age"4) "18"5) "gender"6) "female"7) "tel"8) "13566778899"127.0.0.1:6379&gt; lpush nums 1 2 3 4 5(integer) 5127.0.0.1:6379&gt; lrange nums 0 -11) "5"2) "4"3) "3"4) "2"5) "1"127.0.0.1:6379&gt; lpop nums"5"127.0.0.1:6379&gt; lpop nums"4"127.0.0.1:6379&gt; rpop nums"1"127.0.0.1:6379&gt; rpop nums"2"127.0.0.1:6379&gt; sadd fruits apple banana orange apple grape grape(integer) 4127.0.0.1:6379&gt; scard fruits(integer) 4127.0.0.1:6379&gt; smembers fruits1) "grape"2) "orange"3) "banana"4) "apple"127.0.0.1:6379&gt; sismember fruits apple(integer) 1127.0.0.1:6379&gt; sismember fruits durian(integer) 0127.0.0.1:6379&gt; sadd nums1 1 2 3 4 5(integer) 5127.0.0.1:6379&gt; sadd nums2 2 4 6 8(integer) 4127.0.0.1:6379&gt; sinter nums1 nums21) "2"2) "4"127.0.0.1:6379&gt; sunion nums1 nums21) "1"2) "2"3) "3"4) "4"5) "5"6) "6"7) "8"127.0.0.1:6379&gt; sdiff nums1 nums21) "1"2) "3"3) "5"127.0.0.1:6379&gt; zadd topsinger 5234 zhangxy 1978 chenyx 2235 zhoujl 3520 xuezq(integer) 4127.0.0.1:6379&gt; zrange topsinger 0 -1 withscores1) "chenyx"2) "1978"3) "zhoujl"4) "2235"5) "xuezq"6) "3520"7) "zhangxy"8) "5234"127.0.0.1:6379&gt; zrevrange topsinger 0 -11) "zhangxy"2) "xuezq"3) "zhoujl"4) "chenyx"127.0.0.1:6379&gt; geoadd pois 116.39738549206541 39.90862689286386 tiananmen 116.27172936413572 39.99135172904494 yiheyuan 117.27766503308104 40.65332064313784 gubeishuizhen(integer) 3127.0.0.1:6379&gt; geodist pois tiananmen gubeishuizhen km"111.5333"127.0.0.1:6379&gt; geodist pois tiananmen yiheyuan km"14.1230"127.0.0.1:6379&gt; georadius pois 116.86499108288572 40.40149669363615 50 km withdist1) 1) "gubeishuizhen" 2) "44.7408" 在Python程序中使用Redis可以使用pip安装redis模块。redis模块的核心是名为Redis的类，该类的对象代表一个Redis客户端，通过该客户端可以向Redis服务器发送命令并获取执行的结果。上面我们在Redis客户端中使用的命令基本上就是Redis对象可以接收的消息，所以如果了解了Redis的命令就可以在Python中玩转Redis。 12pip3 install redispython3 1234567891011121314&gt;&gt;&gt; import redis&gt;&gt;&gt; client = redis.Redis(host='1.2.3.4', port=6379, password='1qaz2wsx')&gt;&gt;&gt; client.set('username', 'admin')True&gt;&gt;&gt; client.hset('student', 'name', 'hao')1&gt;&gt;&gt; client.hset('student', 'age', 38)1&gt;&gt;&gt; client.keys('*')[b'username', b'student']&gt;&gt;&gt; client.get('username')b'admin'&gt;&gt;&gt; client.hgetall('student')&#123;b'name': b'hao', b'age': b'38'&#125; MongoDB概述MongoDB简介MongoDB是2009年问世的一个面向文档的数据库管理系统，由C++语言编写，旨在为Web应用提供可扩展的高性能数据存储解决方案。虽然在划分类别的时候后，MongoDB被认为是NoSQL的产品，但是它更像一个介于关系数据库和非关系数据库之间的产品，在非关系数据库中它功能最丰富，最像关系数据库。 MongoDB将数据存储为一个文档，一个文档由一系列的“键值对”组成，其文档类似于JSON对象，但是MongoDB对JSON进行了二进制处理（能够更快的定位key和value），因此其文档的存储格式称为BSON。关于JSON和BSON的差别大家可以看看MongoDB官方网站的文章《JSON and BSON》。 目前，MongoDB已经提供了对Windows、MacOS、Linux、Solaris等多个平台的支持，而且也提供了多种开发语言的驱动程序，Python当然是其中之一。 MongoDB的安装和配置可以从MongoDB的官方下载链接下载MongoDB，官方为Windows系统提供了一个Installer程序，而Linux和MacOS则提供了压缩文件。下面简单说一下Linux系统如何安装和配置MongoDB。 1234567891011121314wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-amazon-3.6.5.tgzgunzip mongodb-linux-x86_64-amazon-3.6.5.tgzmkdir mongodb-3.6.5tar -xvf mongodb-linux-x86_64-amazon-3.6.5.tar --strip-components 1 -C mongodb-3.6.5/export PATH=$PATH:~/mongodb-3.6.5/binmkdir -p /data/dbmongod --bind_ip 172.18.61.2502018-06-03T18:03:28.232+0800 I CONTROL [initandlisten] MongoDB starting : pid=1163 port=27017 dbpath=/data/db 64-bit host=iZwz97tbgo9lkabnat2lo8Z2018-06-03T18:03:28.232+0800 I CONTROL [initandlisten] db version v3.6.52018-06-03T18:03:28.232+0800 I CONTROL [initandlisten] git version: a20ecd3e3a174162052ff99913bc2ca9a839d6182018-06-03T18:03:28.232+0800 I CONTROL [initandlisten] OpenSSL version: OpenSSL 1.0.0-fips29 Mar 2010...2018-06-03T18:03:28.945+0800 I NETWORK [initandlisten] waiting for connections on port 27017 说明：上面的操作中，export命令是设置PATH环境变量，这样可以在任意路径下执行mongod来启动MongoDB服务器。MongoDB默认保存数据的路径是/data/db目录，为此要提前创建该目录。此外，在使用mongod启动MongoDB服务器时，–bind_ip参数用来将服务绑定到指定的IP地址，也可以用–port参数来指定端口，默认端口为27017。 MongoDB基本概念我们通过与关系型数据库进行对照的方式来说明MongoDB中的一些概念。 SQL MongoDB 解释（SQL/MongoDB） database database 数据库/数据库 table collection 二维表/集合 row document 记录（行）/文档 column field 字段（列）/域 index index 索引/索引 table joins — 表连接/嵌套文档 primary key primary key 主键/主键（_id字段） 通过Shell操作MongoDB启动服务器后可以使用交互式环境跟服务器通信，如下所示。 1234mongo --host 172.18.61.250MongoDB shell version v3.6.5connecting to: mongodb://172.18.61.250:27017/ 查看、创建和删除数据库。 123456789101112&gt; // 显示所有数据库&gt; show dbsadmin 0.000GBconfig 0.000GBlocal 0.000GB&gt; // 创建并切换到school数据库&gt; use schoolswitched to db school&gt; // 删除当前数据库&gt; db.dropDatabase()&#123; "ok" : 1 &#125;&gt; 创建、删除和查看集合。 1234567891011121314151617&gt; // 创建并切换到school数据库&gt; use schoolswitched to db school&gt; // 创建colleges集合&gt; db.createCollection('colleges')&#123; "ok" : 1 &#125;&gt; // 创建students集合&gt; db.createCollection('students')&#123; "ok" : 1 &#125;&gt; // 查看所有集合&gt; show collectionscollegesstudents&gt; // 删除colleges集合&gt; db.colleges.drop()true&gt; 说明：在MongoDB中插入文档时如果集合不存在会自动创建集合，所以也可以按照下面的方式通过创建文档来创建集合。 文档的CRUD操作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192&gt; // 向students集合插入文档&gt; db.students.insert(&#123;stuid: 1001, name: '骆昊', age: 38&#125;)WriteResult(&#123; "nInserted" : 1 &#125;)&gt; // 向students集合插入文档&gt; db.students.save(&#123;stuid: 1002, name: '王大锤', tel: '13012345678', gender: '男'&#125;)WriteResult(&#123; "nInserted" : 1 &#125;)&gt; // 查看所有文档&gt; db.students.find()&#123; "_id" : ObjectId("5b13c72e006ad854460ee70b"), "stuid" : 1001, "name" : "骆昊", "age" : 38 &#125;&#123; "_id" : ObjectId("5b13c790006ad854460ee70c"), "stuid" : 1002, "name" : "王大锤", "tel" : "13012345678", "gender" : "男" &#125;&gt; // 更新stuid为1001的文档&gt; db.students.update(&#123;stuid: 1001&#125;, &#123;'$set': &#123;tel: '13566778899', gender: '男'&#125;&#125;)WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)&gt; // 插入或更新stuid为1003的文档&gt; db.students.update(&#123;stuid: 1003&#125;, &#123;'$set': &#123;name: '白元芳', tel: '13022223333', gender: '男'&#125;&#125;, upsert=true)WriteResult(&#123; "nMatched" : 0, "nUpserted" : 1, "nModified" : 0, "_id" : ObjectId("5b13c92dd185894d7283efab")&#125;)&gt; // 查询所有文档&gt; db.students.find().pretty()&#123; "_id" : ObjectId("5b13c72e006ad854460ee70b"), "stuid" : 1001, "name" : "骆昊", "age" : 38, "gender" : "男", "tel" : "13566778899"&#125;&#123; "_id" : ObjectId("5b13c790006ad854460ee70c"), "stuid" : 1002, "name" : "王大锤", "tel" : "13012345678", "gender" : "男"&#125;&#123; "_id" : ObjectId("5b13c92dd185894d7283efab"), "stuid" : 1003, "gender" : "男", "name" : "白元芳", "tel" : "13022223333"&#125;&gt; // 查询stuid大于1001的文档&gt; db.students.find(&#123;stuid: &#123;'$gt': 1001&#125;&#125;).pretty()&#123; "_id" : ObjectId("5b13c790006ad854460ee70c"), "stuid" : 1002, "name" : "王大锤", "tel" : "13012345678", "gender" : "男"&#125;&#123; "_id" : ObjectId("5b13c92dd185894d7283efab"), "stuid" : 1003, "gender" : "男", "name" : "白元芳", "tel" : "13022223333"&#125;&gt; // 查询stuid大于1001的文档只显示name和tel字段&gt; db.students.find(&#123;stuid: &#123;'$gt': 1001&#125;&#125;, &#123;_id: 0, name: 1, tel: 1&#125;).pretty()&#123; "name" : "王大锤", "tel" : "13012345678" &#125;&#123; "name" : "白元芳", "tel" : "13022223333" &#125;&gt; // 查询name为“骆昊”或者tel为“13022223333”的文档&gt; db.students.find(&#123;'$or': [&#123;name: '骆昊'&#125;, &#123;tel: '13022223333'&#125;]&#125;, &#123;_id: 0, name: 1, tel: 1&#125;).pretty()&#123; "name" : "骆昊", "tel" : "13566778899" &#125;&#123; "name" : "白元芳", "tel" : "13022223333" &#125;&gt; // 查询学生文档跳过第1条文档只查1条文档&gt; db.students.find().skip(1).limit(1).pretty()&#123; "_id" : ObjectId("5b13c790006ad854460ee70c"), "stuid" : 1002, "name" : "王大锤", "tel" : "13012345678", "gender" : "男"&#125;&gt; // 对查询结果进行排序(1表示升序，-1表示降序)&gt; db.students.find(&#123;&#125;, &#123;_id: 0, stuid: 1, name: 1&#125;).sort(&#123;stuid: -1&#125;)&#123; "stuid" : 1003, "name" : "白元芳" &#125;&#123; "stuid" : 1002, "name" : "王大锤" &#125;&#123; "stuid" : 1001, "name" : "骆昊" &#125;&gt; // 在指定的一个或多个字段上创建索引&gt; db.students.ensureIndex(&#123;name: 1&#125;)&#123; "createdCollectionAutomatically" : false, "numIndexesBefore" : 1, "numIndexesAfter" : 2, "ok" : 1&#125;&gt; 使用MongoDB可以非常方便的配置数据复制，通过冗余数据来实现数据的高可用以及灾难恢复，也可以通过数据分片来应对数据量迅速增长的需求。关于MongoDB更多的操作可以查阅官方文档 ，同时推荐大家阅读Kristina Chodorow写的《MongoDB权威指南》。 在Python程序中操作MongoDB可以通过pip安装pymongo来实现对MongoDB的操作。 12pip3 install pymongopython3 12345678910111213141516171819202122232425262728293031323334353637383940414243&gt;&gt;&gt; from pymongo import MongoClient&gt;&gt;&gt; client = MongoClient('mongodb://127.0.0.1:27017') &gt;&gt;&gt; db = client.school&gt;&gt;&gt; for student in db.students.find():... print('学号:', student['stuid'])... print('姓名:', student['name'])... print('电话:', student['tel'])... 学号: 1001.0姓名: 骆昊电话: 13566778899学号: 1002.0姓名: 王大锤电话: 13012345678学号: 1003.0姓名: 白元芳电话: 13022223333&gt;&gt;&gt; db.students.find().count()3&gt;&gt;&gt; db.students.remove()&#123;'n': 3, 'ok': 1.0&#125;&gt;&gt;&gt; db.students.find().count()0&gt;&gt;&gt; coll = db.students&gt;&gt;&gt; from pymongo import ASCENDING&gt;&gt;&gt; coll.create_index([('name', ASCENDING)], unique=True)'name_1'&gt;&gt;&gt; coll.insert_one(&#123;'stuid': int(1001), 'name': '骆昊', 'gender': True&#125;)&lt;pymongo.results.InsertOneResult object at 0x1050cc6c8&gt;&gt;&gt;&gt; coll.insert_many([&#123;'stuid': int(1002), 'name': '王大锤', 'gender': False&#125;, &#123;'stuid': int(1003), 'name': '白元芳', 'gender': True&#125;])&lt;pymongo.results.InsertManyResult object at 0x1050cc8c8&gt;&gt;&gt;&gt; for student in coll.find(&#123;'gender': True&#125;):... print('学号:', student['stuid'])... print('姓名:', student['name'])... print('性别:', '男' if student['gender'] else '女')... 学号: 1001姓名: 骆昊性别: 男学号: 1003姓名: 白元芳性别: 男&gt;&gt;&gt; 关于PyMongo更多的知识可以通过它的官方文档进行了解，也可以使用MongoEngine这样的库来简化Python程序对MongoDB的操作，除此之外，还有以异步I/O方式访问MongoDB的三方库motor都是不错的选择。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>Python</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[36-38.关系数据库入门]]></title>
    <url>%2F2014%2F07%2F29%2Flang-python-Python-100-Days-36-38-%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%2F</url>
    <content type="text"><![CDATA[关系数据库入门关系数据库概述 数据持久化 - 将数据保存到能够长久保存数据的存储介质中，在掉电的情况下数据也不会丢失。 数据库发展史 - 网状数据库、层次数据库、关系数据库、NoSQL数据库。 1970年，IBM的研究员E.F.Codd在Communication of the ACM上发表了名为A Relational Model of Data for Large Shared Data Banks的论文，提出了关系模型的概念，奠定了关系模型的理论基础。后来Codd又陆续发表多篇文章，论述了范式理论和衡量关系系统的12条标准，用数学理论奠定了关系数据库的基础。 关系数据库特点。 理论基础：集合论和关系代数。 具体表象：用二维表（有行和列）组织数据。 编程语言：结构化查询语言（SQL）。 ER模型（实体关系模型）和概念模型图。 ER模型，全称为实体关系模型（Entity-Relationship Model），由美籍华裔计算机科学家陈品山先生提出，是概念数据模型的高层描述方式，如下图所示。 实体 - 矩形框 属性 - 椭圆框 关系 - 菱形框 重数 - 1:1（一对一） / 1:N（一对多） / M:N（多对多） 实际项目开发中，我们可以利用数据库建模工具（如：PowerDesigner）来绘制概念数据模型（其本质就是ER模型），然后再设置好目标数据库系统，将概念模型转换成物理模型，最终生成创建二维表的SQL（很多工具都可以根据我们设计的物理模型图以及设定的目标数据库来导出SQL或直接生成数据表）。 关系数据库产品。 Oracle - 目前世界上使用最为广泛的数据库管理系统，作为一个通用的数据库系统，它具有完整的数据管理功能；作为一个关系数据库，它是一个完备关系的产品；作为分布式数据库，它实现了分布式处理的功能。在Oracle最新的12c版本中，还引入了多承租方架构，使用该架构可轻松部署和管理数据库云。 DB2 - IBM公司开发的、主要运行于Unix（包括IBM自家的AIX）、Linux、以及Windows服务器版等系统的关系数据库产品。DB2历史悠久且被认为是最早使用SQL的数据库产品，它拥有较为强大的商业智能功能。 SQL Server - 由Microsoft开发和推广的关系型数据库产品，最初适用于中小企业的数据管理，但是近年来它的应用范围有所扩展，部分大企业甚至是跨国公司也开始基于它来构建自己的数据管理系统。 MySQL - MySQL是开放源代码的，任何人都可以在GPL（General Public License）的许可下下载并根据个性化的需要对其进行修改。MySQL因为其速度、可靠性和适应性而备受关注。 PostgreSQL - 在BSD许可证下发行的开放源代码的关系数据库产品。 MySQL简介MySQL最早是由瑞典的MySQL AB公司开发的一个开放源码的关系数据库管理系统，该公司于2008年被昇阳微系统公司（Sun Microsystems）收购。在2009年，甲骨文公司（Oracle）收购昇阳微系统公司，因此在这之后MySQL成为了Oracle旗下产品。 MySQL在过去由于性能高、成本低、可靠性好，已经成为最流行的开源数据库，因此被广泛地应用于中小型网站开发。随着MySQL的不断成熟，它也逐渐被应用于更多大规模网站和应用，比如维基百科、谷歌（Google）、脸书（Facebook）、淘宝网等网站都使用了MySQL来提供数据持久化服务。 甲骨文公司收购后昇阳微系统公司，大幅调涨MySQL商业版的售价，且甲骨文公司不再支持另一个自由软件项目OpenSolaris的发展，因此导致自由软件社区对于Oracle是否还会持续支持MySQL社区版（MySQL的各个发行版本中唯一免费的版本）有所担忧，MySQL的创始人麦克尔·维德纽斯以MySQL为基础，成立分支计划MariaDB（以他女儿的名字命名的数据库）。有许多原来使用MySQL数据库的公司（例如：维基百科）已经陆续完成了从MySQL数据库到MariaDB数据库的迁移。 安装和配置 说明：下面的安装和配置都是以CentOS Linux环境为例，如果需要在其他系统下安装MySQL，读者可以自行在网络上查找对应的安装教程）。 刚才说过，MySQL有一个分支版本名叫MariaDB，该数据库旨在继续保持MySQL数据库在GNU GPL下开源。如果要使用MariaDB作为MySQL的替代品，可以使用下面的命令进行安装。 1yum install mariadb mariadb-server 如果要安装官方版本的MySQL，可以在MySQL官方网站下载安装文件。首先在下载页面中选择平台和版本，然后找到对应的下载链接。下面以MySQL 5.7.26版本和Red Hat Enterprise Linux为例，直接下载包含所有安装文件的归档文件，解归档之后通过包管理工具进行安装。 12wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.26-1.el7.x86_64.rpm-bundle.tartar -xvf mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar 如果系统上有MariaDB相关的文件，需要先移除MariaDB相关的文件。 1yum list installed | grep mariadb | awk '&#123;print $1&#125;' | xargs yum erase -y 接下来可以按照如下所示的顺序用RPM（Redhat Package Manager）工具安装MySQL。 1234rpm -ivh mysql-community-common-5.7.26-1.el7.x86_64.rpmrpm -ivh mysql-community-libs-5.7.26-1.el7.x86_64.rpmrpm -ivh mysql-community-client-5.7.26-1.el7.x86_64.rpmrpm -ivh mysql-community-server-5.7.26-1.el7.x86_64.rpm 可以使用下面的命令查看已经安装的MySQL相关的包。 1rpm -qa | grep mysql 配置MySQL。 MySQL的配置文件在/etc目录下，名为my.cnf，默认的配置文件内容如下所示。如果对这个文件不理解并没有关系，什么时候用到这个配置文件什么时候再了解它就行了。 1cat /etc/my.cnf 123456789101112131415161718192021222324252627# For advice on how to change settings please see# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html[mysqld]## Remove leading # and set to the amount of RAM for the most important data# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.# innodb_buffer_pool_size = 128M## Remove leading # to turn on a very important data integrity option: logging# changes to the binary log between backups.# log_bin## Remove leading # to set options mainly useful for reporting servers.# The server defaults are faster for transactions and fast SELECTs.# Adjust sizes as needed, experiment to find the optimal values.# join_buffer_size = 128M# sort_buffer_size = 2M# read_rnd_buffer_size = 2Mdatadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sock# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid 启动MySQL服务。 可以使用下面的命令来启动MySQL。 1service mysqld start 在CentOS 7中，更推荐使用下面的命令来启动MySQL。 1systemctl start mysqld 启动MySQL成功后，可以通过下面的命令来检查网络端口使用情况，MySQL默认使用3306端口。 1netstat -ntlp | grep mysql 也可以使用下面的命令查找是否有名为mysqld的进程。 1pgrep mysqld 使用MySQL客户端工具连接服务器。 命令行工具： 1mysql -u root -p 说明：启动客户端时，-u参数用来指定用户名，MySQL默认的超级管理账号为root；-p表示要输入密码（用户口令）；如果连接的是其他主机而非本机，可以用-h来指定连接主机的主机名或IP地址。 如果是首次安装MySQL，可以使用下面的命令来找到默认的初始密码。 1cat /var/log/mysqld.log | grep password 上面的命令会查看MySQL的日志带有password的行，在显示的结果中root@localhost:后面的部分就是默认设置的初始密码。 修改超级管理员（root）的访问口令为123456。 123set global validate_password_policy=0;set global validate_password_length=6;alter user 'root'@'localhost' identified by '123456'; 说明：MySQL较新的版本默认不允许使用弱口令作为用户口令，所以我们通过上面的前两条命令修改了验证用户口令的策略和口令的长度。事实上我们不应该使用弱口令，因为存在用户口令被暴力破解的风险。近年来，攻击数据库窃取数据和劫持数据库勒索比特币的事件屡见不鲜，要避免这些潜在的风险，最为重要的一点是不要让数据库服务器暴露在公网上（最好的做法是将数据库置于内网，至少要做到不向公网开放数据库服务器的访问端口），另外要保管好root账号的口令，应用系统需要访问数据库时，通常不使用root账号进行访问，而是创建其他拥有适当权限的账号来访问。 再次使用客户端工具连接MySQL服务器时，就可以使用新设置的口令了。在实际开发中，为了方便用户操作，可以选择图形化的客户端工具来连接MySQL服务器，包括： MySQL Workbench（官方提供的工具） Navicat for MySQL（界面简单优雅，功能直观强大） SQLyog for MySQL（强大的MySQL数据库管理员工具） 常用命令。 查看服务器版本。 1select version(); 查看所有数据库。 1show databases; 切换到指定数据库。 1use mysql; 查看数据库下所有表。 1show tables; 获取帮助。 1234567? contents;? functions;? numeric functions;? round;? data types;? longblob; SQL详解基本操作我们通常可以将SQL分为三类：DDL（数据定义语言）、DML（数据操作语言）和DCL（数据控制语言）。DDL主要用于创建（create）、删除（drop）、修改（alter）数据库中的对象，比如创建、删除和修改二维表；DML主要负责插入数据（insert）、删除数据（delete）、更新数据（update）和查询（select）；DCL通常用于授予权限（grant）和召回权限（revoke）。 说明：SQL是不区分大小写的语言，为了书写方便，下面的SQL都使用了小写字母来书写。 DDL（数据定义语言） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566-- 如果存在名为school的数据库就删除它drop database if exists school;-- 创建名为school的数据库并设置默认的字符集和排序方式create database school default charset utf8 collate utf8_bin;-- 切换到school数据库上下文环境use school;-- 创建学院表create table tb_college(collid int auto_increment comment '编号',collname varchar(50) not null comment '名称',collmaster varchar(20) not null comment '院长',primary key (collid));-- 创建学生表create table tb_student(stuid int not null comment '学号',stuname varchar(20) not null comment '姓名',stusex boolean default 1 comment '性别',stubirth date not null comment '出生日期',stuaddr varchar(255) default '' comment '籍贯',collid int not null comment '所属学院',primary key (stuid),foreign key (collid) references tb_college (collid));-- 创建教师表create table tb_teacher(teaid int not null comment '工号',teaname varchar(20) not null comment '姓名',teatitle varchar(10) default '助教' comment '职称',collid int not null comment '所属学院',primary key (teaid),foreign key (collid) references tb_college (collid));-- 创建课程表create table tb_course(couid int not null comment '编号',couname varchar(50) not null comment '名称',coucredit int not null comment '学分',teaid int not null comment '授课老师',primary key (couid),foreign key (teaid) references tb_teacher (teaid));-- 创建选课记录表create table tb_record(recid int auto_increment comment '选课记录编号',sid int not null comment '选课学生',cid int not null comment '所选课程',seldate datetime default now() comment '选课时间日期',score decimal(4,1) comment '考试成绩',primary key (recid),foreign key (sid) references tb_student (stuid),foreign key (cid) references tb_course (couid),unique (sid, cid)); 上面的DDL有几个地方需要强调一下： 创建数据库时，我们通过default charset utf8指定了数据库默认使用的字符集，我们推荐使用该字符集，因为utf8能够支持国际化编码。如果将来数据库中用到的字符可能包括类似于Emoji这样的图片字符，也可以将默认字符集设定为utf8mb4（最大4字节的utf-8编码）。查看MySQL支持的字符集可以执行下面的语句。 1show character set; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546+----------+---------------------------------+---------------------+--------+| Charset | Description | Default collation | Maxlen |+----------+---------------------------------+---------------------+--------+| big5 | Big5 Traditional Chinese | big5_chinese_ci | 2 || dec8 | DEC West European | dec8_swedish_ci | 1 || cp850 | DOS West European | cp850_general_ci | 1 || hp8 | HP West European | hp8_english_ci | 1 || koi8r | KOI8-R Relcom Russian | koi8r_general_ci | 1 || latin1 | cp1252 West European | latin1_swedish_ci | 1 || latin2 | ISO 8859-2 Central European | latin2_general_ci | 1 || swe7 | 7bit Swedish | swe7_swedish_ci | 1 || ascii | US ASCII | ascii_general_ci | 1 || ujis | EUC-JP Japanese | ujis_japanese_ci | 3 || sjis | Shift-JIS Japanese | sjis_japanese_ci | 2 || hebrew | ISO 8859-8 Hebrew | hebrew_general_ci | 1 || tis620 | TIS620 Thai | tis620_thai_ci | 1 || euckr | EUC-KR Korean | euckr_korean_ci | 2 || koi8u | KOI8-U Ukrainian | koi8u_general_ci | 1 || gb2312 | GB2312 Simplified Chinese | gb2312_chinese_ci | 2 || greek | ISO 8859-7 Greek | greek_general_ci | 1 || cp1250 | Windows Central European | cp1250_general_ci | 1 || gbk | GBK Simplified Chinese | gbk_chinese_ci | 2 || latin5 | ISO 8859-9 Turkish | latin5_turkish_ci | 1 || armscii8 | ARMSCII-8 Armenian | armscii8_general_ci | 1 || utf8 | UTF-8 Unicode | utf8_general_ci | 3 || ucs2 | UCS-2 Unicode | ucs2_general_ci | 2 || cp866 | DOS Russian | cp866_general_ci | 1 || keybcs2 | DOS Kamenicky Czech-Slovak | keybcs2_general_ci | 1 || macce | Mac Central European | macce_general_ci | 1 || macroman | Mac West European | macroman_general_ci | 1 || cp852 | DOS Central European | cp852_general_ci | 1 || latin7 | ISO 8859-13 Baltic | latin7_general_ci | 1 || utf8mb4 | UTF-8 Unicode | utf8mb4_general_ci | 4 || cp1251 | Windows Cyrillic | cp1251_general_ci | 1 || utf16 | UTF-16 Unicode | utf16_general_ci | 4 || utf16le | UTF-16LE Unicode | utf16le_general_ci | 4 || cp1256 | Windows Arabic | cp1256_general_ci | 1 || cp1257 | Windows Baltic | cp1257_general_ci | 1 || utf32 | UTF-32 Unicode | utf32_general_ci | 4 || binary | Binary pseudo charset | binary | 1 || geostd8 | GEOSTD8 Georgian | geostd8_general_ci | 1 || cp932 | SJIS for Windows Japanese | cp932_japanese_ci | 2 || eucjpms | UJIS for Windows Japanese | eucjpms_japanese_ci | 3 || gb18030 | China National Standard GB18030 | gb18030_chinese_ci | 4 |+----------+---------------------------------+---------------------+--------+41 rows in set (0.00 sec) 如果要设置MySQL服务启动时默认使用的字符集，可以修改MySQL的配置并添加以下内容 12[mysqld]character-set-server=utf8 在创建表的时候，我们可以在右圆括号的后面通过engine=XXX来指定表的存储引擎，MySQL支持多种存储引擎，可以通过show engines命令进行查看。MySQL 5.5以后的版本默认使用的存储引擎是InnoDB，它正好也就是我们推荐大家使用的存储引擎（因为InnoDB更适合互联网应用对高并发、性能以及事务支持等方面的需求）。 1show engines\G 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364*************************** 1. row *************************** Engine: InnoDB Support: DEFAULT Comment: Supports transactions, row-level locking, and foreign keysTransactions: YES XA: YES Savepoints: YES*************************** 2. row *************************** Engine: MRG_MYISAM Support: YES Comment: Collection of identical MyISAM tablesTransactions: NO XA: NO Savepoints: NO*************************** 3. row *************************** Engine: MEMORY Support: YES Comment: Hash based, stored in memory, useful for temporary tablesTransactions: NO XA: NO Savepoints: NO*************************** 4. row *************************** Engine: BLACKHOLE Support: YES Comment: /dev/null storage engine (anything you write to it disappears)Transactions: NO XA: NO Savepoints: NO*************************** 5. row *************************** Engine: MyISAM Support: YES Comment: MyISAM storage engineTransactions: NO XA: NO Savepoints: NO*************************** 6. row *************************** Engine: CSV Support: YES Comment: CSV storage engineTransactions: NO XA: NO Savepoints: NO*************************** 7. row *************************** Engine: ARCHIVE Support: YES Comment: Archive storage engineTransactions: NO XA: NO Savepoints: NO*************************** 8. row *************************** Engine: PERFORMANCE_SCHEMA Support: YES Comment: Performance SchemaTransactions: NO XA: NO Savepoints: NO*************************** 9. row *************************** Engine: FEDERATED Support: NO Comment: Federated MySQL storage engineTransactions: NULL XA: NULL Savepoints: NULL9 rows in set (0.00 sec) 下面的表格对MySQL几种常用的数据引擎进行了简单的对比。 特性 InnoDB MRG_MYISAM MEMORY MyISAM 存储限制 有 没有 有 有 事务 支持 锁机制 行锁 表锁 表锁 表锁 B树索引 支持 支持 支持 支持 哈希索引 支持 全文检索 支持（5.6+） 支持 集群索引 支持 数据缓存 支持 支持 索引缓存 支持 支持 支持 支持 数据可压缩 支持 内存使用 高 低 中 低 存储空间使用 高 低 低 批量插入性能 低 高 高 高 是否支持外键 支持 通过上面的比较我们可以了解到，InnoDB是唯一能够支持外键、事务以及行锁的存储引擎，所以我们之前说它更适合互联网应用，而且它也是较新的MySQL版本中默认使用的存储引擎。 在定义表结构为每个字段选择数据类型时，如果不清楚哪个数据类型更合适，可以通过MySQL的帮助系统来了解每种数据类型的特性、数据的长度和精度等相关信息。 1? data types 1234567891011121314151617181920212223242526272829303132333435363738You asked for help about help category: &quot;Data Types&quot;For more information, type &apos;help &lt;item&gt;&apos;, where &lt;item&gt; is one of the followingtopics: AUTO_INCREMENT BIGINT BINARY BIT BLOB BLOB DATA TYPE BOOLEAN CHAR CHAR BYTE DATE DATETIME DEC DECIMAL DOUBLE DOUBLE PRECISION ENUM FLOAT INT INTEGER LONGBLOB LONGTEXT MEDIUMBLOB MEDIUMINT MEDIUMTEXT SET DATA TYPE SMALLINT TEXT TIME TIMESTAMP TINYBLOB TINYINT TINYTEXT VARBINARY VARCHAR YEAR DATA TYPE 1? varchar 12345678910111213141516171819202122232425262728293031Name: &apos;VARCHAR&apos;Description:[NATIONAL] VARCHAR(M) [CHARACTER SET charset_name] [COLLATEcollation_name]A variable-length string. M represents the maximum column length incharacters. The range of M is 0 to 65,535. The effective maximum lengthof a VARCHAR is subject to the maximum row size (65,535 bytes, which isshared among all columns) and the character set used. For example, utf8characters can require up to three bytes per character, so a VARCHARcolumn that uses the utf8 character set can be declared to be a maximumof 21,844 characters. Seehttp://dev.mysql.com/doc/refman/5.7/en/column-count-limit.html.MySQL stores VARCHAR values as a 1-byte or 2-byte length prefix plusdata. The length prefix indicates the number of bytes in the value. AVARCHAR column uses one length byte if values require no more than 255bytes, two length bytes if values may require more than 255 bytes.*Note*:MySQL follows the standard SQL specification, and does not removetrailing spaces from VARCHAR values.VARCHAR is shorthand for CHARACTER VARYING. NATIONAL VARCHAR is thestandard SQL way to define that a VARCHAR column should use somepredefined character set. MySQL uses utf8 as this predefined characterset. http://dev.mysql.com/doc/refman/5.7/en/charset-national.html.NVARCHAR is shorthand for NATIONAL VARCHAR.URL: http://dev.mysql.com/doc/refman/5.7/en/string-type-overview.html 在数据类型的选择上，保存字符串数据通常都使用VARCHAR和CHAR两种类型，前者通常称为变长字符串，而后者通常称为定长字符串；对于InnoDB存储引擎，行存储格式没有区分固定长度和可变长度列，因此VARCHAR类型好CHAR类型没有本质区别，后者不一定比前者性能更好。如果要保存的很大字符串，可以使用TEXT类型；如果要保存很大的字节串，可以使用BLOB（二进制大对象）类型。在MySQL中，TEXT和BLOB又分别包括TEXT、MEDIUMTEXT、LONGTEXT和BLOB、MEDIUMBLOB、LONGBLOB三种不同的类型，它们主要的区别在于存储数据的最大大小不同。保存浮点数可以用FLOAT或DOUBLE类型，而保存定点数应该使用DECIMAL类型。如果要保存时间日期，DATETIME类型优于TIMESTAMP类型，因为前者能表示的时间日期范围更大。 DML 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566-- 插入学院数据insert into tb_college (collname, collmaster) values ('计算机学院', '左冷禅'),('外国语学院', '岳不群'),('经济管理学院', '风清扬');-- 插入学生数据insert into tb_student (stuid, stuname, stusex, stubirth, stuaddr, collid) values(1001, '杨逍', 1, '1990-3-4', '四川成都', 1),(1002, '任我行', 1, '1992-2-2', '湖南长沙', 1),(1033, '王语嫣', 0, '1989-12-3', '四川成都', 1),(1572, '岳不群', 1, '1993-7-19', '陕西咸阳', 1),(1378, '纪嫣然', 0, '1995-8-12', '四川绵阳', 1),(1954, '林平之', 1, '1994-9-20', '福建莆田', 1),(2035, '东方不败', 1, '1988-6-30', null, 2),(3011, '林震南', 1, '1985-12-12', '福建莆田', 3),(3755, '项少龙', 1, '1993-1-25', null, 3),(3923, '杨不悔', 0, '1985-4-17', '四川成都', 3),(4040, '隔壁老王', 1, '1989-1-1', '四川成都', 2);-- 删除学生数据delete from tb_student where stuid=4040;-- 更新学生数据update tb_student set stuname='杨过', stuaddr='湖南长沙' where stuid=1001;-- 插入老师数据insert into tb_teacher (teaid, teaname, teatitle, collid) values (1122, '张三丰', '教授', 1),(1133, '宋远桥', '副教授', 1),(1144, '杨逍', '副教授', 1),(2255, '范遥', '副教授', 2),(3366, '韦一笑', '讲师', 3);-- 插入课程数据insert into tb_course (couid, couname, coucredit, teaid) values (1111, 'Python程序设计', 3, 1122),(2222, 'Web前端开发', 2, 1122),(3333, '操作系统', 4, 1122),(4444, '计算机网络', 2, 1133),(5555, '编译原理', 4, 1144),(6666, '算法和数据结构', 3, 1144),(7777, '经贸法语', 3, 2255),(8888, '成本会计', 2, 3366),(9999, '审计学', 3, 3366);-- 插入选课数据insert into tb_record (sid, cid, seldate, score) values (1001, 1111, '2017-09-01', 95),(1001, 2222, '2017-09-01', 87.5),(1001, 3333, '2017-09-01', 100),(1001, 4444, '2018-09-03', null),(1001, 6666, '2017-09-02', 100),(1002, 1111, '2017-09-03', 65),(1002, 5555, '2017-09-01', 42),(1033, 1111, '2017-09-03', 92.5),(1033, 4444, '2017-09-01', 78),(1033, 5555, '2017-09-01', 82.5),(1572, 1111, '2017-09-02', 78),(1378, 1111, '2017-09-05', 82),(1378, 7777, '2017-09-02', 65.5),(2035, 7777, '2018-09-03', 88),(2035, 9999, default, null),(3755, 1111, default, null),(3755, 8888, default, null),(3755, 9999, '2017-09-01', 92); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091-- 查询所有学生信息select * from tb_student;-- 查询所有课程名称及学分(投影和别名)select couname, coucredit from tb_course;select couname as 课程名称, coucredit as 学分 from tb_course;-- 查询所有学生的姓名和性别(条件运算)select stuname as 姓名, case stusex when 1 then '男' else '女' end as 性别 from tb_student;select stuname as 姓名, if(stusex, '男', '女') as 性别 from tb_student;-- 查询所有女学生的姓名和出生日期(筛选)select stuname, stubirth from tb_student where stusex=0;-- 查询所有80后学生的姓名、性别和出生日期(筛选)select stuname, stusex, stubirth from tb_student where stubirth&gt;='1980-1-1' and stubirth&lt;='1989-12-31';select stuname, stusex, stubirth from tb_student where stubirth between '1980-1-1' and '1989-12-31';-- 查询姓"杨"的学生姓名和性别(模糊)select stuname, stusex from tb_student where stuname like '杨%';-- 查询姓"杨"名字两个字的学生姓名和性别(模糊)select stuname, stusex from tb_student where stuname like '杨_';-- 查询姓"杨"名字三个字的学生姓名和性别(模糊)select stuname, stusex from tb_student where stuname like '杨__';-- 查询名字中有"不"字或"嫣"字的学生的姓名(模糊)select stuname, stusex from tb_student where stuname like '%不%' or stuname like '%嫣%';-- 查询没有录入家庭住址的学生姓名(空值)select stuname from tb_student where stuaddr is null;-- 查询录入了家庭住址的学生姓名(空值)select stuname from tb_student where stuaddr is not null;-- 查询学生选课的所有日期(去重)select distinct seldate from tb_record;-- 查询学生的家庭住址(去重)select distinct stuaddr from tb_student where stuaddr is not null;-- 查询男学生的姓名和生日按年龄从大到小排列(排序)select stuname as 姓名, datediff(curdate(), stubirth) div 365 as 年龄 from tb_student where stusex=1 order by 年龄 desc;-- 查询年龄最大的学生的出生日期(聚合函数)select min(stubirth) from tb_student;-- 查询年龄最小的学生的出生日期(聚合函数)select max(stubirth) from tb_student;-- 查询男女学生的人数(分组和聚合函数)select stusex, count(*) from tb_student group by stusex;-- 查询课程编号为1111的课程的平均成绩(筛选和聚合函数)select avg(score) from tb_record where cid=1111;-- 查询学号为1001的学生所有课程的平均分(筛选和聚合函数)select avg(score) from tb_record where sid=1001;-- 查询每个学生的学号和平均成绩(分组和聚合函数)select sid as 学号, avg(score) as 平均分 from tb_record group by sid;-- 查询平均成绩大于等于90分的学生的学号和平均成绩-- 分组以前的筛选使用where子句 / 分组以后的筛选使用having子句select sid as 学号, avg(score) as 平均分 from tb_record group by sid having 平均分&gt;=90;-- 查询年龄最大的学生的姓名(子查询/嵌套的查询)select stuname from tb_student where stubirth=( select min(stubirth) from tb_student );-- 查询年龄最大的学生姓名和年龄(子查询+运算)select stuname as 姓名, datediff(curdate(), stubirth) div 365 as 年龄 from tb_student where stubirth=( select min(stubirth) from tb_student );-- 查询选了两门以上的课程的学生姓名(子查询/分组条件/集合运算)select stuname from tb_student where stuid in ( select stuid from tb_record group by stuid having count(stuid)&gt;2 );-- 查询学生姓名、课程名称以及成绩(连接查询)select stuname, couname, score from tb_student t1, tb_course t2, tb_record t3 where stuid=sid and couid=cid and score is not null;-- 查询学生姓名、课程名称以及成绩按成绩从高到低查询第11-15条记录(内连接+分页)select stuname, couname, score from tb_student inner join tb_record on stuid=sid inner join tb_course on couid=cid where score is not null order by score desc limit 5 offset 10;select stuname, couname, score from tb_student inner join tb_record on stuid=sid inner join tb_course on couid=cid where score is not null order by score desc limit 10, 5;-- 查询选课学生的姓名和平均成绩(子查询和连接查询)select stuname, avgmark from tb_student, ( select sid, avg(score) as avgmark from tb_record group by sid ) temp where stuid=sid;select stuname, avgmark from tb_student inner join ( select sid, avg(score) as avgmark from tb_record group by sid ) temp on stuid=sid;-- 查询每个学生的姓名和选课数量(左外连接和子查询)select stuname, ifnull(total, 0) from tb_student left outer join ( select sid, count(sid) as total from tb_record group by sid ) temp on stuid=sid; 上面的DML有几个地方需要加以说明： MySQL中支持多种类型的运算符，包括：算术运算符（+、-、*、/、%）、比较运算符（=、&lt;&gt;、&lt;=&gt;、&lt;、&lt;=、&gt;、&gt;=、BETWEEN…AND…、IN、IS NULL、IS NOT NULL、LIKE、RLIKE、REGEXP）、逻辑运算符（NOT、AND、OR、XOR）和位运算符（&amp;、|、^、~、&gt;&gt;、&lt;&lt;），我们可以在DML中使用这些运算符处理数据。 在查询数据时，可以在SELECT语句及其子句（如WHERE子句、ORDER BY子句、HAVING子句等）中使用函数，这些函数包括字符串函数、数值函数、时间日期函数、流程函数等，如下面的表格所示。 常用字符串函数。 函数 功能 CONCAT 将多个字符串连接成一个字符串 FORMAT 将数值格式化成字符串并指定保留几位小数 FROM_BASE64 / TO_BASE64 BASE64解码/编码 BIN / OCT / HEX 将数值转换成二进制/八进制/十六进制字符串 LOCATE 在字符串中查找一个子串的位置 LEFT / RIGHT 返回一个字符串左边/右边指定长度的字符 LENGTH / CHAR_LENGTH 返回字符串的长度以字节/字符为单位 LOWER / UPPER 返回字符串的小写/大写形式 LPAD / RPAD 如果字符串的长度不足，在字符串左边/右边填充指定的字符 LTRIM / RTRIM 去掉字符串前面/后面的空格 ORD / CHAR 返回字符对应的编码/返回编码对应的字符 STRCMP 比较字符串，返回-1、0、1分别表示小于、等于、大于 SUBSTRING 返回字符串指定范围的子串 常用数值函数。 函数 功能 ABS 返回一个数的绝度值 CEILING / FLOOR 返回一个数上取整/下取整的结果 CONV 将一个数从一种进制转换成另一种进制 CRC32 计算循环冗余校验码 EXP / LOG / LOG2 / LOG10 计算指数/对数 POW 求幂 RAND 返回[0,1)范围的随机数 ROUND 返回一个数四舍五入后的结果 SQRT 返回一个数的平方根 TRUNCATE 截断一个数到指定的精度 SIN / COS / TAN / COT / ASIN / ACOS / ATAN 三角函数 常用时间日期函数。 函数 功能 CURDATE / CURTIME / NOW 获取当前日期/时间/日期和时间 ADDDATE / SUBDATE 将两个日期表达式相加/相减并返回结果 DATE / TIME 从字符串中获取日期/时间 YEAR / MONTH / DAY 从日期中获取年/月/日 HOUR / MINUTE / SECOND 从时间中获取时/分/秒 DATEDIFF / TIMEDIFF 返回两个时间日期表达式相差多少天/小时 MAKEDATE / MAKETIME 制造一个日期/时间 常用流程函数。 函数 功能 IF 根据条件是否成立返回不同的值 IFNULL 如果为NULL则返回指定的值否则就返回本身 NULLIF 两个表达式相等就返回NULL否则返回第一个表达式的值 其他常用函数。 函数 功能 MD5 / SHA1 / SHA2 返回字符串对应的哈希摘要 CHARSET / COLLATION 返回字符集/校对规则 USER / CURRENT_USER 返回当前用户 DATABASE 返回当前数据库名 VERSION 返回当前数据库版本 FOUND_ROWS / ROW_COUNT 返回查询到的行数/受影响的行数 LAST_INSERT_ID 返回最后一个自增主键的值 UUID / UUID_SHORT 返回全局唯一标识符 DCL 1234567891011121314-- 创建可以远程登录的root账号并为其指定口令create user 'root'@'%' identified by '123456';-- 为远程登录的root账号授权操作所有数据库所有对象的所有权限并允许其将权限再次赋予其他用户grant all privileges on *.* to 'root'@'%' with grant option;-- 创建名为hellokitty的用户并为其指定口令create user 'hellokitty'@'%' identified by '123123';-- 将对school数据库所有对象的所有操作权限授予hellokittygrant all privileges on school.* to 'hellokitty'@'%';-- 召回hellokitty对school数据库所有对象的insert/delete/update权限revoke insert, delete, update on school.* from 'hellokitty'@'%'; 说明：创建一个可以允许任意主机登录并且具有超级管理员权限的用户在现实中并不是一个明智的决定，因为一旦该账号的口令泄露或者被破解，数据库将会面临灾难级的风险。 索引索引是关系型数据库中用来提升查询性能最为重要的手段。关系型数据库中的索引就像一本书的目录，我们可以想象一下，如果要从一本书中找出某个知识点，但是这本书没有目录，这将是意见多么可怕的事情（我们估计得一篇一篇的翻下去，才能确定这个知识点到底在什么位置）。创建索引虽然会带来存储空间上的开销，就像一本书的目录会占用一部分的篇幅一样，但是在牺牲空间后换来的查询时间的减少也是非常显著的。 MySQL中，所有数据类型的列都可以被索引，常用的存储引擎InnoDB和MyISAM能支持每个表创建16个索引。InnoDB和MyISAM使用的索引其底层算法是B-tree（B树），B-tree是一种自平衡的树，类似于平衡二叉排序树，能够保持数据有序。这种数据结构能够让查找数据、顺序访问、插入数据及删除的操作都在对数时间内完成。 接下来我们通过一个简单的例子来说明索引的意义，比如我们要根据学生的姓名来查找学生，这个场景在实际开发中应该经常遇到，就跟通过商品名称查找商品道理是一样的。我们可以使用MySQL的explain关键字来查看SQL的执行计划。 1explain select * from tb_student where stuname='林震南'\G 1234567891011121314*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_student partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 11 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 在上面的SQL执行计划中，有几项值得我们关注： type：MySQL在表中找到满足条件的行的方式，也称为访问类型，包括：ALL（全表扫描）、index（索引全扫描）、range（索引范围扫描）、ref（非唯一索引扫描）、eq_ref（唯一索引扫描）、const/system、NULL。在所有的访问类型中，很显然ALL是性能最差的，它代表了全表扫描是指要扫描表中的每一行才能找到匹配的行。 possible_keys：MySQL可以选择的索引，但是有可能不会使用。 key：MySQL真正使用的索引。 rows：执行查询需要扫描的行数，这是一个预估值。 从上面的执行计划可以看出，当我们通过学生名字查询学生时实际上是进行了全表扫描，不言而喻这个查询性能肯定是非常糟糕的，尤其是在表中的行很多的时候。如果我们需要经常通过学生姓名来查询学生，那么就应该在学生姓名对应的列上创建索引，通过索引来加速查询。 1create index idx_student_name on tb_student(stuname); 再次查看刚才的SQL对应的执行计划。 1explain select * from tb_student where stuname='林震南'\G 1234567891011121314*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_student partitions: NULL type: refpossible_keys: idx_student_name key: idx_student_name key_len: 62 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 可以注意到，在对学生姓名创建索引后，刚才的查询已经不是全表扫描而是基于索引的查询，而且扫描的行只有唯一的一行，这显然大大的提升了查询的性能。MySQL中还允许创建前缀索引，即对索引字段的前N个字符创建索引，这样的话可以减少索引占用的空间（但节省了空间很有可能会浪费时间，时间和空间是不可调和的矛盾），如下所示。 1create index idx_student_name_1 on tb_student(stuname(1)); 上面的索引相当于是根据学生姓名的第一个字来创建的索引，我们再看看SQL执行计划。 1explain select * from tb_student where stuname='林震南'\G 1234567891011121314*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_student partitions: NULL type: refpossible_keys: idx_student_name key: idx_student_name key_len: 5 ref: const rows: 2 filtered: 100.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 不知道大家是否注意到，这一次扫描的行变成了2行，因为学生表中有两个姓“林”的学生，我们只用姓名的第一个字作为索引的话，在查询时通过索引就会找到这两行。 如果要删除索引，可以使用下面的SQL。 1alter table tb_student drop index idx_student_name; 或者 1drop index idx_student_name on tb_student; 我们简单的为大家总结一下索引的设计原则： 最适合索引的列是出现在WHERE子句和连接子句中的列。 索引列的基数越大（取值多重复值少），索引的效果就越好。 使用前缀索引可以减少索引占用的空间，内存中可以缓存更多的索引。 索引不是越多越好，虽然索引加速了读操作（查询），但是写操作（增、删、改）都会变得更慢，因为数据的变化会导致索引的更新，就如同书籍章节的增删需要更新目录一样。 使用InnoDB存储引擎时，表的普通索引都会保存主键的值，所以主键要尽可能选择较短的数据类型，这样可以有效的减少索引占用的空间，利用提升索引的缓存效果。 最后，还有一点需要说明，InnoDB使用的B-tree索引，数值类型的列除了等值判断时索引会生效之外，使用&gt;、&lt;、&gt;=、&lt;=、BETWEEN…AND… 、&lt;&gt;时，索引仍然生效；对于字符串类型的列，如果使用不以通配符开头的模糊查询，索引也是起作用的，但是其他的情况会导致索引失效，这就意味着很有可能会做全表查询。 视图视图是关系型数据库中将一组查询指令构成的结果集组合成可查询的数据表的对象。简单的说，视图就是虚拟的表，但与数据表不同的是，数据表是一种实体结构，而视图是一种虚拟结构，你也可以将视图理解为保存在数据库中被赋予名字的SQL语句。 使用视图可以获得以下好处： 可以将实体数据表隐藏起来，让外部程序无法得知实际的数据结构，让访问者可以使用表的组成部分而不是整个表，降低数据库被攻击的风险。 在大多数的情况下视图是只读的（更新视图的操作通常都有诸多的限制），外部程序无法直接透过视图修改数据。 重用SQL语句，将高度复杂的查询包装在视图表中，直接访问该视图即可取出需要的数据；也可以将视图视为数据表进行连接查询。 视图可以返回与实体数据表不同格式的数据， 创建视图。 123456789create view vw_score as select sid, round(avg(score), 1) as avgscore from tb_record group by sid;create view vw_student_score as select stuname, avgscore from tb_student, vw_score where stuid=sid; 提示：因为视图不包含数据，所以每次使用视图时，都必须执行查询以获得数据，如果你使用了连接查询、嵌套查询创建了较为复杂的视图，你可能会发现查询性能下降得很厉害。因此，在使用复杂的视图前，应该进行测试以确保其性能能够满足应用的需求。 使用视图。 1select stuname, avgscore from vw_student_score order by avgscore desc; 1234567891011+--------------+----------+| stuname | avgscore |+--------------+----------+| 杨过 | 95.6 || 任我行 | 53.5 || 王语嫣 | 84.3 || 纪嫣然 | 73.8 || 岳不群 | 78.0 || 东方不败 | 88.0 || 项少龙 | 92.0 |+--------------+----------+ 既然视图是一张虚拟的表，那么视图的中的数据可以更新吗？视图的可更新性要视具体情况而定，以下类型的视图是不能更新的： 使用了聚合函数（SUM、MIN、MAX、AVG、COUNT等）、DISTINCT、GROUP BY、HAVING、UNION或者UNION ALL的视图。 SELECT中包含了子查询的视图。 FROM子句中包含了一个不能更新的视图的视图。 WHERE子句的子查询引用了FROM子句中的表的视图。 删除视图。 1drop view vw_student_score; 说明：如果希望更新视图，可以先用上面的命令删除视图，也可以通过create or replace view来更新视图。 视图的规则和限制。 视图可以嵌套，可以利用从其他视图中检索的数据来构造一个新的视图。视图也可以和表一起使用。 创建视图时可以使用order by子句，但如果从视图中检索数据时也使用了order by，那么该视图中原先的order by会被覆盖。 视图无法使用索引，也不会激发触发器（实际开发中因为性能等各方面的考虑，通常不建议使用触发器，所以我们也不对这个概念进行介绍）的执行。 存储过程存储过程是事先编译好存储在数据库中的一组SQL的集合，调用存储过程可以简化应用程序开发人员的工作，减少与数据库服务器之间的通信，对于提升数据操作的性能也是有帮助的。其实迄今为止，我们使用的SQL语句都是针对一个或多个表的单条语句，但在实际开发中经常会遇到某个操作需要多条SQL语句才能完成的情况。例如，电商网站在受理用户订单时，需要做以下一系列的处理。 通过查询来核对库存中是否有对应的物品以及库存是否充足。 如果库存有物品，需要锁定库存以确保这些物品不再卖给别人， 并且要减少可用的物品数量以反映正确的库存量。 如果库存不足，可能需要进一步与供应商进行交互或者至少产生一条系统提示消息。 不管受理订单是否成功，都需要产生流水记录，而且需要给对应的用户产生一条通知信息。 我们可以通过存储过程将复杂的操作封装起来，这样不仅有助于保证数据的一致性，而且将来如果业务发生了变动，只需要调整和修改存储过程即可。对于调用存储过程的用户来说，存储过程并没有暴露数据表的细节，而且执行存储过程比一条条的执行一组SQL要快得多。 下面的存储过程实现了查询某门课程的最高分、最低分和平均分。 12345678910111213delimiter $$create procedure sp_get_score(courseId int, out maxScore decimal(4,1), out minScore decimal(4,1), out avgScore decimal(4,1))begin select max(score) into maxScore from tb_record where cid=courseId; select min(score) into minScore from tb_record where cid=courseId; select avg(score) into avgScore from tb_record where cid=courseId;end $$delimiter ; 说明：在定义存储过程时，因为可能需要书写多条SQL，而分隔这些SQL需要使用分号作为分隔符，如果这个时候，仍然用分号表示整段代码结束，那么定义存储过程的SQL就会出现错误，所以上面我们用delimiter $$将整段代码结束的标记定义为$$，那么代码中的分号将不再表示整段代码的结束，需要马上执行，整段代码在遇到end $$时才输入完成并执行。在定义完存储过程后，通过delimiter ;将结束符重新改回成分号。 上面定义的存储过程有四个参数，其中第一个参数是输入参数，代表课程的编号，后面的参数都是输出参数，因为存储过程不能定义返回值，只能通过输出参数将执行结果带出，定义输出参数的关键字是out，默认情况下参数都是输入参数。 调用存储过程。 1call sp_get_score(1111, @a, @b, @c); 获取输出参数的值。 1select @a as 最高分, @b as 最低分, @c as 平均分; 删除存储过程。 1drop procedure sp_get_score; 在存储过程中，我们可以定义变量、条件，可以使用分支和循环语句，可以通过游标操作查询结果，还可以使用事件调度器，这些内容我们暂时不在此处进行介绍。虽然我们说了很多存储过程的好处，但是在实际开发中，如果过度的使用存储过程，将大量复杂的运算放到存储过程中，也会导致占用数据库服务器的CPU资源，造成数据库服务器承受巨大的压力。为此，我们一般会将复杂的运算和处理交给应用服务器，因为很容易部署多台应用服务器来分摊这些压力。 几个重要的概念范式理论 - 设计二维表的指导思想 第一范式：数据表的每个列的值域都是由原子值组成的，不能够再分割。 第二范式：数据表里的所有数据都要和该数据表的键（主键与候选键）有完全依赖关系。 第三范式：所有非键属性都只和候选键有相关性，也就是说非键属性之间应该是独立无关的。 数据完整性 实体完整性 - 每个实体都是独一无二的 主键（primary key） / 唯一约束 / 唯一索引（unique） 引用完整性（参照完整性）- 关系中不允许引用不存在的实体 外键（foreign key） 域完整性 - 数据是有效的 数据类型及长度 非空约束（not null） 默认值约束（default） 检查约束（check） 说明：在MySQL数据库中，检查约束并不起作用。 数据一致性 事务：一系列对数据库进行读/写的操作，这些操作要么全都成功，要么全都失败。 事务的ACID特性 原子性：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行 一致性：事务应确保数据库的状态从一个一致状态转变为另一个一致状态 隔离性：多个事务并发执行时，一个事务的执行不应影响其他事务的执行 持久性：已被提交的事务对数据库的修改应该永久保存在数据库中 MySQL中的事务操作 开启事务环境 1start transaction 或 1begin 提交事务 1commit 回滚事务 1rollback 其他内容大家应该能够想到，关于MySQL的知识肯定远远不止上面列出的这些，比如MySQL的性能优化、管理和维护MySQL的相关工具、MySQL数据的备份和恢复、监控MySQL、部署高可用架构等问题我们在这里都没有进行讨论。当然，这些内容也都是跟项目开发密切相关的，我们就留到后续的章节中再续点进行讲解。 Python数据库编程我们用如下所示的数据库来演示在Python中如何访问MySQL数据库。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051drop database if exists hrs;create database hrs default charset utf8;use hrs;drop table if exists tb_emp;drop table if exists tb_dept;create table tb_dept(dno int not null comment '编号',dname varchar(10) not null comment '名称',dloc varchar(20) not null comment '所在地',primary key (dno));insert into tb_dept values (10, '会计部', '北京'), (20, '研发部', '成都'), (30, '销售部', '重庆'), (40, '运维部', '深圳');create table tb_emp(eno int not null comment '员工编号',ename varchar(20) not null comment '员工姓名',job varchar(20) not null comment '员工职位',mgr int comment '主管编号',sal int not null comment '员工月薪',comm int comment '每月补贴',dno int comment '所在部门编号',primary key (eno));alter table tb_emp add constraint fk_emp_dno foreign key (dno) references tb_dept (dno);insert into tb_emp values (7800, '张三丰', '总裁', null, 9000, 1200, 20), (2056, '乔峰', '分析师', 7800, 5000, 1500, 20), (3088, '李莫愁', '设计师', 2056, 3500, 800, 20), (3211, '张无忌', '程序员', 2056, 3200, null, 20), (3233, '丘处机', '程序员', 2056, 3400, null, 20), (3251, '张翠山', '程序员', 2056, 4000, null, 20), (5566, '宋远桥', '会计师', 7800, 4000, 1000, 10), (5234, '郭靖', '出纳', 5566, 2000, null, 10), (3344, '黄蓉', '销售主管', 7800, 3000, 800, 30), (1359, '胡一刀', '销售员', 3344, 1800, 200, 30), (4466, '苗人凤', '销售员', 3344, 2500, null, 30), (3244, '欧阳锋', '程序员', 3088, 3200, null, 20), (3577, '杨过', '会计', 5566, 2200, null, 10), (3588, '朱九真', '会计', 5566, 2500, null, 10); 在Python 3中，我们通常使用纯Python的三方库PyMySQL来访问MySQL数据库，它应该是目前Python操作MySQL数据库最好的选择。 安装PyMySQL。 1pip install pymysql 添加一个部门。 123456789101112131415161718192021222324252627282930import pymysqldef main(): no = int(input('编号: ')) name = input('名字: ') loc = input('所在地: ') # 1. 创建数据库连接对象 con = pymysql.connect(host='localhost', port=3306, database='hrs', charset='utf8', user='root', password='123456') try: # 2. 通过连接对象获取游标 with con.cursor() as cursor: # 3. 通过游标执行SQL并获得执行结果 result = cursor.execute( 'insert into tb_dept values (%s, %s, %s)', (no, name, loc) ) if result == 1: print('添加成功!') # 4. 操作成功提交事务 con.commit() finally: # 5. 关闭连接释放资源 con.close()if __name__ == '__main__': main() 删除一个部门。 1234567891011121314151617181920212223import pymysqldef main(): no = int(input('编号: ')) con = pymysql.connect(host='localhost', port=3306, database='hrs', charset='utf8', user='root', password='123456', autocommit=True) try: with con.cursor() as cursor: result = cursor.execute( 'delete from tb_dept where dno=%s', (no, ) ) if result == 1: print('删除成功!') finally: con.close()if __name__ == '__main__': main() 说明：如果不希望每次SQL操作之后手动提交或回滚事务，可以像上面的代码那样，在创建连接的时候多加一个名为autocommit的参数并将它的值设置为True，表示每次执行SQL之后自动提交。如果程序中不需要使用事务环境也不希望手动的提交或回滚就可以这么做。 更新一个部门。 12345678910111213141516171819202122232425import pymysqldef main(): no = int(input('编号: ')) name = input('名字: ') loc = input('所在地: ') con = pymysql.connect(host='localhost', port=3306, database='hrs', charset='utf8', user='root', password='123456', autocommit=True) try: with con.cursor() as cursor: result = cursor.execute( 'update tb_dept set dname=%s, dloc=%s where dno=%s', (name, loc, no) ) if result == 1: print('更新成功!') finally: con.close()if __name__ == '__main__': main() 查询所有部门。 123456789101112131415161718192021222324import pymysqlfrom pymysql.cursors import DictCursordef main(): con = pymysql.connect(host='localhost', port=3306, database='hrs', charset='utf8', user='root', password='123456') try: with con.cursor(cursor=DictCursor) as cursor: cursor.execute('select dno as no, dname as name, dloc as loc from tb_dept') results = cursor.fetchall() print(results) print('编号\t名称\t\t所在地') for dept in results: print(dept['no'], end='\t') print(dept['name'], end='\t') print(dept['loc']) finally: con.close()if __name__ == '__main__': main() 分页查询员工信息。 12345678910111213141516171819202122232425262728293031323334353637import pymysqlfrom pymysql.cursors import DictCursorclass Emp(object): def __init__(self, no, name, job, sal): self.no = no self.name = name self.job = job self.sal = sal def __str__(self): return f'\n编号：&#123;self.no&#125;\n姓名：&#123;self.name&#125;\n职位：&#123;self.job&#125;\n月薪：&#123;self.sal&#125;\n'def main(): page = int(input('页码: ')) size = int(input('大小: ')) con = pymysql.connect(host='localhost', port=3306, database='hrs', charset='utf8', user='root', password='123456') try: with con.cursor() as cursor: cursor.execute( 'select eno as no, ename as name, job, sal from tb_emp limit %s,%s', ((page - 1) * size, size) ) for emp_tuple in cursor.fetchall(): emp = Emp(*emp_tuple) print(emp) finally: con.close()if __name__ == '__main__': main()]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-逻辑控制]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-2015-07-20-python-%E9%80%BB%E8%BE%91%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[条件语句if 判断条件1.1 and 判断条件1.2`:` 执行语句1…… elif 判断条件2.1 or 判断条件2.2`:` 执行语句2…… elif (num &gt;= 0 and num &lt;= 5) or (num &gt;= 10 and num &lt;= 15)`:` 执行语句3…… else`:` 执行语句4…… #同一行的位置上使用if条件判断语句循环forfor iterating_var in sequence: statements(s)循环嵌套forfor iterating_var in sequence: for iterating_var in sequence: statements(s) statements(s)break,continue,pass# 跳出最近的一层循环 $ break # 跳过本次循环，继续下一次循环 $ continue # 占位符，后面的还运行 $ pass]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-运算符]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-2015-07-21-python-%E8%BF%90%E7%AE%97%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[算术运算符+ - * / % 取模 - 返回除法的余数 ** 幂 - 返回x的y次幂 // 取整除 - 返回商的整数部分（向下取整）比较运算符== !=或&lt;&gt; &gt; &lt; &gt;= &lt;=赋值运算符= += -= *= /= %= **= //=位运算符（二进制）略逻辑运算符and or not成员运算符in not in身份运算符is is not]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[31-35.玩转Linux操作系统]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-31-35-%E7%8E%A9%E8%BD%ACLinux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[玩转Linux操作系统 说明：本文中对Linux命令的讲解都是基于名为CentOS的Linux发行版本，我自己使用的是阿里云服务器，系统版本为CentOS Linux release 7.6.1810。不同的Linux发行版本在Shell命令和工具程序上会有一些差别，但是这些差别是很小的。 操作系统发展史只有硬件没有软件的计算机系统被称之为“裸机”，我们很难用“裸机”来完成计算机日常的工作（如存储和运算），所以必须用特定的软件来控制硬件的工作。最靠近计算机硬件的软件是系统软件，其中最为重要的就是“操作系统”。“操作系统”是控制和管理整个计算机硬件和软件资源、实现资源分配和任务调配、为系统用户以及其他软件提供接口和环境的程序的集合。 没有操作系统（手工操作）在计算机诞生之初没有操作系统的年代，人们先把程序纸带（或卡片）装上计算机，然后启动输入机把程序送入计算机，接着通过控制台开关启动程序运行。当程序执行完毕，打印机输出计算的结果，用户卸下并取走纸带（或卡片）。第二个用户上机，重复同样的步骤。在整个过程中用户独占机器，CPU等待手工操作，资源利用率极低。 批处理系统首先启动计算机上的一个监督程序，在监督程序的控制下，计算机能够自动的、成批的处理一个或多个用户的作业。完成一批作业后，监督程度又从输入机读取作业存入磁带机。按照上面的步骤重复处理任务。监督程序不停的处理各个作业，实现了作业的自动转接，减少了作业的建立时间和手工操作时间，提高了计算机资源的利用率。 批处理系统又可以分为单道批处理系统、多道批处理系统、联机批处理系统、脱机批处理系统。 分时系统和实时系统分时系统是把处理器的运行时间分成很短的时间片，按时间片轮流把处理机分配给各联机作业使用。 若某个作业在分配给它的时间片内不能完成其计算，则该作业暂时中断，把处理机让给另一作业使用，等待下一轮调度时再继续其运行。由于计算机速度很快，作业运行轮转得很快，给每个用户的感觉是他独占了一台计算机。而每个用户可以通过自己的终端向系统发出各种操作控制命令，在充分的人机交互情况下，完成作业的运行。为了解决分时系统不能及时响应用户指令的情况，又出现了能够在在严格的时间范围内完成事件处理，及时响应随机外部事件的实时系统。 通用操作系统 1960s：IBM的System/360系列的机器有了统一的操作系统OS/360。 1965年：AT&amp;T的贝尔实验室加入GE和MIT的合作计划开始开发MULTICS。 1969年：MULTICS项目失败，Ken Tompson赋闲在家，为了玩“Space Travel”游戏用汇编语言在当时已经被淘汰的PDP-7上开发了Unics。 注：很难想象，Unix这么伟大的系统，居然是一个赋闲在家的程序员（关键是老婆回娘家还带上了孩子）在一台被淘汰的设备上为了玩游戏开发出来的。 1970年~1971年：Ken Tompson和Dennis Ritchie用B语言在PDP-11上重写了Unics，并在Brian Kernighan的建议下将其更名为Unix。 1972年~1973年：Dennis Ritchie发明了C语言来取代可移植性较差的B语言，并开启了用C语言重写Unix的工作。 1974年：Unix推出了里程碑意义的第5版，几乎完全用C语言来实现。 1979年：从Unix第7版开始，AT&amp;T发布新的使用条款，将Unix私有化。 1987年：Andrew S. Tanenbaum教授为了能在课堂上为学生讲解操作系统运作的细节，决定在不使用任何AT&amp;T的源代码前提下，自行开发与Unix兼容的操作系统以避免版权上的争议，该系统被命名为Minix。 1991年：Linus Torvalds就读于芬兰赫尔辛基大学期间，尝试在Minix上做一些开发工作，但因为Minix只是作为教学用途的操作系统，功能并不强大，为了方便在学校的新闻组和邮件系统中读写和下载文件，Linus编写了磁盘驱动程序和文件系统，这些东西形成了Linux系统内核的雏形。 下图是Unix操作系统家族的图谱。 Linux概述Linux是一个通用操作系统。一个操作系统要负责任务调度、内存分配、处理外围设备I/O等操作。操作系统通常由内核（运行其他程序，管理像磁盘、打印机等硬件设备的核心程序）和系统程序（设备驱动、底层库、shell、服务程序等）两部分组成。 Linux内核是芬兰人Linus Torvalds开发的，于1991年9月发布。而Linux操作系统作为Internet时代的产物，它是由全世界许多开发者共同合作开发的，是一个自由的操作系统（注意自由和免费并不是同一个概念，想了解二者的差别可以点击这里）。 Linux系统优点 通用操作系统，不跟特定的硬件绑定。 用C语言编写，可移植性强，有内核编程接口。 支持多用户和多任务，支持安全的分层文件系统。 大量的实用程序，完善的网络功能以及强大的支持文档。 可靠的安全性和良好的稳定性，对开发者更友好。 Linux系统发行版本 Redhat Ubuntu CentOS Fedora Debian openSUSE 基础命令Linux系统的命令通常都是如下所示的格式： 1命令名称 [命名参数] [命令对象] 获取登录信息 - w / who / last/ lastb。 1234567891011121314151617181920[root ~]# w 23:31:16 up 12:16, 2 users, load average: 0.00, 0.01, 0.05USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/0 182.139.66.250 23:03 4.00s 0.02s 0.00s wjackfrue pts/1 182.139.66.250 23:26 3:56 0.00s 0.00s -bash[root ~]# whoroot pts/0 2018-04-12 23:03 (182.139.66.250)jackfrued pts/1 2018-04-12 23:26 (182.139.66.250)[root ~]# who am iroot pts/0 2018-04-12 23:03 (182.139.66.250)[root ~]# who mom likesroot pts/0 2018-04-12 23:03 (182.139.66.250)[root ~]# lastroot pts/0 117.136.63.184 Sun May 26 18:57 still logged in reboot system boot 3.10.0-957.10.1. Mon May 27 02:52 - 19:10 (-7:-42) root pts/4 117.136.63.184 Sun May 26 18:51 - crash (08:01) root pts/4 117.136.63.184 Sun May 26 18:49 - 18:49 (00:00) root pts/3 117.136.63.183 Sun May 26 18:35 - crash (08:17) root pts/2 117.136.63.183 Sun May 26 18:34 - crash (08:17) root pts/0 117.136.63.183 Sun May 26 18:10 - crash (08:42) 查看自己使用的Shell - ps。 Shell也被称为“壳”或“壳程序”，它是用户与操作系统内核交流的翻译官，简单的说就是人与计算机交互的界面和接口。目前很多Linux系统默认的Shell都是bash（Bourne Again SHell），因为它可以使用tab键进行命令和路径补全、可以保存历史命令、可以方便的配置环境变量以及执行批处理操作。 1234[root@izwz97tbgo9lkabnat2lo8z ~]# ps PID TTY TIME CMD 3531 pts/0 00:00:00 bash 3553 pts/0 00:00:00 ps 查看命令的说明和位置 - whatis / which / whereis。 123456789101112[root ~]# whatis psps (1) - report a snapshot of the current processes.[root ~]# whatis pythonpython (1) - an interpreted, interactive, object-oriented programming language[root ~]# whereis psps: /usr/bin/ps /usr/share/man/man1/ps.1.gz[root ~]# whereis pythonpython: /usr/bin/python /usr/bin/python2.7 /usr/lib/python2.7 /usr/lib64/python2.7 /etc/python /usr/include/python2.7 /usr/share/man/man1/python.1.gz[root ~]# which ps/usr/bin/ps[root ~]# which python/usr/bin/python 清除屏幕上显示的内容 - clear。 查看帮助文档 - man / info / help / apropos。 123456789101112131415[root@izwz97tbgo9lkabnat2lo8z ~]# ps --helpUsage: ps [options] Try 'ps --help &lt;simple|list|output|threads|misc|all&gt;' or 'ps --help &lt;s|l|o|t|m|a&gt;' for additional help text.For more details see ps(1).[root@izwz97tbgo9lkabnat2lo8z ~]# man psPS(1) User Commands PS(1)NAME ps - report a snapshot of the current processes.SYNOPSIS ps [options]DESCRIPTION... 查看系统和主机名 - uname / hostname。 123456[root@izwz97tbgo9lkabnat2lo8z ~]# unameLinux[root@izwz97tbgo9lkabnat2lo8z ~]# hostnameizwz97tbgo9lkabnat2lo8z[root@iZwz97tbgo9lkabnat2lo8Z ~]# cat /etc/centos-releaseCentOS Linux release 7.6.1810 (Core) 说明：cat是连接文件内容并打印到标准输出的命令，后面会讲到该命令；/etc是Linux系统上的一个非常重要的目录，它保存了很多的配置文件；centos-release是该目录下的一个文件，因为我自己使用的Linux发行版本是CentOS 7.6，因此这里会有一个这样的文件。 时间和日期 - date / cal。 123456789101112131415161718[root@iZwz97tbgo9lkabnat2lo8Z ~]# dateWed Jun 20 12:53:19 CST 2018[root@iZwz97tbgo9lkabnat2lo8Z ~]# cal June 2018Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 910 11 12 13 14 15 1617 18 19 20 21 22 2324 25 26 27 28 29 30[root@iZwz97tbgo9lkabnat2lo8Z ~]# cal 5 2017 May 2017Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 10 11 12 1314 15 16 17 18 19 2021 22 23 24 25 26 2728 29 30 31 重启和关机 - reboot / shutdown。 123456789101112131415161718[root ~]# shutdown -h +5Shutdown scheduled for Sun 2019-05-26 19:34:27 CST, use 'shutdown -c' to cancel.[root ~]# Broadcast message from root (Sun 2019-05-26 19:29:27 CST):The system is going down for power-off at Sun 2019-05-26 19:34:27 CST![root ~]# shutdown -cBroadcast message from root (Sun 2019-05-26 19:30:22 CST):The system shutdown has been cancelled at Sun 2019-05-26 19:31:22 CST![root ~]# shutdown -r 23:58Shutdown scheduled for Sun 2019-05-26 23:58:00 CST, use 'shutdown -c' to cancel.[root ~]# shutdown -cBroadcast message from root (Sun 2019-05-26 19:31:06 CST):The system shutdown has been cancelled at Sun 2019-05-26 19:32:06 CST! 说明：在执行shutdown命令时会向登录系统的用户发出警告，可以在命令后面跟上警告消息来替换默认的警告消息，也可以在-h参数后通过now来表示立刻关机。 退出登录 - exit / logout。 查看历史命令 - history。 1234567[root@iZwz97tbgo9lkabnat2lo8Z ~]# history...452 ls453 cd Python-3.6.5/454 clear455 history[root@iZwz97tbgo9lkabnat2lo8Z ~]# !454 说明：查看到历史命令之后，可以用!历史命令编号来重新执行该命令；通过history -c可以清除历史命令。 实用程序文件和文件夹操作 创建/删除空目录 - mkdir / rmdir。 123[root ~]# mkdir abc[root ~]# mkdir -p xyz/abc[root ~]# rmdir abc 创建/删除文件 - touch / rm。 12345[root ~]# touch readme.txt[root ~]# touch error.txt[root ~]# rm error.txtrm: remove regular empty file ‘error.txt’? y[root ~]# rm -rf xyz touch命令用于创建空白文件或修改文件时间。在Linux系统中一个文件有三种时间： 更改内容的时间 - mtime。 更改权限的时间 - ctime。 最后访问时间 - atime。 rm的几个重要参数： -i：交互式删除，每个删除项都会进行询问。 -r：删除目录并递归的删除目录中的文件和目录。 -f：强制删除，忽略不存在的文件，没有任何提示。 切换和查看当前工作目录 - cd / pwd。 说明：cd命令后面可以跟相对路径（以当前路径作为参照）或绝对路径（以/开头）来切换到指定的目录，也可以用cd ..来返回上一级目录。请大家想一想，如果要返回到上上一级目录应该给cd命令加上什么样的参数呢？ 查看目录内容 - ls。 -l：以长格式查看文件和目录。 -a：显示以点开头的文件和目录（隐藏文件）。 -R：遇到目录要进行递归展开（继续列出目录下面的文件和目录）。 -d：只列出目录，不列出其他内容。 -S / -t：按大小/时间排序。 查看文件内容 - cat / tac / head / tail / more / less / rev / od。 12345678910111213141516171819202122232425262728[root ~]# wget http://www.sohu.com/ -O sohu.html--2018-06-20 18:42:34-- http://www.sohu.com/Resolving www.sohu.com (www.sohu.com)... 14.18.240.6Connecting to www.sohu.com (www.sohu.com)|14.18.240.6|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 212527 (208K) [text/html]Saving to: ‘sohu.html’100%[==================================================&gt;] 212,527 --.-K/s in 0.03s2018-06-20 18:42:34 (7.48 MB/s) - ‘sohu.html’ saved [212527/212527][root ~]# cat sohu.html...[root ~]# head -10 sohu.html&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;搜狐&lt;/title&gt;&lt;meta name="Keywords" content="搜狐,门户网站,新媒体,网络媒体,新闻,财经,体育,娱乐,时尚,汽车,房产,科技,图片,论坛,微博,博客,视频,电影,电视剧"/&gt;&lt;meta name="Description" content="搜狐网为用户提供24小时不间断的最新资讯，及搜索、邮件等网络服务。内容包括全球热点事件、突发新闻、时事评论、热播影视剧、体育赛事、行业动态、生活服务信息，以及论坛、博客、微博、我的搜狐等互动空间。" /&gt;&lt;meta name="shenma-site-verification" content="1237e4d02a3d8d73e96cbd97b699e9c3_1504254750"&gt;&lt;meta charset="utf-8"/&gt;&lt;meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/&gt;[root ~]# tail -2 sohu.html&lt;/body&gt;&lt;/html&gt;[root ~]# less sohu.html...[root ~]# cat -n sohu.html | more... 说明：上面用到了一个名为wget的命令，它是一个网络下载器程序，可以从指定的URL下载资源。 拷贝/移动文件 - cp / mv。 12345678[root ~]# mkdir backup[root ~]# cp sohu.html backup/[root ~]# cd backup[root backup]# lssohu.html[root backup]# mv sohu.html sohu_index.html[root backup]# lssohu_index.html 文件重命名 - rename。 1[root@iZwz97tbgo9lkabnat2lo8Z ~]# rename .htm .html *.htm 查找文件和查找内容 - find / grep。 1234567891011121314151617[root@iZwz97tbgo9lkabnat2lo8Z ~]# find / -name "*.html"/root/sohu.html/root/backup/sohu_index.html[root@izwz97tbgo9lkabnat2lo8z ~]# find . -atime 7 -type f -print[root@izwz97tbgo9lkabnat2lo8z ~]# find . -type f -size +2k[root@izwz97tbgo9lkabnat2lo8z ~]# find . -type f -name "*.swp" -delete[root@iZwz97tbgo9lkabnat2lo8Z ~]# grep "&lt;script&gt;" sohu.html -n20:&lt;script&gt;[root@iZwz97tbgo9lkabnat2lo8Z ~]# grep -E \&lt;\/?script.*\&gt; sohu.html -n20:&lt;script&gt;22:&lt;/script&gt;24:&lt;script src="//statics.itc.cn/web/v3/static/js/es5-shim-08e41cfc3e.min.js"&gt;&lt;/script&gt;25:&lt;script src="//statics.itc.cn/web/v3/static/js/es5-sham-1d5fa1124b.min.js"&gt;&lt;/script&gt;26:&lt;script src="//statics.itc.cn/web/v3/static/js/html5shiv-21fc8c2ba6.js"&gt;&lt;/script&gt;29:&lt;script type="text/javascript"&gt;52:&lt;/script&gt;... 说明：grep在搜索字符串时可以使用正则表达式，如果需要使用正则表达式可以用grep -E或者直接使用egrep。 创建链接和查看链接 - ln / readlink。 123456789101112131415[root@iZwz97tbgo9lkabnat2lo8Z ~]# ls -l sohu.html-rw-r--r-- 1 root root 212131 Jun 20 19:15 sohu.html[root@iZwz97tbgo9lkabnat2lo8Z ~]# ln /root/sohu.html /root/backup/sohu_backup[root@iZwz97tbgo9lkabnat2lo8Z ~]# ls -l sohu.html-rw-r--r-- 2 root root 212131 Jun 20 19:15 sohu.html[root@iZwz97tbgo9lkabnat2lo8Z ~]# ln /root/sohu.html /root/backup/sohu_backup2[root@iZwz97tbgo9lkabnat2lo8Z ~]# ls -l sohu.html-rw-r--r-- 3 root root 212131 Jun 20 19:15 sohu.html[root@iZwz97tbgo9lkabnat2lo8Z ~]# ln -s /etc/centos-release sysinfo[root@iZwz97tbgo9lkabnat2lo8Z ~]# ls -l sysinfolrwxrwxrwx 1 root root 19 Jun 20 19:21 sysinfo -&gt; /etc/centos-release[root@iZwz97tbgo9lkabnat2lo8Z ~]# cat sysinfoCentOS Linux release 7.4.1708 (Core)[root@iZwz97tbgo9lkabnat2lo8Z ~]# cat /etc/centos-releaseCentOS Linux release 7.4.1708 (Core) 说明：链接可以分为硬链接和软链接（符号链接）。硬链接可以认为是一个指向文件数据的指针，就像Python中对象的引用计数，每添加一个硬链接，文件的对应链接数就增加1，只有当文件的链接数为0时，文件所对应的存储空间才有可能被其他文件覆盖。我们平常删除文件时其实并没有删除硬盘上的数据，我们删除的只是一个指针，或者说是数据的一条使用记录，所以类似于“文件粉碎机”之类的软件在“粉碎”文件时除了删除文件指针，还会在文件对应的存储区域填入数据来保证文件无法再恢复。软链接类似于Windows系统下的快捷方式，当软链接链接的文件被删除时，软链接也就失效了。 压缩/解压缩和归档/解归档 - gzip / gunzip / xz。 1234567891011121314[root@iZwz97tbgo9lkabnat2lo8Z ~]# wget http://download.redis.io/releases/redis-4.0.10.tar.gz--2018-06-20 19:29:59-- http://download.redis.io/releases/redis-4.0.10.tar.gzResolving download.redis.io (download.redis.io)... 109.74.203.151Connecting to download.redis.io (download.redis.io)|109.74.203.151|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 1738465 (1.7M) [application/x-gzip]Saving to: ‘redis-4.0.10.tar.gz’100%[==================================================&gt;] 1,738,465 70.1KB/s in 74s2018-06-20 19:31:14 (22.9 KB/s) - ‘redis-4.0.10.tar.gz’ saved [1738465/1738465][root@iZwz97tbgo9lkabnat2lo8Z ~]# ls redis*redis-4.0.10.tar.gz[root@iZwz97tbgo9lkabnat2lo8Z ~]# gunzip redis-4.0.10.tar.gz[root@iZwz97tbgo9lkabnat2lo8Z ~]# ls redis*redis-4.0.10.tar 归档和解归档 - tar。 123456789101112131415[root@iZwz97tbgo9lkabnat2lo8Z ~]# tar -xvf redis-4.0.10.tarredis-4.0.10/redis-4.0.10/.gitignoreredis-4.0.10/00-RELEASENOTESredis-4.0.10/BUGSredis-4.0.10/CONTRIBUTINGredis-4.0.10/COPYINGredis-4.0.10/INSTALLredis-4.0.10/MANIFESTOredis-4.0.10/Makefileredis-4.0.10/README.mdredis-4.0.10/deps/redis-4.0.10/deps/Makefileredis-4.0.10/deps/README.md... 说明：归档（也称为创建归档）和解归档都使用tar命令，通常创建归档需要-cvf三个参数，其中c表示创建（create），v表示显示创建归档详情（verbose），f表示指定归档的文件（file）；解归档需要加上-xvf参数，其中x表示抽取（extract），其他两个参数跟创建归档相同。 将标准输入转成命令行参数 - xargs。 下面的命令会将查找当前路径下的html文件，然后通过xargs将这些文件作为参数传给rm命令，实现查找并删除文件的操作。 1[root@iZwz97tbgo9lkabnat2lo8Z ~]# find . -type f -name "*.html" | xargs rm -f 下面的命令将a.txt文件中的多行内容变成一行输出到b.txt文件中，其中&lt;表示从a.txt中读取输入，&gt;表示将命令的执行结果输出到b.txt中。 1[root@iZwz97tbgo9lkabnat2lo8Z ~]# xargs &lt; a.txt &gt; b.txt 说明：这个命令就像上面演示的那样常在管道（实现进程间通信的一种方式）和重定向（重新指定输入输出的位置）操作中用到，后面的内容中会讲到管道操作和输入输出重定向操作。 显示文件或目录 - basename / dirname。 其他相关工具。 sort - 对内容排序 uniq - 去掉相邻重复内容 tr - 替换指定内容为新内容 cut / paste - 剪切/黏贴内容 split - 拆分文件 file - 判断文件类型 wc - 统计文件行数、单词数、字节数 iconv - 编码转换 1234567891011121314151617181920212223242526272829303132333435[root ~]# cat foo.txtgrapeapplepitaya[root ~]# cat bar.txt100200300400[root ~]# paste foo.txt bar.txtgrape 100apple 200pitaya 300 400[root ~]# paste foo.txt bar.txt &gt; hello.txt[root ~]# cut -b 4-8 hello.txtpe 10le 20aya 30[root ~]# cat hello.txt | tr '\t' ','grape,100apple,200pitaya,300,400[root ~]# split -l 100 sohu.html hello[root ~]# wget https://www.baidu.com/img/bd_logo1.png[root ~]# file bd_logo1.pngbd_logo1.png: PNG image data, 540 x 258, 8-bit colormap, non-interlaced[root ~]# wc sohu.html 2979 6355 212527 sohu.html[root ~]# wc -l sohu.html2979 sohu.html[root ~]# wget http://www.qq.com -O qq.html[root ~]# iconv -f gb2312 -t utf-8 qq.html 管道和重定向 管道的使用 - |。 例子：查找当前目录下文件个数。 12[root ~]# find ./ | wc -l6152 例子：列出当前路径下的文件和文件夹，给每一项加一个编号。 123456[root ~]# ls | cat -n 1 dump.rdb 2 mongodb-3.6.5 3 Python-3.6.5 4 redis-3.2.11 5 redis.conf 例子：查找record.log中包含AAA，但不包含BBB的记录的总数 1[root ~]# cat record.log | grep AAA | grep -v BBB | wc -l 输出重定向和错误重定向 - &gt; / &gt;&gt; / 2&gt;。 1234567891011121314151617[root ~]# cat readme.txtbananaapplegrapeapplegrapewatermelonpearpitaya[root ~]# cat readme.txt | sort | uniq &gt; result.txt[root ~]# cat result.txtapplebananagrapepearpitayawatermelon 输入重定向 - &lt;。 1234567891011[root ~]# echo 'hello, world!' &gt; hello.txt[root ~]# wall &lt; hello.txt[root ~]#Broadcast message from root (Wed Jun 20 19:43:05 2018):hello, world![root ~]# echo 'I will show you some code.' &gt;&gt; hello.txt[root ~]# wall &lt; hello.txt[root ~]#Broadcast message from root (Wed Jun 20 19:43:55 2018):hello, world!I will show you some code. 多重定向 - tee。 下面的命令除了在终端显示命令ls的结果之外，还会追加输出到ls.txt文件中。 1[root ~]# ls | tee -a ls.txt 别名 alias 1234567[root ~]# alias ll='ls -l'[root ~]# alias frm='rm -rf'[root ~]# ll...drwxr-xr-x 2 root root 4096 Jun 20 12:52 abc...[root ~]# frm abc unalias 123[root ~]# unalias frm[root ~]# frm sohu.html-bash: frm: command not found 文本处理 字符流编辑器 - sed。 sed是操作、过滤和转换文本内容的工具。假设有一个名为fruit.txt的文件，内容如下所示。 123456[root ~]# cat -n fruit.txt 1 banana 2 grape 3 apple 4 watermelon 5 orange 接下来，我们在第2行后面添加一个pitaya。 1234567[root ~]# sed '2a pitaya' fruit.txt bananagrapepitayaapplewatermelonorange 注意：刚才的命令和之前我们讲过的很多命令一样并没有改变fruit.txt文件，而是将添加了新行的内容输出到终端中，如果想保存到fruit.txt中，可以使用输出重定向操作。 在第2行前面插入一个waxberry。 1234567[root ~]# sed '2i waxberry' fruit.txtbananawaxberrygrapeapplewatermelonorange 删除第3行。 12345[root ~]# sed '3d' fruit.txtbananagrapewatermelonorange 删除第2行到第4行。 123[root ~]# sed '2,4d' fruit.txtbananaorange 将文本中的字符a替换为@。 123456[root ~]# sed 's#a#@#' fruit.txt b@nanagr@pe@pplew@termelonor@nge 将文本中的字符a替换为@，使用全局模式。 123456[root ~]# sed 's#a#@#g' fruit.txt b@n@n@gr@pe@pplew@termelonor@nge 模式匹配和处理语言 - awk。 awk是一种编程语言，也是Linux系统中处理文本最为强大的工具，它的作者之一和现在的维护者就是之前提到过的Brian Kernighan（ken和dmr最亲密的伙伴）。通过该命令可以从文本中提取出指定的列、用正则表达式从文本中取出我们想要的内容、显示指定的行以及进行统计和运算，总之它非常强大。 假设有一个名为fruit2.txt的文件，内容如下所示。 123456[root ~]# cat fruit2.txt 1 banana 1202 grape 5003 apple 12304 watermelon 805 orange 400 显示文件的第3行。 12[root ~]# awk 'NR==3' fruit2.txt 3 apple 1230 显示文件的第2列。 123456[root ~]# awk '&#123;print $2&#125;' fruit2.txt bananagrapeapplewatermelonorange 显示文件的最后一列。 123456[root ~]# awk '&#123;print $NF&#125;' fruit2.txt 120500123080400 输出末尾数字大于等于300的行。 1234[root ~]# awk '&#123;if($3 &gt;= 300) &#123;print $0&#125;&#125;' fruit2.txt 2 grape 5003 apple 12305 orange 400 上面展示的只是awk命令的冰山一角，更多的内容留给读者自己在实践中去探索。 用户管理 创建和删除用户 - useradd / userdel。 12[root home]# useradd hellokitty[root home]# userdel hellokitty -d - 创建用户时为用户指定用户主目录 -g - 创建用户时指定用户所属的用户组 创建和删除用户组 - groupadd / groupdel。 说明：用户组主要是为了方便对一个组里面所有用户的管理。 修改密码 - passwd。 1234[root ~]# passwd hellokittyNew password: Retype new password: passwd: all authentication tokens updated successfully. 说明：输入密码和确认密码没有回显且必须一气呵成的输入完成（不能使用退格键），密码和确认密码需要一致。如果使用passwd命令时没有指定命令作用的对象，则表示要修改当前用户的密码。如果想批量修改用户密码，可以使用chpasswd命令。 -l / -u - 锁定/解锁用户。 -d - 清除用户密码。 -e - 设置密码立即过期，用户登录时会强制要求修改密码。 -i - 设置密码过期多少天以后禁用该用户。 查看和修改密码有效期 - chage。 设置hellokitty用户100天后必须修改密码，过期前15天通知该用户，过期后15天禁用该用户。 1chage -M 100 -W 15 -I 15 hellokitty 切换用户 - su。 12[root ~]# su hellokitty[hellokitty root]$ 以管理员身份执行命令 - sudo。 1234[hellokitty ~]$ ls /rootls: cannot open directory /root: Permission denied[hellokitty ~]$ sudo ls /root[sudo] password for hellokitty: 说明：如果希望用户能够以管理员身份执行命令，用户必须要出现在sudoers名单中，sudoers文件在 /etc目录下，如果希望直接编辑该文件也可以使用下面的命令。 编辑sudoers文件 - visudo。 这里使用的编辑器是vi，关于vi的知识在后面有讲解。该文件的部分内容如下所示： 123456789101112131415161718## Allow root to run any commands anywhere root ALL=(ALL) ALL## Allows members of the &apos;sys&apos; group to run networking, software, ## service management apps and more.# %sys ALL = NETWORKING, SOFTWARE, SERVICES, STORAGE, DELEGATING, PROCESSES, LOCATE, DRIVERS## Allows people in group wheel to run all commands%wheel ALL=(ALL) ALL## Same thing without a password# %wheel ALL=(ALL) NOPASSWD: ALL## Allows members of the users group to mount and unmount the## cdrom as root# %users ALL=/sbin/mount /mnt/cdrom, /sbin/umount /mnt/cdrom## Allows members of the users group to shutdown this system# %users localhost=/sbin/shutdown -h now 显示用户与用户组的信息 - id。 给其他用户发消息 -write / wall。 发送方： 123[root ~]# write hellokittyDinner is on me.Call me at 6pm. 接收方： 12345[hellokitty ~]$ Message from root on pts/0 at 17:41 ...Dinner is on me.Call me at 6pm.EOF 查看/设置是否接收其他用户发送的消息 - mesg。 12345[hellokitty ~]$ mesgis y[hellokitty ~]$ mesg n[hellokitty ~]$ mesgis n 文件系统文件和路径 命名规则：文件名的最大长度与文件系统类型有关，一般情况下，文件名不应该超过255个字符，虽然绝大多数的字符都可以用于文件名，但是最好使用英文大小写字母、数字、下划线、点这样的符号。文件名中虽然可以使用空格，但应该尽可能避免使用空格，否则在输入文件名时需要用将文件名放在双引号中或者通过\对空格进行转义。 扩展名：在Linux系统下文件的扩展名是可选的，但是使用扩展名有助于对文件内容的理解。有些应用程序要通过扩展名来识别文件，但是更多的应用程序并不依赖文件的扩展名，就像file命令在识别文件时并不是依据扩展名来判定文件的类型。 隐藏文件：以点开头的文件在Linux系统中是隐藏文件（不可见文件）。 目录结构 /bin - 基本命令的二进制文件。 /boot - 引导加载程序的静态文件。 /dev - 设备文件。 /etc - 配置文件。 /home - 普通用户主目录的父目录。 /lib - 共享库文件。 /lib64 - 共享64位库文件。 /lost+found - 存放未链接文件。 /media - 自动识别设备的挂载目录。 /mnt - 临时挂载文件系统的挂载点。 /opt - 可选插件软件包安装位置。 /proc - 内核和进程信息。 /root - 超级管理员用户主目录。 /run - 存放系统运行时需要的东西。 /sbin - 超级用户的二进制文件。 /sys - 设备的伪文件系统。 /tmp - 临时文件夹。 /usr - 用户应用目录。 /var - 变量数据目录。 访问权限 chmod - 改变文件模式比特。 1234567891011121314[root ~]# ls -l...-rw-r--r-- 1 root root 211878 Jun 19 16:06 sohu.html...[root ~]# chmod g+w,o+w sohu.html[root ~]# ls -l...-rw-rw-rw- 1 root root 211878 Jun 19 16:06 sohu.html...[root ~]# chmod 644 sohu.html[root ~]# ls -l...-rw-r--r-- 1 root root 211878 Jun 19 16:06 sohu.html... 说明：通过上面的例子可以看出，用chmod改变文件模式比特有两种方式：一种是字符设定法，另一种是数字设定法。除了chmod之外，可以通过umask来设定哪些权限将在新文件的默认权限中被删除。 长格式查看目录或文件时显示结果及其对应权限的数值如下表所示。 chown - 改变文件所有者。 123456789[root ~]# ls -l...-rw-r--r-- 1 root root 54 Jun 20 10:06 readme.txt...[root ~]# chown hellokitty readme.txt[root ~]# ls -l...-rw-r--r-- 1 hellokitty root 54 Jun 20 10:06 readme.txt... chgrp - 改变用户组。 磁盘管理 列出文件系统的磁盘使用状况 - df。 12345678[root ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/vda1 40G 5.0G 33G 14% /devtmpfs 486M 0 486M 0% /devtmpfs 497M 0 497M 0% /dev/shmtmpfs 497M 356K 496M 1% /runtmpfs 497M 0 497M 0% /sys/fs/cgrouptmpfs 100M 0 100M 0% /run/user/0 磁盘分区表操作 - fdisk。 12345678910111213[root ~]# fdisk -lDisk /dev/vda: 42.9 GB, 42949672960 bytes, 83886080 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000a42f4 Device Boot Start End Blocks Id System/dev/vda1 * 2048 83884031 41940992 83 LinuxDisk /dev/vdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes 磁盘分区工具 - parted。 格式化文件系统 - mkfs。 1[root ~]# mkfs -t ext4 -v /dev/sdb -t - 指定文件系统的类型。 -c - 创建文件系统时检查磁盘损坏情况。 -v - 显示详细信息。 文件系统检查 - fsck。 转换或拷贝文件 - dd。 挂载/卸载 - mount / umount。 创建/激活/关闭交换分区 - mkswap / swapon / swapoff。 说明：执行上面这些命令会带有一定的风险，如果不清楚这些命令的用法，最好不用随意使用，在使用的过程中，最好对照参考资料进行操作，并在操作前确认是否要这么做。 编辑器 - vim 启动vim。可以通过vi或vim命令来启动vim，启动时可以指定文件名来打开一个文件，如果没有指定文件名，也可以在保存的时候指定文件名。 1[root ~]# vim guess.py 命令模式、编辑模式和末行模式：启动vim进入的是命令模式（也称为Normal模式），在命令模式下输入英文字母i会进入编辑模式（Insert模式），屏幕下方出现-- INSERT --提示；在编辑模式下按下Esc会回到命令模式，此时如果输入英文:会进入末行模式，在末行模式下输入q!可以在不保存当前工作的情况下强行退出vim；在命令模式下输入v会进入可视模式（Visual模式），可以用光标选择一个区域再完成对应的操作。 保存和退出vim：在命令模式下输入: 进入末行模式，输入wq可以实现保存退出；如果想放弃编辑的内容输入q!强行退出，这一点刚才已经提到过了；在命令模式下也可以直接输入ZZ实现保存退出。如果只想保存文件不退出，那么可以在末行模式下输入w；可以在w后面输入空格再指定要保存的文件名。 光标操作。 在命令模式下可以通过h、j、k、l来控制光标向左、下、上、右的方向移动，可以在字母前输入数字来表示移动的距离，例如：10h表示向左移动10个字符。 在命令模式下可以通过Ctrl+y和Ctrl+e来实现向上、向下滚动一行文本的操作，可以通过Ctrl+f和Ctrl+b来实现向前和向后翻页的操作。 在命令模式下可以通过输入英文字母G将光标移到文件的末尾，可以通过gg将光标移到文件的开始，也可以通过在G前输入数字来将光标移动到指定的行。 文本操作。 删除：在命令模式下可以用dd来删除整行；可以在dd前加数字来指定删除的行数；可以用d$来实现删除从光标处删到行尾的操作，也可以通过d0来实现从光标处删到行首的操作；如果想删除一个单词，可以使用dw；如果要删除全文，可以在输入:%d（其中:用来从命令模式进入末行模式）。 复制和粘贴：在命令模式下可以用yy来复制整行；可以在yy前加数字来指定复制的行数；可以通过p将复制的内容粘贴到光标所在的地方。 撤销和恢复：在命令模式下输入u可以撤销之前的操作；通过Ctrl+r可以恢复被撤销的操作。 对内容进行排序：在命令模式下输入%!sort。 查找和替换。 查找操作需要输入/进入末行模式并提供正则表达式来匹配与之对应的内容，例如：/doc.*\.，输入n来向前搜索，也可以输入N来向后搜索。 替换操作需要输入:进入末行模式并指定搜索的范围、正则表达式以及替换后的内容和匹配选项，例如：:1,$s/doc.*/hello/gice，其中： g - global：全局匹配。 i - ignore case：忽略大小写匹配。 c - confirm：替换时需要确认。 e - error：忽略错误。 参数设定：在输入:进入末行模式后可以对vim进行设定。 设置Tab键的空格数：set ts=4 设置显示/不显示行号：set nu / set nonu 设置启用/关闭高亮语法：syntax on / syntax off 设置显示标尺（光标所在的行和列）： set ruler 设置启用/关闭搜索结果高亮：set hls / set nohls 说明：如果希望上面的这些设定在每次启动vim时都能自动生效，需要将这些设定写到用户主目录下的.vimrc文件中。 高级技巧 比较多个文件。 1[root ~]# vim -d foo.txt bar.txt 打开多个文件。 1[root ~]# vim foo.txt bar.txt hello.txt 启动vim后只有一个窗口显示的是foo.txt，可以在末行模式中输入ls查看到打开的三个文件，也可以在末行模式中输入b &lt;num&gt;来显示另一个文件，例如可以用:b 2将bar.txt显示出来，可以用:b 3将hello.txt显示出来。 拆分和切换窗口。 可以在末行模式中输入sp或vs来实现对窗口的水平或垂直拆分，这样我们就可以同时打开多个编辑窗口，通过按两次Ctrl+w就可以实现编辑窗口的切换，在一个窗口中执行退出操作只会关闭对应的窗口，其他的窗口继续保留。 映射快捷键：在vim下可以将一些常用操作映射为快捷键来提升工作效率。 例子1：在命令模式下输入F4执行从第一行开始删除10000行代码的操作。 :map &lt;F4&gt; gg10000dd。 例子2：在编辑模式下输入__main直接补全为if __name__ == &#39;__main__&#39;:。 :inoremap __main if __name__ == &#39;__main__&#39;: 说明：上面例子2的inoremap中的i表示映射的键在编辑模式使用， nore表示不要递归，这一点非常重要，否则如果键对应的内容中又出现键本身，就会引发递归（相当于进入了死循环）。如果希望映射的快捷键每次启动vim时都能生效，需要将映射写到用户主目录下的.vimrc文件中。 录制宏。 在命令模式下输入qa开始录制宏（其中a是寄存器的名字，也可以是其他英文字母或0-9的数字）。 执行你的操作（光标操作、编辑操作等），这些操作都会被录制下来。 如果录制的操作已经完成了，按q结束录制。 通过@a（a是刚才使用的寄存器的名字）播放宏，如果要多次执行宏可以在前面加数字，例如100@a表示将宏播放100次。 可以试一试下面的例子来体验录制宏的操作，该例子来源于Harttle Land网站，该网站上提供了很多关于vim的使用技巧，有兴趣的可以了解一下。 软件安装和配置使用包管理工具 yum - Yellowdog Updater Modified。 yum search：搜索软件包，例如yum search nginx。 yum list installed：列出已经安装的软件包，例如yum list installed | grep zlib。 yum install：安装软件包，例如yum install nginx。 yum remove：删除软件包，例如yum remove nginx。 yum update：更新软件包，例如yum update可以更新所有软件包，而yum update tar只会更新tar。 yum check-update：检查有哪些可以更新的软件包。 yum info：显示软件包的相关信息，例如yum info nginx。 rpm - Redhat Package Manager。 安装软件包：rpm -ivh &lt;packagename&gt;.rpm。 移除软件包：rpm -e &lt;packagename&gt;。 查询软件包：rpm -qa，例如可以用rpm -qa | grep mysql来检查是否安装了MySQL相关的软件包。 下面以Nginx为例，演示如何使用yum安装软件。 123456789101112131415161718192021222324252627282930313233[root ~]# yum -y install nginx...Installed: nginx.x86_64 1:1.12.2-2.el7Dependency Installed: nginx-all-modules.noarch 1:1.12.2-2.el7 nginx-mod-http-geoip.x86_64 1:1.12.2-2.el7 nginx-mod-http-image-filter.x86_64 1:1.12.2-2.el7 nginx-mod-http-perl.x86_64 1:1.12.2-2.el7 nginx-mod-http-xslt-filter.x86_64 1:1.12.2-2.el7 nginx-mod-mail.x86_64 1:1.12.2-2.el7 nginx-mod-stream.x86_64 1:1.12.2-2.el7Complete![root ~]# yum info nginxLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfileInstalled PackagesName : nginxArch : x86_64Epoch : 1Version : 1.12.2Release : 2.el7Size : 1.5 MRepo : installedFrom repo : epelSummary : A high performance web server and reverse proxy serverURL : http://nginx.org/License : BSDDescription : Nginx is a web server and a reverse proxy server for HTTP, SMTP, POP3 and : IMAP protocols, with a strong focus on high concurrency, performance and low : memory usage.[root ~]# nginx -vnginx version: nginx/1.12.2 移除Nginx。 1[root ~]# yum -y remove nginx 下面以MySQL为例，演示如何使用rpm安装软件。要安装MySQL需要先到MySQL官方网站下载对应的RPM文件，当然要选择和你使用的Linux系统对应的版本。MySQL现在是Oracle公司旗下的产品，在MySQL被收购后，MySQL的作者重新制作了一个MySQL的分支MariaDB，可以通过yum进行安装。 123456789101112131415[root mysql]# lsmysql-community-client-5.7.22-1.el7.x86_64.rpmmysql-community-common-5.7.22-1.el7.x86_64.rpmmysql-community-libs-5.7.22-1.el7.x86_64.rpmmysql-community-server-5.7.22-1.el7.x86_64.rpm[root mysql]# yum -y remove mariadb-libs[root mysql]# yum -y install libaio[root mysql]#rpm -ivh mysql-community-common-5.7.26-1.el7.x86_64.rpm...[root mysql]#rpm -ivh mysql-community-libs-5.7.26-1.el7.x86_64.rpm...[root mysql]#rpm -ivh mysql-community-client-5.7.26-1.el7.x86_64.rpm...[root mysql]#rpm -ivh mysql-community-server-5.7.26-1.el7.x86_64.rpm... 说明：由于MySQL和MariaDB的底层依赖库是有冲突的，所以上面我们首先用yum移除了名为mariadb-libs的依赖库并安装了名为libaio支持异步I/O操作的依赖库。关于MySQL和MariaDB之间的关系，可以阅读维基百科上关于MariaDB的介绍。 移除安装的MySQL。 1[root ~]# rpm -qa | grep mysql | xargs rpm -e 下载解压配置环境变量下面以安装MongoDB为例，演示这类软件应该如何安装。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root ~]# wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-3.6.5.tgz--2018-06-21 18:32:53-- https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-3.6.5.tgzResolving fastdl.mongodb.org (fastdl.mongodb.org)... 52.85.83.16, 52.85.83.228, 52.85.83.186, ...Connecting to fastdl.mongodb.org (fastdl.mongodb.org)|52.85.83.16|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 100564462 (96M) [application/x-gzip]Saving to: ‘mongodb-linux-x86_64-rhel70-3.6.5.tgz’100%[==================================================&gt;] 100,564,462 630KB/s in 2m 9s2018-06-21 18:35:04 (760 KB/s) - ‘mongodb-linux-x86_64-rhel70-3.6.5.tgz’ saved [100564462/100564462][root ~]# gunzip mongodb-linux-x86_64-rhel70-3.6.5.tgz[root ~]# tar -xvf mongodb-linux-x86_64-rhel70-3.6.5.tarmongodb-linux-x86_64-rhel70-3.6.5/READMEmongodb-linux-x86_64-rhel70-3.6.5/THIRD-PARTY-NOTICESmongodb-linux-x86_64-rhel70-3.6.5/MPL-2mongodb-linux-x86_64-rhel70-3.6.5/GNU-AGPL-3.0mongodb-linux-x86_64-rhel70-3.6.5/bin/mongodumpmongodb-linux-x86_64-rhel70-3.6.5/bin/mongorestoremongodb-linux-x86_64-rhel70-3.6.5/bin/mongoexportmongodb-linux-x86_64-rhel70-3.6.5/bin/mongoimportmongodb-linux-x86_64-rhel70-3.6.5/bin/mongostatmongodb-linux-x86_64-rhel70-3.6.5/bin/mongotopmongodb-linux-x86_64-rhel70-3.6.5/bin/bsondumpmongodb-linux-x86_64-rhel70-3.6.5/bin/mongofilesmongodb-linux-x86_64-rhel70-3.6.5/bin/mongoreplaymongodb-linux-x86_64-rhel70-3.6.5/bin/mongoperfmongodb-linux-x86_64-rhel70-3.6.5/bin/mongodmongodb-linux-x86_64-rhel70-3.6.5/bin/mongosmongodb-linux-x86_64-rhel70-3.6.5/bin/mongomongodb-linux-x86_64-rhel70-3.6.5/bin/install_compass[root ~]# vim .bash_profile...PATH=$PATH:$HOME/bin:$HOME/mongodb-linux-x86_64-rhel70-3.6.5/binexport PATH...[root ~]# source .bash_profile[root ~]# mongod --versiondb version v3.6.5git version: a20ecd3e3a174162052ff99913bc2ca9a839d618OpenSSL version: OpenSSL 1.0.1e-fips 11 Feb 2013allocator: tcmallocmodules: nonebuild environment: distmod: rhel70 distarch: x86_64 target_arch: x86_64[root ~]# mongo --versionMongoDB shell version v3.6.5git version: a20ecd3e3a174162052ff99913bc2ca9a839d618OpenSSL version: OpenSSL 1.0.1e-fips 11 Feb 2013allocator: tcmallocmodules: nonebuild environment: distmod: rhel70 distarch: x86_64 target_arch: x86_64 说明：当然也可以通过yum来安装MongoDB，具体可以参照官方网站上给出的说明。 源代码构建安装 安装Python 3.6。 1234567891011121314[root ~]# yum install gcc[root ~]# wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz[root ~]# gunzip Python-3.6.5.tgz[root ~]# tar -xvf Python-3.6.5.tar[root ~]# cd Python-3.6.5[root ~]# ./configure --prefix=/usr/local/python36 --enable-optimizations[root ~]# yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel[root ~]# make &amp;&amp; make install...[root ~]# ln -s /usr/local/python36/bin/python3.6 /usr/bin/python3[root ~]# python3 --versionPython 3.6.5[root ~]# python3 -m pip install -U pip[root ~]# pip3 --version 说明：上面在安装好Python之后还需要注册PATH环境变量，将Python安装路径下bin文件夹的绝对路径注册到PATH环境变量中。注册环境变量可以修改用户主目录下的.bash_profile或者/etc目录下的profile文件，二者的区别在于前者相当于是用户环境变量，而后者相当于是系统环境变量。 安装Redis-3.2.12。 123456789[root ~]# wget http://download.redis.io/releases/redis-3.2.12.tar.gz[root ~]# gunzip redis-3.2.12.tar.gz[root ~]# tar -xvf redis-3.2.12.tar[root ~]# cd redis-3.2.12[root ~]# make &amp;&amp; make install[root ~]# redis-server --versionRedis server v=3.2.12 sha=00000000:0 malloc=jemalloc-4.0.3 bits=64 build=5bc5cd3c03d6ceb6[root ~]# redis-cli --versionredis-cli 3.2.12 配置服务我们可以Linux系统下安装和配置各种服务，也就是说我们可以把Linux系统打造成数据库服务器、Web服务器、缓存服务器、文件服务器、消息队列服务器等等。Linux下的大多数服务都被设置为守护进程（驻留在系统后台运行，但不会因为服务还在运行而导致Linux无法停止运行），所以我们安装的服务通常名字后面都有一个字母d，它是英文单词daemon的缩写，例如：防火墙服务叫firewalld，我们之前安装的MySQL服务叫mysqld，Apache服务器叫httpd等。在安装好服务之后，可以使用systemctl命令或service命令来完成对服务的启动、停止等操作，具体操作如下所示。 启动防火墙服务。 1[root ~]# systemctl start firewalld 终止防火墙服务。 1[root ~]# systemctl stop firewalld 重启防火墙服务。 1[root ~]# systemctl restart firewalld 查看防火墙服务状态。 1[root ~]# systemctl status firewalld 设置/禁用防火墙服务开机自启。 123456[root ~]# systemctl enable firewalldCreated symlink from /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service to /usr/lib/systemd/system/firewalld.service.Created symlink from /etc/systemd/system/multi-user.target.wants/firewalld.service to /usr/lib/systemd/system/firewalld.service.[root ~]# systemctl disable firewalldRemoved symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service. 计划任务 在指定的时间执行命令 at - 将任务排队，在指定的时间执行。 atq - 查看待执行的任务队列。 atrm - 从队列中删除待执行的任务。 指定3天以后下午5点要执行的任务。 1234[root ~]# at 5pm+3daysat&gt; rm -f /root/*.htmlat&gt; &lt;EOT&gt;job 9 at Wed Jun 5 17:00:00 2019 查看待执行的任务队列。 12[root ~]# atq9 Wed Jun 5 17:00:00 2019 a root 从队列中删除指定的任务。 1[root ~]$ atrm 9 计划任务表 - crontab。 123[root ~]# crontab -e* * * * * echo "hello, world!" &gt;&gt; /root/hello.txt59 23 * * * rm -f /root/*.log 说明：输入crontab -e命令会打开vim来编辑Cron表达式并指定触发的任务，上面我们定制了两个计划任务，一个是每分钟向/root目录下的hello.txt中追加输出hello, world!；另一个是每天23时59分执行删除/root目录下以log为后缀名的文件。如果不知道Cron表达式如何书写，可以参照/etc/crontab文件中的提示（下面会讲到）或者用搜索引擎找一下“Cron表达式在线生成器”来生成Cron表达式。 和crontab相关的文件在/etc目录下，通过修改/etc目录下的crontab文件也能够定制计划任务。 12345678910111213141516171819202122232425[root ~]# cd /etc[root etc]# ls -l | grep cron-rw-------. 1 root root 541 Aug 3 2017 anacrontabdrwxr-xr-x. 2 root root 4096 Mar 27 11:56 cron.ddrwxr-xr-x. 2 root root 4096 Mar 27 11:51 cron.daily-rw-------. 1 root root 0 Aug 3 2017 cron.denydrwxr-xr-x. 2 root root 4096 Mar 27 11:50 cron.hourlydrwxr-xr-x. 2 root root 4096 Jun 10 2014 cron.monthly-rw-r--r-- 1 root root 493 Jun 23 15:09 crontabdrwxr-xr-x. 2 root root 4096 Jun 10 2014 cron.weekly[root etc]# vim crontab 1 SHELL=/bin/bash 2 PATH=/sbin:/bin:/usr/sbin:/usr/bin 3 MAILTO=root 4 5 # For details see man 4 crontabs 6 7 # Example of job definition: 8 # .---------------- minute (0 - 59) 9 # | .------------- hour (0 - 23) 10 # | | .---------- day of month (1 - 31) 11 # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... 12 # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat 13 # | | | | | 14 # * * * * * user-name command to be executed 网络访问和管理 安全远程连接 - ssh。 1234567[root ~]$ ssh root@120.77.222.217The authenticity of host '120.77.222.217 (120.77.222.217)' can't be established.ECDSA key fingerprint is SHA256:BhUhykv+FvnIL03I9cLRpWpaCxI91m9n7zBWrcXRa8w.ECDSA key fingerprint is MD5:cc:85:e9:f0:d7:07:1a:26:41:92:77:6b:7f:a0:92:65.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added '120.77.222.217' (ECDSA) to the list of known hosts.root@120.77.222.217's password: 通过网络获取资源 - wget。 -b 后台下载模式 -O 下载到指定的目录 -r 递归下载 发送和接收邮件 - mail。 网络配置工具（旧） - ifconfig。 12345678[root ~]# ifconfig eth0eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.18.61.250 netmask 255.255.240.0 broadcast 172.18.63.255 ether 00:16:3e:02:b6:46 txqueuelen 1000 (Ethernet) RX packets 1067841 bytes 1296732947 (1.2 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 409912 bytes 43569163 (41.5 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 网络配置工具（新） - ip。 123456789[root ~]# ip address1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:16:3e:02:b6:46 brd ff:ff:ff:ff:ff:ff inet 172.18.61.250/20 brd 172.18.63.255 scope global eth0 valid_lft forever preferred_lft forever 网络可达性检查 - ping。 12345678[root ~]# ping www.baidu.com -c 3PING www.a.shifen.com (220.181.111.188) 56(84) bytes of data.64 bytes from 220.181.111.188 (220.181.111.188): icmp_seq=1 ttl=51 time=36.3 ms64 bytes from 220.181.111.188 (220.181.111.188): icmp_seq=2 ttl=51 time=36.4 ms64 bytes from 220.181.111.188 (220.181.111.188): icmp_seq=3 ttl=51 time=36.4 ms--- www.a.shifen.com ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2002msrtt min/avg/max/mdev = 36.392/36.406/36.427/0.156 ms 显示或管理路由表 - route。 查看网络服务和端口 - netstat / ss。 1[root ~]# netstat -nap | grep nginx 网络监听抓包 - tcpdump。 安全文件拷贝 - scp。 1[root ~]# scp root@1.2.3.4:/root/guido.jpg hellokitty@4.3.2.1:/home/hellokitty/pic.jpg 文件同步工具 - rsync。 说明：使用rsync可以实现文件的自动同步，这个对于文件服务器来说相当重要。关于这个命令的用法，我们在后面讲项目部署的时候为大家详细说明。 安全文件传输 - sftp。 1234[root ~]# sftp root@1.2.3.4root@1.2.3.4's password:Connected to 1.2.3.4.sftp&gt; help：显示帮助信息。 ls/lls：显示远端/本地目录列表。 cd/lcd：切换远端/本地路径。 mkdir/lmkdir：创建远端/本地目录。 pwd/lpwd：显示远端/本地当前工作目录。 get：下载文件。 put：上传文件。 rm：删除远端文件。 bye/exit/quit：退出sftp。 进程管理 查看进程 - ps。 12345678[root ~]# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 Jun23 ? 00:00:05 /usr/lib/systemd/systemd --switched-root --system --deserialize 21root 2 0 0 Jun23 ? 00:00:00 [kthreadd]...[root ~]# ps -ef | grep mysqldroot 4943 4581 0 22:45 pts/0 00:00:00 grep --color=auto mysqldmysql 25257 1 0 Jun25 ? 00:00:39 /usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid 显示进程状态树 - pstree。 123456789101112131415161718192021[root ~]# pstreesystemd─┬─AliYunDun───18*[&#123;AliYunDun&#125;] ├─AliYunDunUpdate───3*[&#123;AliYunDunUpdate&#125;] ├─2*[agetty] ├─aliyun-service───2*[&#123;aliyun-service&#125;] ├─atd ├─auditd───&#123;auditd&#125; ├─dbus-daemon ├─dhclient ├─irqbalance ├─lvmetad ├─mysqld───28*[&#123;mysqld&#125;] ├─nginx───2*[nginx] ├─ntpd ├─polkitd───6*[&#123;polkitd&#125;] ├─rsyslogd───2*[&#123;rsyslogd&#125;] ├─sshd───sshd───bash───pstree ├─systemd-journal ├─systemd-logind ├─systemd-udevd └─tuned───4*[&#123;tuned&#125;] 查找与指定条件匹配的进程 - pgrep。 12[root ~]$ pgrep mysqld3584 通过进程号终止进程 - kill。 12345678910111213141516[root ~]$ kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR111) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+338) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+1348) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-1253) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-758) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX[root ~]# kill 1234[root ~]# kill -9 1234 例子：用一条命令强制终止正在运行的Redis进程。 1ps -ef | grep redis | grep -v grep | awk '&#123;print $2&#125;' | xargs kill 通过进程名终止进程 - killall / pkill。 结束名为mysqld的进程。 1[root ~]# pkill mysqld 结束hellokitty用户的所有进程。 1[root ~]# pkill -u hellokitty 说明：这样的操作会让hellokitty用户和服务器断开连接。 将进程置于后台运行。 Ctrl+Z - 快捷键，用于停止进程并置于后台。 &amp; - 将进程置于后台运行。 12345[root ~]# mongod &amp;[root ~]# redis-server...^Z[4]+ Stopped redis-server 查询后台进程 - jobs。 1234[root ~]# jobs[2] Running mongod &amp;[3]- Stopped cat[4]+ Stopped redis-server 让进程在后台继续运行 - bg。 123456[root ~]# bg %4[4]+ redis-server &amp;[root ~]# jobs[2] Running mongod &amp;[3]+ Stopped cat[4]- Running redis-server &amp; 将后台进程置于前台 - fg。 12[root ~]# fg %4redis-server 说明：置于前台的进程可以使用Ctrl+C来终止它。 调整程序/进程运行时优先级 - nice / renice。 用户登出后进程继续工作 - nohup。 1[root ~]# nohup ping www.baidu.com &gt; result.txt &amp; 跟踪进程系统调用情况 - strace。 1234567891011121314[root ~]# pgrep mysqld8803[root ~]# strace -c -p 8803strace: Process 8803 attached^Cstrace: Process 8803 detached% time seconds usecs/call calls errors syscall------ ----------- ----------- --------- --------- ---------------- 99.18 0.005719 5719 1 restart_syscall 0.49 0.000028 28 1 mprotect 0.24 0.000014 14 1 clone 0.05 0.000003 3 1 mmap 0.03 0.000002 2 1 accept------ ----------- ----------- --------- --------- ----------------100.00 0.005766 5 total 说明：这个命令的用法和参数都比较复杂，建议大家在真正用到这个命令的时候再根据实际需要进行了解。 查看当前运行级别 - runlevel。 12[root ~]# runlevelN 3 实时监控进程占用资源状况 - top。 1234567[root ~]# toptop - 23:04:23 up 3 days, 14:10, 1 user, load average: 0.00, 0.01, 0.05Tasks: 65 total, 1 running, 64 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.3 us, 0.3 sy, 0.0 ni, 99.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 1016168 total, 191060 free, 324700 used, 500408 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 530944 avail Mem... -c - 显示进程的整个路径。 -d - 指定两次刷屏之间的间隔时间（秒为单位）。 -i - 不显示闲置进程或僵尸进程。 -p - 显示指定进程的信息。 系统诊断 系统启动异常诊断 - dmesg。 查看系统活动信息 - sar。 12345678[root ~]# sar -u -r 5 10Linux 3.10.0-957.10.1.el7.x86_64 (izwz97tbgo9lkabnat2lo8z) 06/02/2019 _x86_64_ (2 CPU)06:48:30 PM CPU %user %nice %system %iowait %steal %idle06:48:35 PM all 0.10 0.00 0.10 0.00 0.00 99.8006:48:30 PM kbmemfree kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty06:48:35 PM 1772012 2108392 54.33 102816 1634528 784940 20.23 793328 1164704 0 -A - 显示所有设备（CPU、内存、磁盘）的运行状况。 -u - 显示所有CPU的负载情况。 -d - 显示所有磁盘的使用情况。 -r - 显示内存的使用情况。 -n - 显示网络运行状态。 查看内存使用情况 - free。 1234[root ~]# free total used free shared buff/cache availableMem: 1016168 323924 190452 356 501792 531800Swap: 0 0 0 虚拟内存统计 - vmstat。 1234[root ~]# vmstatprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 2 0 0 204020 79036 667532 0 0 5 18 101 58 1 0 99 0 0 CPU信息统计 - mpstat。 12345[root ~]# mpstatLinux 3.10.0-957.5.1.el7.x86_64 (iZ8vba0s66jjlfmo601w4xZ) 05/30/2019 _x86_64_ (1 CPU)01:51:54 AM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle01:51:54 AM all 0.71 0.00 0.17 0.04 0.00 0.00 0.00 0.00 0.00 99.07 查看进程使用内存状况 - pmap。 1234567891011121314[root ~]# ps PID TTY TIME CMD 4581 pts/0 00:00:00 bash 5664 pts/0 00:00:00 ps[root ~]# pmap 45814581: -bash0000000000400000 884K r-x-- bash00000000006dc000 4K r---- bash00000000006dd000 36K rw--- bash00000000006e6000 24K rw--- [ anon ]0000000001de0000 400K rw--- [ anon ]00007f82fe805000 48K r-x-- libnss_files-2.17.so00007f82fe811000 2044K ----- libnss_files-2.17.so... 报告设备CPU和I/O统计信息 - iostat。 1234567[root ~]# iostatLinux 3.10.0-693.11.1.el7.x86_64 (iZwz97tbgo9lkabnat2lo8Z) 06/26/2018 _x86_64_ (1 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.79 0.00 0.20 0.04 0.00 98.97Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnvda 0.85 6.78 21.32 2106565 6623024vdb 0.00 0.01 0.00 2088 0 显示所有PCI设备 - lspci。 123456789101112[root ~]# lspci00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC [Natoma] (rev 02)00:01.0 ISA bridge: Intel Corporation 82371SB PIIX3 ISA [Natoma/Triton II]00:01.1 IDE interface: Intel Corporation 82371SB PIIX3 IDE [Natoma/Triton II]00:01.2 USB controller: Intel Corporation 82371SB PIIX3 USB [Natoma/Triton II] (rev 01)00:01.3 Bridge: Intel Corporation 82371AB/EB/MB PIIX4 ACPI (rev 03)00:02.0 VGA compatible controller: Cirrus Logic GD 544600:03.0 Ethernet controller: Red Hat, Inc. Virtio network device00:04.0 Communication controller: Red Hat, Inc. Virtio console00:05.0 SCSI storage controller: Red Hat, Inc. Virtio block device00:06.0 SCSI storage controller: Red Hat, Inc. Virtio block device00:07.0 Unclassified device [00ff]: Red Hat, Inc. Virtio memory balloon 显示进程间通信设施的状态 - ipcs。 12345678910[root ~]# ipcs------ Message Queues --------key msqid owner perms used-bytes messages ------ Shared Memory Segments --------key shmid owner perms bytes nattch status ------ Semaphore Arrays --------key semid owner perms nsems Shell编程之前我们提到过，Shell是一个连接用户和操作系统的应用程序，它提供了人机交互的界面（接口），用户通过这个界面访问操作系统内核的服务。Shell脚本是一种为Shell编写的脚本程序，我们可以通过Shell脚本来进行系统管理，同时也可以通过它进行文件操作。总之，编写Shell脚本对于使用Linux系统的人来说，应该是一项标配技能。 互联网上有大量关于Shell脚本的相关知识，我不打算再此对Shell脚本做一个全面系统的讲解，我们通过下面的代码来感性的认识下Shell脚本就行了。 例子1：输入两个整数m和n，计算从m到n的整数求和的结果。 12345678910111213#!/usr/bin/bashprintf 'm = 'read mprintf 'n = 'read na=$msum=0while [ $a -le $n ]do sum=$[ sum + a ] a=$[ a + 1 ]doneecho '结果: '$sum 例子2：自动创建文件夹和指定数量的文件。 1234567891011121314151617181920212223242526272829303132333435#!/usr/bin/bashprintf '输入文件名: 'read fileprintf '输入文件数量(&lt;1000): 'read numif [ $num -ge 1000 ]then echo '文件数量不能超过1000'else if [ -e $dir -a -d $dir ] then rm -rf $dir else if [ -e $dir -a -f $dir ] then rm -f $dir fi fi mkdir -p $dir index=1 while [ $index -le $num ] do if [ $index -lt 10 ] then pre='00' elif [ $index -lt 100 ] then pre='0' else pre='' fi touch $dir'/'$file'_'$pre$index index=$[ index + 1 ] donefi 例子3：自动安装指定版本的Redis。 123456789101112131415161718#!/usr/bin/bashinstall_redis() &#123; if ! which redis-server &gt; /dev/null then cd /root wget $1$2'.tar.gz' &gt;&gt; install.log gunzip /root/$2'.tar.gz' tar -xf /root/$2'.tar' cd /root/$2 make &gt;&gt; install.log make install &gt;&gt; install.log echo '安装完成' else echo '已经安装过Redis' fi&#125;install_redis 'http://download.redis.io/releases/' $1 相关资源 Linux命令行常用快捷键 快捷键 功能说明 tab 自动补全命令或路径 Ctrl+a 将光标移动到命令行行首 Ctrl+e 将光标移动到命令行行尾 Ctrl+f 将光标向右移动一个字符 Ctrl+b 将光标向左移动一个字符 Ctrl+k 剪切从光标到行尾的字符 Ctrl+u 剪切从光标到行首的字符 Ctrl+w 剪切光标前面的一个单词 Ctrl+y 复制剪切命名剪切的内容 Ctrl+c 中断正在执行的任务 Ctrl+h 删除光标前面的一个字符 Ctrl+d 退出当前命令行 Ctrl+r 搜索历史命令 Ctrl+g 退出历史命令搜索 Ctrl+l 清除屏幕上所有内容在屏幕的最上方开启一个新行 Ctrl+s 锁定终端使之暂时无法输入内容 Ctrl+q 退出终端锁定 Ctrl+z 将正在终端执行的任务停下来放到后台 !! 执行上一条命令 !数字 执行数字对应的历史命令 !字母 执行最近的以字母打头的命令 !$ / Esc+. 获得上一条命令最后一个参数 Esc+b 移动到当前单词的开头 Esc+f 移动到当前单词的结尾 man查阅命令手册的内容说明 手册中的标题 功能说明 NAME 命令的说明和介绍 SYNOPSIS 使用该命令的基本语法 DESCRIPTION 使用该命令的详细描述，各个参数的作用，有时候这些信息会出现在OPTIONS中 OPTIONS 命令相关参数选项的说明 EXAMPLES 使用该命令的参考例子 EXIT STATUS 命令结束的退出状态码，通常0表示成功执行 SEE ALSO 和命令相关的其他命令或信息 BUGS 和命令相关的缺陷的描述 AUTHOR 该命令的作者介绍]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[21-30.Web前端概述]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-21-30-Web%E5%89%8D%E7%AB%AF%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Web前端概述 说明：本文使用的部分插图来自Jon Duckett*先生的HTML and CSS: Design and Build Websites*一书，这是一本非常棒的前端入门书，有兴趣的读者可以在亚马逊或者其他网站上找到该书的购买链接。 HTML简史 1991年10月：一个非正式CERN（欧洲核子研究中心）文件首次公开18个HTML标签，这个文件的作者是物理学家蒂姆·伯纳斯-李，因此他是万维网的发明者，也是万维网联盟的主席。 1995年11月：HTML 2.0标准发布（RFC 1866）。 1997年1月：HTML 3.2作为W3C推荐标准发布。 1997年12月：HTML 4.0作为W3C推荐标准发布。 1999年12月：HTML4.01作为W3C推荐标准发布。 2008年1月：HTML5由W3C作为工作草案发布。 2011年5月：W3C将HTML5推进至“最终征求”（Last Call）阶段。 2012年12月：W3C指定HTML5作为“候选推荐”阶段。 2014年10月：HTML5作为稳定W3C推荐标准发布，这意味着HTML5的标准化已经完成。 HTML5新特性 引入原生多媒体支持（audio和video标签） 引入可编程内容（canvas标签） 引入语义Web（article、aside、details、figure、footer、header、nav、section、summary等标签） 引入新的表单控件（日历、邮箱、搜索、滑条等） 引入对离线存储更好的支持（localStorage和sessionStorage） 引入对定位、拖放、WebSocket、后台任务等的支持 使用标签承载内容结构 html head title meta body 文本 标题（heading）和段落（paragraph） h1 ~ h6 p 上标（superscript）和下标（subscript） sup sub 空白（白色空间折叠） 折行（break）和水平标尺（horizontal ruler） br hr 语义化标签 加粗和强调 - strong 引用 - blockquote 缩写词和首字母缩写词 - abbr / acronym 引文 - cite 所有者联系信息 - address 内容的修改 - ins / del 列表（list） 有序列表（ordered list）- ol / li 无序列表（unordered list）- ul / li 定义列表（definition list）- dl / dt / dd 链接（anchor） 页面链接 锚链接 功能链接 图像（image） 图像存储位置 图像及其宽高 选择正确的图像格式 JPEG GIF PNG 矢量图 语义化标签 - figure / figcaption 表格（table） 基本的表格结构 - table / tr / td / th 表格的标题 - caption 跨行和跨列 - rowspan属性 / colspan属性 长表格 - thead / tbody / tfoot 表单（form） 重要属性 - action / method / enctype 表单控件（input）- type属性 文本框 - text / 密码框 - password / 数字框 - number 邮箱 - email / 电话 - tel / 日期 - date / 滑条 - range / URL - url / 搜索 - search 单选按钮 - radio / 复选按钮 - checkbox 文件上传 - file / 隐藏域 - hidden 提交按钮 - submit / 图像按钮 - image / 重置按钮 - reset 下拉列表 - select / option 文本域（多行文本）- textarea 组合表单元素 - fieldset / legend 音视频（audio / video） 视频格式和播放器 视频托管服务 添加视频的准备工作 video标签和属性 - autoplay / controls / loop / muted / preload / src audio标签和属性 - autoplay / controls / loop / muted / preload / src / width / height / poster 窗口（frame） 框架集（过时，不建议使用） - frameset / frame 内嵌窗口 - iframe 其他 文档类型 1&lt;!doctype html&gt; 1&lt;!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd"&gt; 1&lt;!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt; 注释 1&lt;!-- 这是一段注释，注释不能够嵌套 --&gt; 属性 id：唯一标识 class：元素所属的类，用于区分不同的元素 title：元素的额外信息（鼠标悬浮时会显示工具提示文本） tabindex：Tab键切换顺序 contenteditable：元素是否可编辑 draggable：元素是否可拖拽 块级元素 / 行级元素 字符实体（实体替换符） 使用CSS渲染页面简介 CSS的作用 CSS的工作原理 规则、属性和值 常用选择器 颜色（color） 如何指定颜色 颜色术语和颜色对比 背景色 文本（text / font） 文本的大小和字型(font-size / font-family) 粗细、样式、拉伸和装饰(font-weight / font-style / font-stretch / text-decoration) 行间距(line-height)、字母间距(letter-spacing)和单词间距(word-spacing) 对齐(text-align)方式和缩进(text-ident) 链接样式（:link / :visited / :active / :hover） CSS3新属性 阴影效果 - text-shadow 首字母和首行文本(:first-letter / :first-line) 响应用户 盒子（box model） 盒子大小的控制（width / height） 盒子的边框、外边距和内边距（border / margin / padding） 盒子的显示和隐藏（display / visibility） CSS3新属性 边框图像（border-image） 投影（border-shadow） 圆角（border-radius） 列表、表格和表单 列表的项目符号（list-style） 表格的边框和背景（border-collapse） 表单控件的外观 表单控件的对齐 浏览器的开发者工具 图像 控制图像的大小（display: inline-block） 对齐图像 背景图像（background / background-image / background-repeat / background-position） 布局 控制元素的位置（position / z-index） 普通流 相对定位 绝对定位 固定定位 浮动元素（float / clear） 网站布局 HTML5布局 适配屏幕尺寸 固定宽度布局 流体布局 布局网格 使用JavaScript控制行为JavaScript基本语法 语句和注释 变量和数据类型 声明和赋值 简单数据类型和复杂数据类型 变量的命名规则 表达式和运算符 赋值运算符 算术运算符 比较运算符 逻辑运算符 分支结构 if...else... switch...cas...default... 循环结构 for循环 while循环 do...while循环 数组 创建数组 操作数组中的元素 函数 声明函数 调用函数 参数和返回值 匿名函数 立即调用函数 面向对象 对象的概念 创建对象的字面量语法 访问成员运算符 创建对象的构造函数语法 this关键字 添加和删除属性 delete关键字 标准对象 Number / String / Boolean / Symbol / Array / Function Date / Error / Math / RegEx / Object / Map / Set JSON / Promise / Generator / Reflect / Proxy BOM window对象的属性和方法 history对象 forward() / back() / go() location对象 navigator对象 screen对象 DOM DOM树 访问元素 getElementById() / querySelector() getElementsByClassName() / getElementsByTagName() / querySelectorAll() parentNode / previousSibling / nextSibling / children / firstChild / lastChild 操作元素 nodeValue innerHTML / textContent / createElement() / createTextNode() / appendChild() / insertBefore() / removeChild() className / id / hasAttribute() / getAttribute() / setAttribute() / removeAttribute() 事件处理 事件类型 UI事件：load / unload / error / resize / scroll 键盘事件：keydown / keyup / keypress 鼠标事件：click / dbclick / mousedown / mouseup / mousemove / mouseover / mouseout 焦点事件：focus / blur 表单事件：input / change / submit / reset / cut / copy / paste / select 事件绑定 HTML事件处理程序（不推荐使用，因为要做到标签与代码分离） 传统的DOM事件处理程序（只能附加一个回调函数） 事件监听器（旧的浏览器中不被支持） 事件流：事件捕获 / 事件冒泡 事件对象（低版本IE中的window.event） target（有些浏览器使用srcElement） type cancelable preventDefault() stopPropagation()（低版本IE中的cancelBubble） 鼠标事件 - 事件发生的位置 屏幕位置：screenX和screenY 页面位置：pageX和pageY 客户端位置：clientX和clientY 键盘事件 - 哪个键被按下了 keyCode属性（有些浏览器使用which） String.fromCharCode(event.keyCode) HTML5事件 DOMContentLoaded hashchange beforeunload JavaScript API 客户端存储 - localStorage和sessionStorage 123localStorage.colorSetting = '#a4509b';localStorage['colorSetting'] = '#a4509b';localStorage.setItem('colorSetting', '#a4509b'); 获取位置信息 - geolocation 1234navigator.geolocation.getCurrentPosition(function(pos) &#123; console.log(pos.coords.latitude) console.log(pos.coords.longitude)&#125;) 从服务器获取数据 - Fetch API 绘制图形 - &lt;canvas&gt;的API 音视频 - &lt;audio&gt;和&lt;video&gt;的API 使用jQueryjQuery概述 Write Less Do More（用更少的代码来完成更多的工作） 使用CSS选择器来查找元素（更简单更方便） 使用jQuery方法来操作元素（解决浏览器兼容性问题、应用于所有元素并施加多个方法） 引入jQuery 下载jQuery的开发版和压缩版 从CDN加载jQuery 12345&lt;script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"&gt;&lt;/script&gt;&lt;script&gt; window.jQuery || document.write('&lt;script src="js/jquery-3.3.1.min.js"&gt;&lt;/script&gt;')&lt;/script&gt; 查找元素 选择器 * / element / #id / .class / selector1, selector2 ancestor descendant / parent&gt;child / previous+next / previous~siblings 筛选器 基本筛选器：:not(selector) / :first / :last / :even / :odd / :eq(index) / :gt(index) / :lt(index) / :animated / :focus 内容筛选器：:contains(‘…’) / :empty / :parent / :has(selector) 可见性筛选器：:hidden / :visible 子节点筛选器：:nth-child(expr) / :first-child / :last-child / :only-child 属性筛选器：[attribute] / [attribute=’value’] / [attribute!=’value’] / [attribute^=’value’] / [attribute$=’value’] / [attribute|=’value’] / [attribute~=’value’] 表单：:input / :text / :password / :radio / :checkbox / :submit / :image / :reset / :button / :file / :selected / :enabled / :disabled / :checked 执行操作 内容操作 获取/修改内容：html() / text() / replaceWith() / remove() 获取/设置元素：before() / after() / prepend() / append() / remove() / clone() / unwrap() / detach() / empty() / add() 获取/修改属性：attr() / removeAttr() / addClass() / removeClass() / css() 获取/设置表单值：val() 查找操作 查找方法：find() / parent() / children() / siblings() / next() / nextAll() / prev() / prevAll() 筛选器：filter() / not() / has() / is() / contains() 索引编号：eq() 尺寸和位置 尺寸相关：height() / width() / innerHeight() / innerWidth() / outerWidth() / outerHeight() 位置相关：offset() / position() / scrollLeft() / scrollTop() 特效和动画 基本动画：show() / hide() / toggle() 消失出现：fadeIn() / fadeOut() / fadeTo() / fadeToggle() 滑动效果：slideDown() / slideUp() / slideToggle() 自定义：delay() / stop() / animate() 事件 文档加载：ready() / load() 用户交互：on() / off() 链式操作检测页面是否可用12345&lt;script&gt; $(document).ready(function() &#123; &#125;);&lt;/script&gt; 12345&lt;script&gt; $(function() &#123; &#125;);&lt;/script&gt; jQuery插件 jQuery Validation jQuery Treeview jQuery Autocomplete jQuery UI 避免和其他库的冲突先引入其他库再引入jQuery的情况。 12345678&lt;script src="other.js"&gt;&lt;/script&gt;&lt;script src="jquery.js"&gt;&lt;/script&gt;&lt;script&gt; jQuery.noConflict(); jQuery(function() &#123; jQuery('div').hide(); &#125;);&lt;/script&gt; 先引入jQuery再引入其他库的情况。 12345678&lt;script src="jquery.js"&gt;&lt;/script&gt;&lt;script src="other.js"&gt;&lt;/script&gt;&lt;script&gt; jQuery(function() &#123; jQuery('div').hide(); &#125;);&lt;/script&gt; 使用AjaxAjax是一种在无需重新加载整个网页的情况下，能够更新部分网页的技术。 原生的Ajax 基于jQuery的Ajax 加载内容 提交表单 前端框架渐进式框架 - Vue.js前后端分离开发（前端渲染）必选框架。 快速上手 引入Vue的JavaScript文件，我们仍然推荐从CDN服务器加载它。 1&lt;script src="https://cdn.jsdelivr.net/npm/vue"&gt;&lt;/script&gt; 数据绑定（声明式渲染 ）。 12345678910111213&lt;div id="app"&gt; &lt;h1&gt;&#123;&#123; product &#125;&#125;库存信息&lt;/h1&gt;&lt;/div&gt;&lt;script src="https://cdn.jsdelivr.net/npm/vue"&gt;&lt;/script&gt;&lt;script&gt; const app = new Vue(&#123; el: '#app', data: &#123; product: 'iPhone X' &#125; &#125;);&lt;/script&gt; 条件与循环。 1234567891011121314151617181920212223242526&lt;div id="app"&gt; &lt;h1&gt;库存信息&lt;/h1&gt; &lt;hr&gt; &lt;ul&gt; &lt;li v-for="product in products"&gt; &#123;&#123; product.name &#125;&#125; - &#123;&#123; product.quantity &#125;&#125; &lt;span v-if="product.quantity === 0"&gt; 已经售罄 &lt;/span&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;&lt;script src="https://cdn.jsdelivr.net/npm/vue"&gt;&lt;/script&gt;&lt;script&gt; const app = new Vue(&#123; el: '#app', data: &#123; products: [ &#123;"id": 1, "name": "iPhone X", "quantity": 20&#125;, &#123;"id": 2, "name": "华为 Mate20", "quantity": 0&#125;, &#123;"id": 3, "name": "小米 Mix3", "quantity": 50&#125; ] &#125; &#125;);&lt;/script&gt; 计算属性。 12345678910111213141516171819202122232425262728293031323334&lt;div id="app"&gt; &lt;h1&gt;库存信息&lt;/h1&gt; &lt;hr&gt; &lt;ul&gt; &lt;li v-for="product in products"&gt; &#123;&#123; product.name &#125;&#125; - &#123;&#123; product.quantity &#125;&#125; &lt;span v-if="product.quantity === 0"&gt; 已经售罄 &lt;/span&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;库存总量：&#123;&#123; totalQuantity &#125;&#125;台&lt;/h2&gt;&lt;/div&gt;&lt;script src="https://cdn.jsdelivr.net/npm/vue"&gt;&lt;/script&gt;&lt;script&gt; const app = new Vue(&#123; el: '#app', data: &#123; products: [ &#123;"id": 1, "name": "iPhone X", "quantity": 20&#125;, &#123;"id": 2, "name": "华为 Mate20", "quantity": 0&#125;, &#123;"id": 3, "name": "小米 Mix3", "quantity": 50&#125; ] &#125;, computed: &#123; totalQuantity() &#123; return this.products.reduce((sum, product) =&gt; &#123; return sum + product.quantity &#125;, 0); &#125; &#125; &#125;);&lt;/script&gt; 处理事件。 12345678910111213141516171819202122232425262728293031323334353637&lt;div id="app"&gt; &lt;h1&gt;库存信息&lt;/h1&gt; &lt;hr&gt; &lt;ul&gt; &lt;li v-for="product in products"&gt; &#123;&#123; product.name &#125;&#125; - &#123;&#123; product.quantity &#125;&#125; &lt;span v-if="product.quantity === 0"&gt; 已经售罄 &lt;/span&gt; &lt;button @click="product.quantity += 1"&gt; 增加库存 &lt;/button&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;库存总量：&#123;&#123; totalQuantity &#125;&#125;台&lt;/h2&gt;&lt;/div&gt;&lt;script src="https://cdn.jsdelivr.net/npm/vue"&gt;&lt;/script&gt;&lt;script&gt; const app = new Vue(&#123; el: '#app', data: &#123; products: [ &#123;"id": 1, "name": "iPhone X", "quantity": 20&#125;, &#123;"id": 2, "name": "华为 Mate20", "quantity": 0&#125;, &#123;"id": 3, "name": "小米 Mix3", "quantity": 50&#125; ] &#125;, computed: &#123; totalQuantity() &#123; return this.products.reduce((sum, product) =&gt; &#123; return sum + product.quantity &#125;, 0); &#125; &#125; &#125;);&lt;/script&gt; 用户输入。 1234567891011121314151617181920212223242526272829303132333435363738&lt;div id="app"&gt; &lt;h1&gt;库存信息&lt;/h1&gt; &lt;hr&gt; &lt;ul&gt; &lt;li v-for="product in products"&gt; &#123;&#123; product.name &#125;&#125; - &lt;input type="number" v-model.number="product.quantity" min="0"&gt; &lt;span v-if="product.quantity === 0"&gt; 已经售罄 &lt;/span&gt; &lt;button @click="product.quantity += 1"&gt; 增加库存 &lt;/button&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;库存总量：&#123;&#123; totalQuantity &#125;&#125;台&lt;/h2&gt;&lt;/div&gt;&lt;script src="https://cdn.jsdelivr.net/npm/vue"&gt;&lt;/script&gt;&lt;script&gt; const app = new Vue(&#123; el: '#app', data: &#123; products: [ &#123;"id": 1, "name": "iPhone X", "quantity": 20&#125;, &#123;"id": 2, "name": "华为 Mate20", "quantity": 0&#125;, &#123;"id": 3, "name": "小米 Mix3", "quantity": 50&#125; ] &#125;, computed: &#123; totalQuantity() &#123; return this.products.reduce((sum, product) =&gt; &#123; return sum + product.quantity &#125;, 0); &#125; &#125; &#125;);&lt;/script&gt; 通过网络加载JSON数据。 12345678910111213141516171819202122232425262728&lt;div id="app"&gt; &lt;h2&gt;库存信息&lt;/h2&gt; &lt;ul&gt; &lt;li v-for="product in products"&gt; &#123;&#123; product.name &#125;&#125; - &#123;&#123; product.quantity &#125;&#125; &lt;span v-if="product.quantity === 0"&gt; 已经售罄 &lt;/span&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;&lt;script src="https://cdn.jsdelivr.net/npm/vue"&gt;&lt;/script&gt;&lt;script&gt; const app = new Vue(&#123; el: '#app', data: &#123; products: [] &#125;， created() &#123; fetch('https://jackfrued.top/api/products') .then(response =&gt; response.json()) .then(json =&gt; &#123; this.products = json &#125;); &#125; &#125;);&lt;/script&gt; 使用脚手架 - vue-cliVue为商业项目开发提供了非常便捷的脚手架工具vue-cli，通过工具可以省去手工配置开发环境、测试环境和运行环境的步骤，让开发者只需要关注要解决的问题。 安装脚手架。 创建项目。 安装依赖包。 运行项目。 UI框架 - Element基于Vue 2.0的桌面端组件库，用于构造用户界面，支持响应式布局。 引入Element的CSS和JavaScript文件。 1234&lt;!-- 引入样式 --&gt;&lt;link rel="stylesheet" href="https://unpkg.com/element-ui/lib/theme-chalk/index.css"&gt;&lt;!-- 引入组件库 --&gt;&lt;script src="https://unpkg.com/element-ui/lib/index.js"&gt;&lt;/script&gt; 一个简单的例子。 12345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;link rel="stylesheet" href="https://unpkg.com/element-ui/lib/theme-chalk/index.css"&gt; &lt;/head&gt; &lt;body&gt; &lt;div id="app"&gt; &lt;el-button @click="visible = true"&gt;点我&lt;/el-button&gt; &lt;el-dialog :visible.sync="visible" title="Hello world"&gt; &lt;p&gt;开始使用Element吧&lt;/p&gt; &lt;/el-dialog&gt; &lt;/div&gt; &lt;/body&gt; &lt;script src="https://unpkg.com/vue/dist/vue.js"&gt;&lt;/script&gt; &lt;script src="https://unpkg.com/element-ui/lib/index.js"&gt;&lt;/script&gt; &lt;script&gt; new Vue(&#123; el: '#app', data: &#123; visible: false, &#125; &#125;) &lt;/script&gt;&lt;/html&gt; 使用组件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;link rel="stylesheet" href="https://unpkg.com/element-ui/lib/theme-chalk/index.css"&gt; &lt;/head&gt; &lt;body&gt; &lt;div id="app"&gt; &lt;el-table :data="tableData" stripe style="width: 100%"&gt; &lt;el-table-column prop="date" label="日期" width="180"&gt; &lt;/el-table-column&gt; &lt;el-table-column prop="name" label="姓名" width="180"&gt; &lt;/el-table-column&gt; &lt;el-table-column prop="address" label="地址"&gt; &lt;/el-table-column&gt; &lt;/el-table&gt; &lt;/div&gt; &lt;/body&gt; &lt;script src="https://unpkg.com/vue/dist/vue.js"&gt;&lt;/script&gt; &lt;script src="https://unpkg.com/element-ui/lib/index.js"&gt;&lt;/script&gt; &lt;script&gt; new Vue(&#123; el: '#app', data: &#123; tableData: [ &#123; date: '2016-05-02', name: '王一霸', address: '上海市普陀区金沙江路 1518 弄' &#125;, &#123; date: '2016-05-04', name: '刘二狗', address: '上海市普陀区金沙江路 1517 弄' &#125;, &#123; date: '2016-05-01', name: '杨三萌', address: '上海市普陀区金沙江路 1519 弄' &#125;, &#123; date: '2016-05-03', name: '陈四吹', address: '上海市普陀区金沙江路 1516 弄' &#125; ] &#125; &#125;) &lt;/script&gt;&lt;/html&gt; 报表框架 - ECharts百度出品的开源可视化库，常用于生成各种类型的报表。 基于弹性盒子的CSS框架 - BulmaBulma是一个基于Flexbox的现代化的CSS框架，其初衷就是移动优先（Mobile First），模块化设计，可以轻松用来实现各种简单或者复杂的内容布局，即使不懂CSS的开发者也能够使用它定制出漂亮的页面。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Bulma&lt;/title&gt; &lt;link href="https://cdn.bootcss.com/bulma/0.7.4/css/bulma.min.css" rel="stylesheet"&gt; &lt;style type="text/css"&gt; div &#123; margin-top: 10px; &#125; .column &#123; color: #fff; background-color: #063; margin: 10px 10px; text-align: center; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="columns"&gt; &lt;div class="column"&gt;1&lt;/div&gt; &lt;div class="column"&gt;2&lt;/div&gt; &lt;div class="column"&gt;3&lt;/div&gt; &lt;div class="column"&gt;4&lt;/div&gt; &lt;/div&gt; &lt;div&gt; &lt;a class="button is-primary"&gt;Primary&lt;/a&gt; &lt;a class="button is-link"&gt;Link&lt;/a&gt; &lt;a class="button is-info"&gt;Info&lt;/a&gt; &lt;a class="button is-success"&gt;Success&lt;/a&gt; &lt;a class="button is-warning"&gt;Warning&lt;/a&gt; &lt;a class="button is-danger"&gt;Danger&lt;/a&gt; &lt;/div&gt; &lt;div&gt; &lt;progress class="progress is-danger is-medium" max="100"&gt;60%&lt;/progress&gt; &lt;/div&gt; &lt;div&gt; &lt;table class="table is-hoverable"&gt; &lt;tr&gt; &lt;th&gt;One&lt;/th&gt; &lt;th&gt;Two&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Three&lt;/td&gt; &lt;td&gt;Four&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Five&lt;/td&gt; &lt;td&gt;Six&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Seven&lt;/td&gt; &lt;td&gt;Eight&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Nine&lt;/td&gt; &lt;td&gt;Ten&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Eleven&lt;/td&gt; &lt;td&gt;Twelve&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 响应式布局框架 - Bootstrap用于快速开发Web应用程序的前端框架，支持响应式布局。 特点 支持主流的浏览器和移动设备 容易上手 响应式设计 内容 网格系统 封装的CSS 现成的组件 JavaScript插件 可视化]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Web前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[16-20.Python语言进阶]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-16-20-Python%E8%AF%AD%E8%A8%80%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[Python语言进阶 数据结构和算法 算法：解决问题的方法和步骤 评价算法的好坏：渐近时间复杂度和渐近空间复杂度。 渐近时间复杂度的大O标记： - 常量时间复杂度 - 布隆过滤器 / 哈希存储 - 对数时间复杂度 - 折半查找（二分查找） - 线性时间复杂度 - 顺序查找 / 桶排序 - 对数线性时间复杂度 - 高级排序算法（归并排序、快速排序） - 平方时间复杂度 - 简单排序算法（选择排序、插入排序、冒泡排序） - 立方时间复杂度 - Floyd算法 / 矩阵乘法运算 - 几何级数时间复杂度 - 汉诺塔 - 阶乘时间复杂度 - 旅行经销商问题 - NP 排序算法（选择、冒泡和归并）和查找算法（顺序和折半） 12345678910def select_sort(origin_items, comp=lambda x, y: x &lt; y): """简单选择排序""" items = origin_items[:] for i in range(len(items) - 1): min_index = i for j in range(i + 1, len(items)): if comp(items[j], items[min_index]): min_index = j items[i], items[min_index] = items[min_index], items[i] return items 123456789101112131415161718def bubble_sort(origin_items, comp=lambda x, y: x &gt; y): """高质量冒泡排序(搅拌排序)""" items = origin_items[:] for i in range(len(items) - 1): swapped = False for j in range(i, len(items) - 1 - i): if comp(items[j], items[j + 1]): items[j], items[j + 1] = items[j + 1], items[j] swapped = True if swapped: swapped = False for j in range(len(items) - 2 - i, i, -1): if comp(items[j - 1], items[j]): items[j], items[j - 1] = items[j - 1], items[j] swapped = True if not swapped: break return items 123456789101112131415161718192021222324def merge_sort(items, comp=lambda x, y: x &lt;= y): """归并排序(分治法)""" if len(items) &lt; 2: return items[:] mid = len(items) // 2 left = merge_sort(items[:mid], comp) right = merge_sort(items[mid:], comp) return merge(left, right, comp)def merge(items1, items2, comp): """合并(将两个有序的列表合并成一个有序的列表)""" items = [] index1, index2 = 0, 0 while index1 &lt; len(items1) and index2 &lt; len(items2): if comp(items1[index1], items2[index2]): items.append(items1[index1]) index1 += 1 else: items.append(items2[index2]) index2 += 1 items += items1[index1:] items += items2[index2:] return items 123456def seq_search(items, key): """顺序查找""" for index, item in enumerate(items): if item == key: return index return -1 123456789101112def bin_search(items, key): """折半查找""" start, end = 0, len(items) - 1 while start &lt;= end: mid = (start + end) // 2 if key &gt; items[mid]: start = mid + 1 elif key &lt; items[mid]: end = mid - 1 else: return mid return -1 使用生成式（推导式）语法 123456789101112prices = &#123; 'AAPL': 191.88, 'GOOG': 1186.96, 'IBM': 149.24, 'ORCL': 48.44, 'ACN': 166.89, 'FB': 208.09, 'SYMC': 21.29&#125;# 用股票价格大于100元的股票构造一个新的字典prices2 = &#123;key: value for key, value in prices.items() if value &gt; 100&#125;print(prices2) 说明：生成式（推导式）可以用来生成列表、集合和字典。 嵌套的列表 12345678910names = ['关羽', '张飞', '赵云', '马超', '黄忠']courses = ['语文', '数学', '英语']# 录入五个学生三门课程的成绩# 错误 - 参考http://pythontutor.com/visualize.html#mode=edit# scores = [[None] * len(courses)] * len(names)scores = [[None] * len(courses) for _ in range(len(names))]for row, name in enumerate(names): for col, course in enumerate(courses): scores[row][col] = float(input(f'请输入&#123;name&#125;的&#123;course&#125;成绩: ')) print(scores) Python Tutor - VISUALIZE CODE AND GET LIVE HELP heapq、itertools等的用法 12345678910111213141516171819"""从列表中找出最大的或最小的N个元素堆结构(大根堆/小根堆)"""import heapqlist1 = [34, 25, 12, 99, 87, 63, 58, 78, 88, 92]list2 = [ &#123;'name': 'IBM', 'shares': 100, 'price': 91.1&#125;, &#123;'name': 'AAPL', 'shares': 50, 'price': 543.22&#125;, &#123;'name': 'FB', 'shares': 200, 'price': 21.09&#125;, &#123;'name': 'HPQ', 'shares': 35, 'price': 31.75&#125;, &#123;'name': 'YHOO', 'shares': 45, 'price': 16.35&#125;, &#123;'name': 'ACME', 'shares': 75, 'price': 115.65&#125;]print(heapq.nlargest(3, list1))print(heapq.nsmallest(3, list1))print(heapq.nlargest(2, list2, key=lambda x: x['price']))print(heapq.nlargest(2, list2, key=lambda x: x['shares'])) 12345678"""迭代工具 - 排列 / 组合 / 笛卡尔积"""import itertoolsitertools.permutations('ABCD')itertools.combinations('ABCDE', 3)itertools.product('ABCD', '123') collections模块下的工具类 12345678910111213"""找出序列中出现次数最多的元素"""from collections import Counterwords = [ 'look', 'into', 'my', 'eyes', 'look', 'into', 'my', 'eyes', 'the', 'eyes', 'the', 'eyes', 'the', 'eyes', 'not', 'around', 'the', 'eyes', "don't", 'look', 'around', 'the', 'eyes', 'look', 'into', 'my', 'eyes', "you're", 'under']counter = Counter(words)print(counter.most_common(3)) 常用算法： 穷举法 - 又称为暴力破解法，对所有的可能性进行验证，直到找到正确答案。 贪婪法 - 在对问题求解时，总是做出在当前看来 最好的选择，不追求最优解，快速找到满意解。 分治法 - 把一个复杂的问题分成两个或更多的相同或相似的子问题，再把子问题分成更小的子问题，直到可以直接求解的程度，最后将子问题的解进行合并得到原问题的解。 回溯法 - 回溯法又称为试探法，按选优条件向前搜索，当搜索到某一步发现原先选择并不优或达不到目标时，就退回一步重新选择。 动态规划 - 基本思想也是将待求解问题分解成若干个子问题，先求解并保存这些子问题的解，避免产生大量的重复运算。 穷举法例子：百钱百鸡和五人分鱼。 1234567891011121314151617181920212223242526# 公鸡5元一只 母鸡3元一只 小鸡1元三只# 用100元买100只鸡 问公鸡/母鸡/小鸡各多少只for x in range(20): for y in range(33): z = 100 - x - y if 5 * x + 3 * y + z // 3 == 100 and z % 3 == 0: print(x, y, z)# A、B、C、D、E五人在某天夜里合伙捕鱼 最后疲惫不堪各自睡觉# 第二天A第一个醒来 他将鱼分为5份 扔掉多余的1条 拿走自己的一份# B第二个醒来 也将鱼分为5份 扔掉多余的1条 拿走自己的一份# 然后C、D、E依次醒来也按同样的方式分鱼 问他们至少捕了多少条鱼fish = 6while True: total = fish enough = True for _ in range(5): if (total - 1) % 5 == 0: total = (total - 1) // 5 * 4 else: enough = False break if enough: print(fish) break fish += 5 贪婪法例子：假设小偷有一个背包，最多能装20公斤赃物，他闯入一户人家，发现如下表所示的物品。很显然，他不能把所有物品都装进背包，所以必须确定拿走哪些物品，留下哪些物品。 名称 价格（美元） 重量（kg） 电脑 200 20 收音机 20 4 钟 175 10 花瓶 50 2 书 10 1 油画 90 9 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950"""贪婪法：在对问题求解时，总是做出在当前看来是最好的选择，不追求最优解，快速找到满意解。输入：20 6电脑 200 20收音机 20 4钟 175 10花瓶 50 2书 10 1油画 90 9"""class Thing(object): """物品""" def __init__(self, name, price, weight): self.name = name self.price = price self.weight = weight @property def value(self): """价格重量比""" return self.price / self.weightdef input_thing(): """输入物品信息""" name_str, price_str, weight_str = input().split() return name_str, int(price_str), int(weight_str)def main(): """主函数""" max_weight, num_of_things = map(int, input().split()) all_things = [] for _ in range(num_of_things): all_things.append(Thing(*input_thing())) all_things.sort(key=lambda x: x.value, reverse=True) total_weight = 0 total_price = 0 for thing in all_things: if total_weight + thing.weight &lt;= max_weight: print(f'小偷拿走了&#123;thing.name&#125;') total_weight += thing.weight total_price += thing.price print(f'总价值: &#123;total_price&#125;美元')if __name__ == '__main__': main() 分治法例子：快速排序。 12345678910111213141516171819202122232425"""快速排序 - 选择枢轴对元素进行划分，左边都比枢轴小右边都比枢轴大"""def quick_sort(origin_items, comp=lambda x, y: x &lt;= y): items = origin_items[:] _quick_sort(items, 0, len(items) - 1, comp) return itemsdef _quick_sort(items, start, end, comp): if start &lt; end: pos = _partition(items, start, end, comp) _quick_sort(items, start, pos - 1, comp) _quick_sort(items, pos + 1, end, comp)def _partition(items, start, end, comp): pivot = items[end] i = start - 1 for j in range(start, end): if comp(items[j], pivot): i += 1 items[i], items[j] = items[j], items[i] items[i + 1], items[end] = items[end], items[i + 1] return i + 1 回溯法例子：骑士巡逻。 123456789101112131415161718192021222324252627282930313233343536373839404142434445"""递归回溯法：叫称为试探法，按选优条件向前搜索，当搜索到某一步，发现原先选择并不优或达不到目标时，就退回一步重新选择，比较经典的问题包括骑士巡逻、八皇后和迷宫寻路等。"""import sysimport timeSIZE = 5total = 0def print_board(board): for row in board: for col in row: print(str(col).center(4), end='') print()def patrol(board, row, col, step=1): if row &gt;= 0 and row &lt; SIZE and \ col &gt;= 0 and col &lt; SIZE and \ board[row][col] == 0: board[row][col] = step if step == SIZE * SIZE: global total total += 1 print(f'第&#123;total&#125;种走法: ') print_board(board) patrol(board, row - 2, col - 1, step + 1) patrol(board, row - 1, col - 2, step + 1) patrol(board, row + 1, col - 2, step + 1) patrol(board, row + 2, col - 1, step + 1) patrol(board, row + 2, col + 1, step + 1) patrol(board, row + 1, col + 2, step + 1) patrol(board, row - 1, col + 2, step + 1) patrol(board, row - 2, col + 1, step + 1) board[row][col] = 0def main(): board = [[0] * SIZE for _ in range(SIZE)] patrol(board, SIZE - 1, SIZE - 1)if __name__ == '__main__': main() 动态规划例子1：斐波拉切数列。（不使用动态规划将会是几何级数复杂度） 12345678910111213"""动态规划 - 适用于有重叠子问题和最优子结构性质的问题使用动态规划方法所耗时间往往远少于朴素解法(用空间换取时间)"""def fib(num, temp=&#123;&#125;): """用递归计算Fibonacci数""" if num in (1, 2): return 1 try: return temp[num] except KeyError: temp[num] = fib(num - 1) + fib(num - 2) return temp[num] 动态规划例子2：子列表元素之和的最大值。（使用动态规划可以避免二重循环） 说明：子列表指的是列表中索引（下标）连续的元素构成的列表；列表中的元素是int类型，可能包含正整数、0、负整数；程序输入列表中的元素，输出子列表元素求和的最大值，例如： 输入：1 -2 3 5 -3 2 输出：8 输入：0 -2 3 5 -1 2 输出：9 输入：-9 -2 -3 -5 -3 输出：-2 12345678910111213def main(): items = list(map(int, input().split())) size = len(items) overall, partial = &#123;&#125;, &#123;&#125; overall[size - 1] = partial[size - 1] = items[size - 1] for i in range(size - 2, -1, -1): partial[i] = max(items[i], partial[i + 1] + items[i]) overall[i] = max(partial[i], overall[i + 1]) print(overall[0])if __name__ == '__main__': main() 函数的使用方式 将函数视为“一等公民” 函数可以赋值给变量 函数可以作为函数的参数 函数可以作为函数的返回值 高阶函数的用法（filter、map以及它们的替代品） 12items1 = list(map(lambda x: x ** 2, filter(lambda x: x % 2, range(1, 10))))items2 = [x ** 2 for x in range(1, 10) if x % 2] 位置参数、可变参数、关键字参数、命名关键字参数 参数的元信息（代码可读性问题） 匿名函数和内联函数的用法（lambda函数） 闭包和作用域问题 Python搜索变量的LEGB顺序（Local –&gt; Embedded –&gt; Global –&gt; Built-in） global和nonlocal关键字的作用 global：声明或定义全局变量（要么直接使用现有的全局作用域的变量，要么定义一个变量放到全局作用域）。 nonlocal：声明使用嵌套作用域的变量（嵌套作用域必须存在该变量，否则报错）。 装饰器函数（使用装饰器和取消装饰器） 例子：输出函数执行时间的装饰器。 1234567891011def record_time(func): """自定义装饰函数的装饰器""" @wraps(func) def wrapper(*args, **kwargs): start = time() result = func(*args, **kwargs) print(f'&#123;func.__name__&#125;: &#123;time() - start&#125;秒') return result return wrapper 如果装饰器不希望跟print函数耦合，可以编写带参数的装饰器。 12345678910111213141516171819from functools import wrapsfrom time import timedef record(output): """自定义带参数的装饰器""" def decorate(func): @wraps(func) def wrapper(*args, **kwargs): start = time() result = func(*args, **kwargs) output(func.__name__, time() - start) return result return wrapper return decorate 1234567891011121314151617181920from functools import wrapsfrom time import timeclass Record(): """自定义装饰器类(通过__call__魔术方法使得对象可以当成函数调用)""" def __init__(self, output): self.output = output def __call__(self, func): @wraps(func) def wrapper(*args, **kwargs): start = time() result = func(*args, **kwargs) self.output(func.__name__, time() - start) return result return wrapper 说明：由于对带装饰功能的函数添加了@wraps装饰器，可以通过func.__wrapped__方式获得被装饰之前的函数或类来取消装饰器的作用。 例子：用装饰器来实现单例模式。 1234567891011121314151617181920from functools import wrapsdef singleton(cls): """装饰类的装饰器""" instances = &#123;&#125; @wraps(cls) def wrapper(*args, **kwargs): if cls not in instances: instances[cls] = cls(*args, **kwargs) return instances[cls] return wrapper@singletonclass President(): """总统(单例类)""" pass 说明：上面的代码中用到了闭包（closure），不知道你是否已经意识到了。还没有一个小问题就是，上面的代码并没有实现线程安全的单例，如果要实现线程安全的单例应该怎么做呢？ 123456789101112131415161718from functools import wrapsfrom threading import Lockdef singleton(cls): """线程安全的单例装饰器""" instances = &#123;&#125; locker = Lock() @wraps(cls) def wrapper(*args, **kwargs): if cls not in instances: with locker: if cls not in instances: instances[cls] = cls(*args, **kwargs) return instances[cls] return wrapper 面向对象相关知识 三大支柱：封装、继承、多态 例子：工资结算系统。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778"""月薪结算系统 - 部门经理每月15000 程序员每小时200 销售员1800底薪加销售额5%提成"""from abc import ABCMeta, abstractmethodclass Employee(metaclass=ABCMeta): """员工(抽象类)""" def __init__(self, name): self.name = name @abstractmethod def get_salary(self): """结算月薪(抽象方法)""" passclass Manager(Employee): """部门经理""" def get_salary(self): return 15000.0class Programmer(Employee): """程序员""" def __init__(self, name, working_hour=0): self.working_hour = working_hour super().__init__(name) def get_salary(self): return 200.0 * self.working_hourclass Salesman(Employee): """销售员""" def __init__(self, name, sales=0.0): self.sales = sales super().__init__(name) def get_salary(self): return 1800.0 + self.sales * 0.05class EmployeeFactory(): """创建员工的工厂（工厂模式 - 通过工厂实现对象使用者和对象之间的解耦合）""" @staticmethod def create(emp_type, *args, **kwargs): """创建员工""" emp_type = emp_type.upper() emp = None if emp_type == 'M': emp = Manager(*args, **kwargs) elif emp_type == 'P': emp = Programmer(*args, **kwargs) elif emp_type == 'S': emp = Salesman(*args, **kwargs) return empdef main(): """主函数""" emps = [ EmployeeFactory.create('M', '曹操'), EmployeeFactory.create('P', '荀彧', 120), EmployeeFactory.create('P', '郭嘉', 85), EmployeeFactory.create('S', '典韦', 123000), ] for emp in emps: print('%s: %.2f元' % (emp.name, emp.get_salary()))if __name__ == '__main__': main() 类与类之间的关系 is-a关系：继承 has-a关系：关联 / 聚合 / 合成 use-a关系：依赖 例子：扑克游戏。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596"""经验：符号常量总是优于字面常量，枚举类型是定义符号常量的最佳选择"""from enum import Enum, uniqueimport random@uniqueclass Suite(Enum): """花色""" SPADE, HEART, CLUB, DIAMOND = range(4) def __lt__(self, other): return self.value &lt; other.valueclass Card(): """牌""" def __init__(self, suite, face): """初始化方法""" self.suite = suite self.face = face def show(self): """显示牌面""" suites = ['♠️', '♥️', '♣️', '♦️'] faces = ['', 'A', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K'] return f'&#123;suites[self.suite.value]&#125; &#123;faces[self.face]&#125;' def __str__(self): return self.show() def __repr__(self): return self.show()class Poker(): """扑克""" def __init__(self): self.index = 0 self.cards = [Card(suite, face) for suite in Suite for face in range(1, 14)] def shuffle(self): """洗牌（随机乱序）""" random.shuffle(self.cards) self.index = 0 def deal(self): """发牌""" card = self.cards[self.index] self.index += 1 return card @property def has_more(self): return self.index &lt; len(self.cards)class Player(): """玩家""" def __init__(self, name): self.name = name self.cards = [] def get_one(self, card): """摸一张牌""" self.cards.append(card) def sort(self, comp=lambda card: (card.suite, card.face)): """整理手上的牌""" self.cards.sort(key=comp)def main(): """主函数""" poker = Poker() poker.shuffle() players = [Player('东邪'), Player('西毒'), Player('南帝'), Player('北丐')] while poker.has_more: for player in players: player.get_one(poker.deal()) for player in players: player.sort() print(player.name, end=': ') print(player.cards)if __name__ == '__main__': main() 说明：上面的代码中使用了Emoji字符来表示扑克牌的四种花色，在某些不支持Emoji字符的系统上可能无法显示。 对象的复制（深复制/深拷贝/深度克隆和浅复制/浅拷贝/影子克隆） 垃圾回收、循环引用和弱引用 Python使用了自动化内存管理，这种管理机制以引用计数为基础，同时也引入了标记-清除和分代收集两种机制为辅的策略。 123456typedef struct_object &#123; /* 引用计数 */ int ob_refcnt; /* 对象指针 */ struct_typeobject *ob_type;&#125; PyObject; 12345678/* 增加引用计数的宏定义 */#define Py_INCREF(op) ((op)-&gt;ob_refcnt++)/* 减少引用计数的宏定义 */#define Py_DECREF(op) \ //减少计数 if (--(op)-&gt;ob_refcnt != 0) \ ; \ else \ __Py_Dealloc((PyObject *)(op)) 导致引用计数+1的情况： 对象被创建，例如a = 23 对象被引用，例如b = a 对象被作为参数，传入到一个函数中，例如f(a) 对象作为一个元素，存储在容器中，例如list1 = [a, a] 导致引用计数-1的情况： 对象的别名被显式销毁，例如del a 对象的别名被赋予新的对象，例如a = 24 一个对象离开它的作用域，例如f函数执行完毕时，f函数中的局部变量（全局变量不会） 对象所在的容器被销毁，或从容器中删除对象 引用计数可能会导致循环引用问题，而循环引用会导致内存泄露，如下面的代码所示。为了解决这个问题，Python中引入了“标记-清除”和“分代收集”。在创建一个对象的时候，对象被放在第一代中，如果在第一代的垃圾检查中对象存活了下来，该对象就会被放到第二代中，同理在第二代的垃圾检查中对象存活下来，该对象就会被放到第三代中。 1234567# 循环引用会导致内存泄露 - Python除了引用技术还引入了标记清理和分代回收# 在Python 3.6以前如果重写__del__魔术方法会导致循环引用处理失效# 如果不想造成循环引用可以使用弱引用list1 = []list2 = [] list1.append(list2)list2.append(list1) 以下情况会导致垃圾回收： 调用gc.collect() gc模块的计数器达到阀值 程序退出 如果循环引用中两个对象都定义了__del__方法，gc模块不会销毁这些不可达对象，因为gc模块不知道应该先调用哪个对象的__del__方法，这个问题在Python 3.6中得到了解决。 也可以通过weakref模块构造弱引用的方式来解决循环引用的问题。 魔法属性和方法（请参考《Python魔法方法指南》） 有几个小问题请大家思考： 自定义的对象能不能使用运算符做运算？ 自定义的对象能不能放到set中？能去重吗？ 自定义的对象能不能作为dict的键？ 自定义的对象能不能使用上下文语法？ 混入（Mixin） 例子：自定义字典限制只有在指定的key不存在时才能在字典中设置键值对。 12345678910111213141516171819202122class SetOnceMappingMixin(): """自定义混入类""" __slots__ = () def __setitem__(self, key, value): if key in self: raise KeyError(str(key) + ' already set') return super().__setitem__(key, value)class SetOnceDict(SetOnceMappingMixin, dict): """自定义字典""" passmy_dict= SetOnceDict()try: my_dict['username'] = 'jackfrued' my_dict['username'] = 'hellokitty'except KeyError: passprint(my_dict) 元编程和元类 例子：用元类实现单例模式。 12345678910111213141516171819202122import threadingclass SingletonMeta(type): """自定义元类""" def __init__(cls, *args, **kwargs): cls.__instance = None cls.__lock = threading.Lock() super().__init__(*args, **kwargs) def __call__(cls, *args, **kwargs): if cls.__instance is None: with cls.__lock: if cls.__instance is None: cls.__instance = super().__call__(*args, **kwargs) return cls.__instanceclass President(metaclass=SingletonMeta): """总统(单例类)""" pass 面向对象设计原则 单一职责原则 （SRP）- 一个类只做该做的事情（类的设计要高内聚） 开闭原则 （OCP）- 软件实体应该对扩展开发对修改关闭 依赖倒转原则（DIP）- 面向抽象编程（在弱类型语言中已经被弱化） 里氏替换原则（LSP） - 任何时候可以用子类对象替换掉父类对象 接口隔离原则（ISP）- 接口要小而专不要大而全（Python中没有接口的概念） 合成聚合复用原则（CARP） - 优先使用强关联关系而不是继承关系复用代码 最少知识原则（迪米特法则，LoD）- 不要给没有必然联系的对象发消息 说明：上面加粗的字母放在一起称为面向对象的SOLID原则。 GoF设计模式 创建型模式：单例、工厂、建造者、原型 结构型模式：适配器、门面（外观）、代理 行为型模式：迭代器、观察者、状态、策略 例子：可插拔的哈希算法。 1234567891011121314151617181920212223242526272829class StreamHasher(): """哈希摘要生成器(策略模式)""" def __init__(self, alg='md5', size=4096): self.size = size alg = alg.lower() self.hasher = getattr(__import__('hashlib'), alg.lower())() def __call__(self, stream): return self.to_digest(stream) def to_digest(self, stream): """生成十六进制形式的摘要""" for buf in iter(lambda: stream.read(self.size), b''): self.hasher.update(buf) return self.hasher.hexdigest()def main(): """主函数""" hasher1 = StreamHasher() with open('Python-3.7.1.tgz', 'rb') as stream: print(hasher1.to_digest(stream)) hasher2 = StreamHasher('sha1') with open('Python-3.7.1.tgz', 'rb') as stream: print(hasher2(stream))if __name__ == '__main__': main() 迭代器和生成器 和迭代器相关的魔术方法（__iter__和__next__） 两种创建生成器的方式（生成器表达式和yield关键字） 12345678910111213141516171819202122232425def fib(num): """生成器""" a, b = 0, 1 for _ in range(num): a, b = b, a + b yield a class Fib(object): """迭代器""" def __init__(self, num): self.num = num self.a, self.b = 0, 1 self.idx = 0 def __iter__(self): return self def __next__(self): if self.idx &lt; self.num: self.a, self.b = self.b, self.a + self.b self.idx += 1 return self.a raise StopIteration() 并发编程 Python中实现并发编程的三种方案：多线程、多进程和异步I/O。并发编程的好处在于可以提升程序的执行效率以及改善用户体验；坏处在于并发的程序不容易开发和调试，同时对其他程序来说它并不友好。 多线程：Python中提供了Thread类并辅以Lock、Condition、Event、Semaphore和Barrier。Python中有GIL来防止多个线程同时执行本地字节码，这个锁对于CPython是必须的，因为CPython的内存管理并不是线程安全的，因为GIL的存在多线程并不能发挥CPU的多核特性。 123456789101112131415161718192021222324252627282930313233343536373839404142"""面试题：进程和线程的区别和联系？进程 - 操作系统分配内存的基本单位 - 一个进程可以包含一个或多个线程线程 - 操作系统分配CPU的基本单位并发编程（concurrent programming）1. 提升执行性能 - 让程序中没有因果关系的部分可以并发的执行2. 改善用户体验 - 让耗时间的操作不会造成程序的假死"""import globimport osimport threadingfrom PIL import ImagePREFIX = 'thumbnails'def generate_thumbnail(infile, size, format='PNG'): """生成指定图片文件的缩略图""" file, ext = os.path.splitext(infile) file = file[file.rfind('/') + 1:] outfile = f'&#123;PREFIX&#125;/&#123;file&#125;_&#123;size[0]&#125;_&#123;size[1]&#125;.&#123;ext&#125;' img = Image.open(infile) img.thumbnail(size, Image.ANTIALIAS) img.save(outfile, format)def main(): """主函数""" if not os.path.exists(PREFIX): os.mkdir(PREFIX) for infile in glob.glob('images/*.png'): for size in (32, 64, 128): # 创建并启动线程 threading.Thread( target=generate_thumbnail, args=(infile, (size, size)) ).start() if __name__ == '__main__': main() 多个线程竞争资源的情况 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465"""多线程程序如果没有竞争资源处理起来通常也比较简单当多个线程竞争临界资源的时候如果缺乏必要的保护措施就会导致数据错乱说明：临界资源就是被多个线程竞争的资源"""import timeimport threadingfrom concurrent.futures import ThreadPoolExecutorclass Account(object): """银行账户""" def __init__(self): self.balance = 0.0 self.lock = threading.Lock() def deposit(self, money): # 通过锁保护临界资源 with self.lock: new_balance = self.balance + money time.sleep(0.001) self.balance = new_balanceclass AddMoneyThread(threading.Thread): """自定义线程类""" def __init__(self, account, money): self.account = account self.money = money # 自定义线程的初始化方法中必须调用父类的初始化方法 super().__init__() def run(self): # 线程启动之后要执行的操作 self.account.deposit(self.money)def main(): """主函数""" account = Account() # 创建线程池 pool = ThreadPoolExecutor(max_workers=10) futures = [] for _ in range(100): # 创建线程的第1种方式 # threading.Thread( # target=account.deposit, args=(1, ) # ).start() # 创建线程的第2种方式 # AddMoneyThread(account, 1).start() # 创建线程的第3种方式 # 调用线程池中的线程来执行特定的任务 future = pool.submit(account.deposit, 1) futures.append(future) # 关闭线程池 pool.shutdown() for future in futures: future.result() print(account.balance)if __name__ == '__main__': main() 修改上面的程序，启动5个线程向账户中存钱，5个线程从账户中取钱，取钱时如果余额不足就暂停线程进行等待。为了达到上述目标，需要对存钱和取钱的线程进行调度，在余额不足时取钱的线程暂停并释放锁，而存钱的线程将钱存入后要通知取钱的线程，使其从暂停状态被唤醒。可以使用threading模块的Condition来实现线程调度，该对象也是基于锁来创建的，代码如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566"""多个线程竞争一个资源 - 保护临界资源 - 锁（Lock/RLock）多个线程竞争多个资源（线程数&gt;资源数） - 信号量（Semaphore）多个线程的调度 - 暂停线程执行/唤醒等待中的线程 - Condition"""from concurrent.futures import ThreadPoolExecutorfrom random import randintfrom time import sleepimport threadingclass Account(): """银行账户""" def __init__(self, balance=0): self.balance = balance lock = threading.Lock() self.condition = threading.Condition(lock) def withdraw(self, money): """取钱""" with self.condition: while money &gt; self.balance: self.condition.wait() new_balance = self.balance - money sleep(0.001) self.balance = new_balance def deposit(self, money): """存钱""" with self.condition: new_balance = self.balance + money sleep(0.001) self.balance = new_balance self.condition.notify_all()def add_money(account): while True: money = randint(5, 10) account.deposit(money) print(threading.current_thread().name, ':', money, '====&gt;', account.balance) sleep(0.5)def sub_money(account): while True: money = randint(10, 30) account.withdraw(money) print(threading.current_thread().name, ':', money, '&lt;====', account.balance) sleep(1)def main(): account = Account() with ThreadPoolExecutor(max_workers=10) as pool: for _ in range(5): pool.submit(add_money, account) pool.submit(sub_money, account)if __name__ == '__main__': main() 多进程：多进程可以有效的解决GIL的问题，实现多进程主要的类是Process，其他辅助的类跟threading模块中的类似，进程间共享数据可以使用管道、套接字等，在multiprocessing模块中有一个Queue类，它基于管道和锁机制提供了多个进程共享的队列。下面是官方文档上关于多进程和进程池的一个示例。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152"""多进程和进程池的使用多线程因为GIL的存在不能够发挥CPU的多核特性对于计算密集型任务应该考虑使用多进程time python3 example22.pyreal 0m11.512suser 0m39.319ssys 0m0.169s使用多进程后实际执行时间为11.512秒，而用户时间39.319秒约为实际执行时间的4倍这就证明我们的程序通过多进程使用了CPU的多核特性，而且这台计算机配置了4核的CPU"""import concurrent.futuresimport mathPRIMES = [ 1116281, 1297337, 104395303, 472882027, 533000389, 817504243, 982451653, 112272535095293, 112582705942171, 112272535095293, 115280095190773, 115797848077099, 1099726899285419] * 5def is_prime(n): """判断素数""" if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return Truedef main(): """主函数""" with concurrent.futures.ProcessPoolExecutor() as executor: for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)): print('%d is prime: %s' % (number, prime))if __name__ == '__main__': main() 说明：多线程和多进程的比较。 以下情况需要使用多线程： 程序需要维护许多共享的状态（尤其是可变状态），Python中的列表、字典、集合都是线程安全的，所以使用线程而不是进程维护共享状态的代价相对较小。 程序会花费大量时间在I/O操作上，没有太多并行计算的需求且不需占用太多的内存。 以下情况需要使用多进程： 程序执行计算密集型任务（如：字节码操作、数据处理、科学计算）。 程序的输入可以并行的分成块，并且可以将运算结果合并。 程序在内存使用方面没有任何限制且不强依赖于I/O操作（如：读写文件、套接字等）。 异步处理：从调度程序的任务队列中挑选任务，该调度程序以交叉的形式执行这些任务，我们并不能保证任务将以某种顺序去执行，因为执行顺序取决于队列中的一项任务是否愿意将CPU处理时间让位给另一项任务。异步任务通常通过多任务协作处理的方式来实现，由于执行时间和顺序的不确定，因此需要通过回调式编程或者future对象来获取任务执行的结果。Python 3通过asyncio模块和await和async关键字（在Python 3.7中正式被列为关键字）来支持异步处理。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950"""异步I/O - async / await"""import asynciodef num_generator(m, n): """指定范围的数字生成器""" yield from range(m, n + 1)async def prime_filter(m, n): """素数过滤器""" primes = [] for i in num_generator(m, n): flag = True for j in range(2, int(i ** 0.5 + 1)): if i % j == 0: flag = False break if flag: print('Prime =&gt;', i) primes.append(i) await asyncio.sleep(0.001) return tuple(primes)async def square_mapper(m, n): """平方映射器""" squares = [] for i in num_generator(m, n): print('Square =&gt;', i * i) squares.append(i * i) await asyncio.sleep(0.001) return squaresdef main(): """主函数""" loop = asyncio.get_event_loop() future = asyncio.gather(prime_filter(2, 100), square_mapper(1, 100)) future.add_done_callback(lambda x: print(x.result())) loop.run_until_complete(future) loop.close()if __name__ == '__main__': main() 说明：上面的代码使用get_event_loop函数获得系统默认的事件循环，通过gather函数可以获得一个future对象，future对象的add_done_callback可以添加执行完成时的回调函数，loop对象的run_until_complete方法可以等待通过future对象获得协程执行结果。 Python中有一个名为aiohttp的三方库，它提供了异步的HTTP客户端和服务器，这个三方库可以跟asyncio模块一起工作，并提供了对Future对象的支持。Python 3.6中引入了async和await来定义异步执行的函数以及创建异步上下文，在Python 3.7中它们正式成为了关键字。下面的代码异步的从5个URL中获取页面并通过正则表达式的命名捕获组提取了网站的标题。 123456789101112131415161718192021222324252627282930313233import asyncioimport reimport aiohttpPATTERN = re.compile(r'\&lt;title\&gt;(?P&lt;title&gt;.*)\&lt;\/title\&gt;')async def fetch_page(session, url): async with session.get(url, ssl=False) as resp: return await resp.text()async def show_title(url): async with aiohttp.ClientSession() as session: html = await fetch_page(session, url) print(PATTERN.search(html).group('title'))def main(): urls = ('https://www.python.org/', 'https://git-scm.com/', 'https://www.jd.com/', 'https://www.taobao.com/', 'https://www.douban.com/') loop = asyncio.get_event_loop() tasks = [show_title(url) for url in urls] loop.run_until_complete(asyncio.wait(tasks)) loop.close()if __name__ == '__main__': main() 说明：异步I/O与多进程的比较。 当程序不需要真正的并发性或并行性，而是更多的依赖于异步处理和回调时，asyncio就是一种很好的选择。如果程序中有大量的等待与休眠时，也应该考虑asyncio，它很适合编写没有实时数据处理需求的Web应用服务器。 Python还有很多用于处理并行任务的三方库，例如：joblib、PyMP等。实际开发中，要提升系统的可扩展性和并发性通常有垂直扩展（增加单个节点的处理能力）和水平扩展（将单个节点变成多个节点）两种做法。可以通过消息队列来实现应用程序的解耦合，消息队列相当于是多线程同步队列的扩展版本，不同机器上的应用程序相当于就是线程，而共享的分布式消息队列就是原来程序中的Queue。消息队列（面向消息的中间件）的最流行和最标准化的实现是AMQP（高级消息队列协议），AMQP源于金融行业，提供了排队、路由、可靠传输、安全等功能，最著名的实现包括：Apache的ActiveMQ、RabbitMQ等。 要实现任务的异步化，可以使用名为Celery的三方库。Celery是Python编写的分布式任务队列，它使用分布式消息进行工作，可以基于RabbitMQ或Redis来作为后端的消息代理。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[15.图像和办公文档处理]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-15-%E5%9B%BE%E5%83%8F%E5%92%8C%E5%8A%9E%E5%85%AC%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[图像和办公文档处理用程序来处理图像和办公文档经常出现在实际开发中，Python的标准库中虽然没有直接支持这些操作的模块，但我们可以通过Python生态圈中的第三方模块来完成这些操作。 操作图像计算机图像相关知识 颜色。如果你有使用颜料画画的经历，那么一定知道混合红、黄、蓝三种颜料可以得到其他的颜色，事实上这三种颜色就是被我们称为美术三原色的东西，它们是不能再分解的基本颜色。在计算机中，我们可以将红、绿、蓝三种色光以不同的比例叠加来组合成其他的颜色，因此这三种颜色就是色光三原色，所以我们通常会将一个颜色表示为一个RGB值或RGBA值（其中的A表示Alpha通道，它决定了透过这个图像的像素，也就是透明度）。 名称 RGBA值 名称 RGBA值 White (255, 255, 255, 255) Red (255, 0, 0, 255) Green (0, 255, 0, 255) Blue (0, 0, 255, 255) Gray (128, 128, 128, 255) Yellow (255, 255, 0, 255) Black (0, 0, 0, 255) Purple (128, 0, 128, 255) 像素。对于一个由数字序列表示的图像来说，最小的单位就是图像上单一颜色的小方格，这些小方块都有一个明确的位置和被分配的色彩数值，而这些一小方格的颜色和位置决定了该图像最终呈现出来的样子，它们是不可分割的单位，我们通常称之为像素（pixel）。每一个图像都包含了一定量的像素，这些像素决定图像在屏幕上所呈现的大小。 用Pillow操作图像Pillow是由从著名的Python图像处理库PIL发展出来的一个分支，通过Pillow可以实现图像压缩和图像处理等各种操作。可以使用下面的命令来安装Pillow。 1pip install pillow Pillow中最为重要的是Image类，读取和处理图像都要通过这个类来完成。 123456&gt;&gt;&gt; from PIL import Image&gt;&gt;&gt;&gt;&gt;&gt; image = Image.open('/images/Python100/guido.jpg')&gt;&gt;&gt; image.format, image.size, image.mode('JPEG', (500, 750), 'RGB')&gt;&gt;&gt; image.show() 剪裁图像 123&gt;&gt;&gt; image = Image.open('/images/Python100/guido.jpg')&gt;&gt;&gt; rect = 80, 20, 310, 360&gt;&gt;&gt; image.crop(rect).show() 生成缩略图 1234&gt;&gt;&gt; image = Image.open('/images/Python100/guido.jpg')&gt;&gt;&gt; size = 128, 128&gt;&gt;&gt; image.thumbnail(size)&gt;&gt;&gt; image.show() 缩放和黏贴图像 123456&gt;&gt;&gt; image1 = Image.open('/images/Python100/luohao.png')&gt;&gt;&gt; image2 = Image.open('/images/Python100/guido.jpg')&gt;&gt;&gt; rect = 80, 20, 310, 360&gt;&gt;&gt; guido_head = image2.crop(rect)&gt;&gt;&gt; width, height = guido_head.size&gt;&gt;&gt; image1.paste(guido_head.resize((int(width / 1.5), int(height / 1.5))), (172, 40)) 旋转和翻转 123&gt;&gt;&gt; image = Image.open('/images/Python100/guido.png')&gt;&gt;&gt; image.rotate(180).show()&gt;&gt;&gt; image.transpose(Image.FLIP_LEFT_RIGHT).show() 操作像素 123456&gt;&gt;&gt; image = Image.open('/images/Python100/guido.jpg')&gt;&gt;&gt; for x in range(80, 310):... for y in range(20, 360):... image.putpixel((x, y), (128, 128, 128))... &gt;&gt;&gt; image.show() 滤镜效果 1234&gt;&gt;&gt; from PIL import Image, ImageFilter&gt;&gt;&gt;&gt;&gt;&gt; image = Image.open('/images/Python100/guido.jpg')&gt;&gt;&gt; image.filter(ImageFilter.CONTOUR).show() 处理Excel电子表格Python的openpyxl模块让我们可以在Python程序中读取和修改Excel电子表格，当然实际工作中，我们可能会用LibreOffice Calc和OpenOffice Calc来处理Excel的电子表格文件，这就意味着openpyxl模块也能处理来自这些软件生成的电子表格。关于openpyxl的使用手册和使用文档可以查看它的官方文档。 处理Word文档利用python-docx模块，Pytho 可以创建和修改Word文档，当然这里的Word文档不仅仅是指通过微软的Office软件创建的扩展名为docx的文档，LibreOffice Writer和OpenOffice Writer都是免费的字处理软件。 处理PDF文档PDF是Portable Document Format的缩写，使用.pdf作为文件扩展名。接下来我们就研究一下如何通过Python实现从PDF读取文本内容和从已有的文档生成新的PDF文件。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[14.网络编程入门]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-14-%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8%E5%92%8C%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[网络编程入门计算机网络基础计算机网络是独立自主的计算机互联而成的系统的总称，组建计算机网络最主要的目的是实现多台计算机之间的通信和资源共享。今天计算机网络中的设备和计算机网络的用户已经多得不可计数，而计算机网络也可以称得上是一个“复杂巨系统”，对于这样的系统，我们不可能用一两篇文章把它讲清楚，有兴趣的读者可以自行阅读Andrew S.Tanenbaum老师的经典之作《计算机网络》或Kurose和Ross老师合著的《计算机网络:自顶向下方法》来了解计算机网络的相关知识。 计算机网络发展史 1960s - 美国国防部ARPANET项目问世，奠定了分组交换网络的基础。 1980s - 国际标准化组织（ISO）发布OSI/RM，奠定了网络技术标准化的基础。 1990s - 英国人蒂姆·伯纳斯-李发明了图形化的浏览器，浏览器的简单易用性使得计算机网络迅速被普及。 在没有浏览器的年代，上网是这样的。 有了浏览器以后，上网是这样的。 TCP/IP模型实现网络通信的基础是网络通信协议，这些协议通常是由互联网工程任务组 （IETF）制定的。所谓“协议”就是通信计算机双方必须共同遵从的一组约定，例如怎样建立连接、怎样互相识别等，网络协议的三要素是：语法、语义和时序。构成我们今天使用的Internet的基础的是TCP/IP协议族，所谓协议族就是一系列的协议及其构成的通信模型，我们通常也把这套东西称为TCP/IP模型。与国际标准化组织发布的OSI/RM这个七层模型不同，TCP/IP是一个四层模型，也就是说，该模型将我们使用的网络从逻辑上分解为四个层次，自底向上依次是：网络接口层、网络层、传输层和应用层，如下图所示。 IP通常被翻译为网际协议，它服务于网络层，主要实现了寻址和路由的功能。接入网络的每一台主机都需要有自己的IP地址，IP地址就是主机在计算机网络上的身份标识。当然由于IPv4地址的匮乏，我们平常在家里、办公室以及其他可以接入网络的公共区域上网时获得的IP地址并不是全球唯一的IP地址，而是一个局域网（LAN）中的内部IP地址，通过网络地址转换（NAT）服务我们也可以实现对网络的访问。计算机网络上有大量的被我们称为“路由器”的网络中继设备，它们会存储转发我们发送到网络上的数据分组，让从源头发出的数据最终能够找到传送到目的地通路，这项功能就是所谓的路由。 TCP全称传输控制协议，它是基于IP提供的寻址和路由服务而建立起来的负责实现端到端可靠传输的协议，之所以将TCP称为可靠的传输协议是因为TCP向调用者承诺了三件事情： 数据不传丢不传错（利用握手、校验和重传机制可以实现）。 流量控制（通过滑动窗口匹配数据发送者和接收者之间的传输速度）。 拥塞控制（通过RTT时间以及对滑动窗口的控制缓解网络拥堵）。 网络应用模式 C/S模式和B/S模式。这里的C指的是Client（客户端），通常是一个需要安装到某个宿主操作系统上的应用程序；而B指的是Browser（浏览器），它几乎是所有图形化操作系统都默认安装了的一个应用软件；通过C或B都可以实现对S（服务器）的访问。关于二者的比较和讨论在网络上有一大堆的文章，在此我们就不再浪费笔墨了。 去中心化的网络应用模式。不管是B/S还是C/S都需要服务器的存在，服务器就是整个应用模式的中心，而去中心化的网络应用通常没有固定的服务器或者固定的客户端，所有应用的使用者既可以作为资源的提供者也可以作为资源的访问者。 基于HTTP协议的网络资源访问HTTP（超文本传输协议）HTTP是超文本传输协议（Hyper-Text Transfer Proctol）的简称，维基百科上对HTTP的解释是：超文本传输协议是一种用于分布式、协作式和超媒体信息系统的应用层协议，它是万维网数据通信的基础，设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法，通过HTTP或者HTTPS（超文本传输安全协议）请求的资源由URI（统一资源标识符）来标识。关于HTTP的更多内容，我们推荐阅读阮一峰老师的《HTTP 协议入门》，简单的说，通过HTTP我们可以获取网络上的（基于字符的）资源，开发中经常会用到的网络API（有的地方也称之为网络数据接口）就是基于HTTP来实现数据传输的。 JSON格式JSON（JavaScript Object Notation）是一种轻量级的数据交换语言，该语言以易于让人阅读的文字（纯文本）为基础，用来传输由属性值或者序列性的值组成的数据对象。尽管JSON是最初只是Javascript中一种创建对象的字面量语法，但它在当下更是一种独立于语言的数据格式，很多编程语言都支持JSON格式数据的生成和解析，Python内置的json模块也提供了这方面的功能。由于JSON是纯文本，它和XML一样都适用于异构系统之间的数据交换，而相较于XML，JSON显得更加的轻便和优雅。下面是表达同样信息的XML和JSON，而JSON的优势是相当直观的。 XML的例子： 123456&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;message&gt; &lt;from&gt;Alice&lt;/from&gt; &lt;to&gt;Bob&lt;/to&gt; &lt;content&gt;Will you marry me?&lt;/content&gt;&lt;/message&gt; JSON的例子： 12345&#123; "from": "Alice", "to": "Bob", "content": "Will you marry me?"&#125; requests库requests是一个基于HTTP协议来使用网络的第三库，其官方网站有这样的一句介绍它的话：“Requests是唯一的一个非转基因的Python HTTP库，人类可以安全享用。”简单的说，使用requests库可以非常方便的使用HTTP，避免安全缺陷、冗余代码以及“重复发明轮子”（行业黑话，通常用在软件工程领域表示重新创造一个已有的或是早已被优化過的基本方法）。前面的文章中我们已经使用过这个库，下面我们还是通过requests来实现一个访问网络数据接口并从中获取美女图片下载链接然后下载美女图片到本地的例子程序，程序中使用了天行数据提供的网络API。 我们可以先通过pip安装requests及其依赖库。 1pip install requests 如果使用PyCharm作为开发工具，可以直接在代码中书写import requests，然后通过代码修复功能来自动下载安装requests。 12345678910111213141516171819202122232425262728293031323334353637from time import timefrom threading import Threadimport requests# 继承Thread类创建自定义的线程类class DownloadHanlder(Thread): def __init__(self, url): super().__init__() self.url = url def run(self): filename = self.url[self.url.rfind('/') + 1:] resp = requests.get(self.url) with open('/Users/Hao/' + filename, 'wb') as f: f.write(resp.content)def main(): # 通过requests模块的get函数获取网络资源 # 下面的代码中使用了天行数据接口提供的网络API # 要使用该数据接口需要在天行数据的网站上注册 # 然后用自己的Key替换掉下面代码的中APIKey即可 resp = requests.get( 'http://api.tianapi.com/meinv/?key=APIKey&amp;num=10') # 将服务器返回的JSON格式的数据解析为字典 data_model = resp.json() for mm_dict in data_model['newslist']: url = mm_dict['picUrl'] # 通过多线程的方式实现图片下载 DownloadHanlder(url).start()if __name__ == '__main__': main() 基于传输层协议的套接字编程套接字这个词对很多不了解网络编程的人来说显得非常晦涩和陌生，其实说得通俗点，套接字就是一套用C语言写成的应用程序开发库，主要用于实现进程间通信和网络编程，在网络应用开发中被广泛使用。在Python中也可以基于套接字来使用传输层提供的传输服务，并基于此开发自己的网络应用。实际开发中使用的套接字可以分为三类：流套接字（TCP套接字）、数据报套接字和原始套接字。 TCP套接字所谓TCP套接字就是使用TCP协议提供的传输服务来实现网络通信的编程接口。在Python中可以通过创建socket对象并指定type属性为SOCK_STREAM来使用TCP套接字。由于一台主机可能拥有多个IP地址，而且很有可能会配置多个不同的服务，所以作为服务器端的程序，需要在创建套接字对象后将其绑定到指定的IP地址和端口上。这里的端口并不是物理设备而是对IP地址的扩展，用于区分不同的服务，例如我们通常将HTTP服务跟80端口绑定，而MySQL数据库服务默认绑定在3306端口，这样当服务器收到用户请求时就可以根据端口号来确定到底用户请求的是HTTP服务器还是数据库服务器提供的服务。端口的取值范围是0~65535，而1024以下的端口我们通常称之为“著名端口”（留给像FTP、HTTP、SMTP等“著名服务”使用的端口，有的地方也称之为“周知端口”），自定义的服务通常不使用这些端口，除非自定义的是HTTP或FTP这样的著名服务。 下面的代码实现了一个提供时间日期的服务器。 12345678910111213141516171819202122232425262728293031323334from socket import socket, SOCK_STREAM, AF_INETfrom datetime import datetimedef main(): # 1.创建套接字对象并指定使用哪种传输服务 # family=AF_INET - IPv4地址 # family=AF_INET6 - IPv6地址 # type=SOCK_STREAM - TCP套接字 # type=SOCK_DGRAM - UDP套接字 # type=SOCK_RAW - 原始套接字 server = socket(family=AF_INET, type=SOCK_STREAM) # 2.绑定IP地址和端口(端口用于区分不同的服务) # 同一时间在同一个端口上只能绑定一个服务否则报错 server.bind(('192.168.1.2', 6789)) # 3.开启监听 - 监听客户端连接到服务器 # 参数512可以理解为连接队列的大小 server.listen(512) print('服务器启动开始监听...') while True: # 4.通过循环接收客户端的连接并作出相应的处理(提供服务) # accept方法是一个阻塞方法如果没有客户端连接到服务器代码不会向下执行 # accept方法返回一个元组其中的第一个元素是客户端对象 # 第二个元素是连接到服务器的客户端的地址(由IP和端口两部分构成) client, addr = server.accept() print(str(addr) + '连接到了服务器.') # 5.发送数据 client.send(str(datetime.now()).encode('utf-8')) # 6.断开连接 client.close()if __name__ == '__main__': main() 运行服务器程序后我们可以通过Windows系统的telnet来访问该服务器，结果如下图所示。 1telnet 192.168.1.2 6789 当然我们也可以通过Python的程序来实现TCP客户端的功能，相较于实现服务器程序，实现客户端程序就简单多了，代码如下所示。 123456789101112131415from socket import socketdef main(): # 1.创建套接字对象默认使用IPv4和TCP协议 client = socket() # 2.连接到服务器(需要指定IP地址和端口) client.connect(('192.168.1.2', 6789)) # 3.从服务器接收数据 print(client.recv(1024).decode('utf-8')) client.close()if __name__ == '__main__': main() 需要注意的是，上面的服务器并没有使用多线程或者异步I/O的处理方式，这也就意味着当服务器与一个客户端处于通信状态时，其他的客户端只能排队等待。很显然，这样的服务器并不能满足我们的需求，我们需要的服务器是能够同时接纳和处理多个用户请求的。下面我们来设计一个使用多线程技术处理多个用户请求的服务器，该服务器会向连接到服务器的客户端发送一张图片。 服务器端代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445from socket import socket, SOCK_STREAM, AF_INETfrom base64 import b64encodefrom json import dumpsfrom threading import Threaddef main(): # 自定义线程类 class FileTransferHandler(Thread): def __init__(self, cclient): super().__init__() self.cclient = cclient def run(self): my_dict = &#123;&#125; my_dict['filename'] = 'guido.jpg' # JSON是纯文本不能携带二进制数据 # 所以图片的二进制数据要处理成base64编码 my_dict['filedata'] = data # 通过dumps函数将字典处理成JSON字符串 json_str = dumps(my_dict) # 发送JSON字符串 self.cclient.send(json_str.encode('utf-8')) self.cclient.close() # 1.创建套接字对象并指定使用哪种传输服务 server = socket() # 2.绑定IP地址和端口(区分不同的服务) server.bind(('192.168.1.2', 5566)) # 3.开启监听 - 监听客户端连接到服务器 server.listen(512) print('服务器启动开始监听...') with open('guido.jpg', 'rb') as f: # 将二进制数据处理成base64再解码成字符串 data = b64encode(f.read()).decode('utf-8') while True: client, addr = server.accept() # 启动一个线程来处理客户端的请求 FileTransferHandler(client).start()if __name__ == '__main__': main() 客户端代码： 1234567891011121314151617181920212223242526272829from socket import socketfrom json import loadsfrom base64 import b64decodedef main(): client = socket() client.connect(('192.168.1.2', 5566)) # 定义一个保存二进制数据的对象 in_data = bytes() # 由于不知道服务器发送的数据有多大每次接收1024字节 data = client.recv(1024) while data: # 将收到的数据拼接起来 in_data += data data = client.recv(1024) # 将收到的二进制数据解码成JSON字符串并转换成字典 # loads函数的作用就是将JSON字符串转成字典对象 my_dict = loads(in_data.decode('utf-8')) filename = my_dict['filename'] filedata = my_dict['filedata'].encode('utf-8') with open('/Users/Hao/' + filename, 'wb') as f: # 将base64格式的数据解码成二进制数据并写入文件 f.write(b64decode(filedata)) print('图片已保存.')if __name__ == '__main__': main() 在这个案例中，我们使用了JSON作为数据传输的格式（通过JSON格式对传输的数据进行了序列化和反序列化的操作），但是JSON并不能携带二进制数据，因此对图片的二进制数据进行了Base64编码的处理。Base64是一种用64个字符表示所有二进制数据的编码方式，通过将二进制数据每6位一组的方式重新组织，刚好可以使用0~9的数字、大小写字母以及“+”和“/”总共64个字符表示从000000到111111的64种状态。维基百科上有关于Base64编码的详细讲解，不熟悉Base64的读者可以自行阅读。 说明： 上面的代码主要为了讲解网络编程的相关内容因此并没有对异常状况进行处理，请读者自行添加异常处理代码来增强程序的健壮性。 UDP套接字传输层除了有可靠的传输协议TCP之外，还有一种非常轻便的传输协议叫做用户数据报协议，简称UDP。TCP和UDP都是提供端到端传输服务的协议，二者的差别就如同打电话和发短信的区别，后者不对传输的可靠性和可达性做出任何承诺从而避免了TCP中握手和重传的开销，所以在强调性能和而不是数据完整性的场景中（例如传输网络音视频数据），UDP可能是更好的选择。可能大家会注意到一个现象，就是在观看网络视频时，有时会出现卡顿，有时会出现花屏，这无非就是部分数据传丢或传错造成的。在Python中也可以使用UDP套接字来创建网络应用，对此我们不进行赘述，有兴趣的读者可以自行研究。 网络应用开发发送电子邮件在即时通信软件如此发达的今天，电子邮件仍然是互联网上使用最为广泛的应用之一，公司向应聘者发出录用通知、网站向用户发送一个激活账号的链接、银行向客户推广它们的理财产品等几乎都是通过电子邮件来完成的，而这些任务应该都是由程序自动完成的。 就像我们可以用HTTP（超文本传输协议）来访问一个网站一样，发送邮件要使用SMTP（简单邮件传输协议），SMTP也是一个建立在TCP（传输控制协议）提供的可靠数据传输服务的基础上的应用级协议，它规定了邮件的发送者如何跟发送邮件的服务器进行通信的细节，而Python中的smtplib模块将这些操作简化成了几个简单的函数。 下面的代码演示了如何在Python发送邮件。 12345678910111213141516171819202122from smtplib import SMTPfrom email.header import Headerfrom email.mime.text import MIMETextdef main(): # 请自行修改下面的邮件发送者和接收者 sender = 'abcdefg@126.com' receivers = ['uvwxyz@qq.com', 'uvwxyz@126.com'] message = MIMEText('用Python发送邮件的示例代码.', 'plain', 'utf-8') message['From'] = Header('王大锤', 'utf-8') message['To'] = Header('骆昊', 'utf-8') message['Subject'] = Header('示例代码实验邮件', 'utf-8') smtper = SMTP('smtp.126.com') # 请自行修改下面的登录口令 smtper.login(sender, 'secretpass') smtper.sendmail(sender, receivers, message.as_string()) print('邮件发送完成!')if __name__ == '__main__': main() 如果要发送带有附件的邮件，那么可以按照下面的方式进行操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from smtplib import SMTPfrom email.header import Headerfrom email.mime.text import MIMETextfrom email.mime.image import MIMEImagefrom email.mime.multipart import MIMEMultipartimport urllibdef main(): # 创建一个带附件的邮件消息对象 message = MIMEMultipart() # 创建文本内容 text_content = MIMEText('附件中有本月数据请查收', 'plain', 'utf-8') message['Subject'] = Header('本月数据', 'utf-8') # 将文本内容添加到邮件消息对象中 message.attach(text_content) # 读取文件并将文件作为附件添加到邮件消息对象中 with open('/Users/Hao/Desktop/hello.txt', 'rb') as f: txt = MIMEText(f.read(), 'base64', 'utf-8') txt['Content-Type'] = 'text/plain' txt['Content-Disposition'] = 'attachment; filename=hello.txt' message.attach(txt) # 读取文件并将文件作为附件添加到邮件消息对象中 with open('/Users/Hao/Desktop/汇总数据.xlsx', 'rb') as f: xls = MIMEText(f.read(), 'base64', 'utf-8') xls['Content-Type'] = 'application/vnd.ms-excel' xls['Content-Disposition'] = 'attachment; filename=month-data.xlsx' message.attach(xls) # 创建SMTP对象 smtper = SMTP('smtp.126.com') # 开启安全连接 # smtper.starttls() sender = 'abcdefg@126.com' receivers = ['uvwxyz@qq.com'] # 登录到SMTP服务器 # 请注意此处不是使用密码而是邮件客户端授权码进行登录 # 对此有疑问的读者可以联系自己使用的邮件服务器客服 smtper.login(sender, 'secretpass') # 发送邮件 smtper.sendmail(sender, receivers, message.as_string()) # 与邮件服务器断开连接 smtper.quit() print('发送完成!')if __name__ == '__main__': main() 发送短信发送短信也是项目中常见的功能，网站的注册码、验证码、营销信息基本上都是通过短信来发送给用户的。在下面的代码中我们使用了互亿无线短信平台（该平台为注册用户提供了50条免费短信以及常用开发语言发送短信的demo，可以登录该网站并在用户自服务页面中对短信进行配置）提供的API接口实现了发送短信的服务，当然国内的短信平台很多，读者可以根据自己的需要进行选择（通常会考虑费用预算、短信达到率、使用的难易程度等指标），如果需要在商业项目中使用短信服务建议购买短信平台提供的套餐服务。 1234567891011121314151617181920212223import urllib.parseimport http.clientimport jsondef main(): host = "106.ihuyi.com" sms_send_uri = "/webservice/sms.php?method=Submit" # 下面的参数需要填入自己注册的账号和对应的密码 params = urllib.parse.urlencode(&#123;'account': '你自己的账号', 'password' : '你自己的密码', 'content': '您的验证码是：147258。请不要把验证码泄露给其他人。', 'mobile': '接收者的手机号', 'format':'json' &#125;) print(params) headers = &#123;'Content-type': 'application/x-www-form-urlencoded', 'Accept': 'text/plain'&#125; conn = http.client.HTTPConnection(host, port=80, timeout=30) conn.request('POST', sms_send_uri, params, headers) response = conn.getresponse() response_str = response.read() jsonstr = response_str.decode('utf-8') print(json.loads(jsonstr)) conn.close()if __name__ == '__main__': main()]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[13.进程和线程]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-13-%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[进程和线程今天我们使用的计算机早已进入多CPU或多核时代，而我们使用的操作系统都是支持“多任务”的操作系统，这使得我们可以同时运行多个程序，也可以将一个程序分解为若干个相对独立的子任务，让多个子任务并发的执行，从而缩短程序的执行时间，同时也让用户获得更好的体验。因此在当下不管是用什么编程语言进行开发，实现让程序同时执行多个任务也就是常说的“并发编程”，应该是程序员必备技能之一。为此，我们需要先讨论两个概念，一个叫进程，一个叫线程。 概念进程就是操作系统中执行的一个程序，操作系统以进程为单位分配存储空间，每个进程都有自己的地址空间、数据栈以及其他用于跟踪进程执行的辅助数据，操作系统管理所有进程的执行，为它们合理的分配资源。进程可以通过fork或spawn的方式来创建新的进程来执行其他的任务，不过新的进程也有自己独立的内存空间，因此必须通过进程间通信机制（IPC，Inter-Process Communication）来实现数据共享，具体的方式包括管道、信号、套接字、共享内存区等。 一个进程还可以拥有多个并发的执行线索，简单的说就是拥有多个可以获得CPU调度的执行单元，这就是所谓的线程。由于线程在同一个进程下，它们可以共享相同的上下文，因此相对于进程而言，线程间的信息共享和通信更加容易。当然在单核CPU系统中，真正的并发是不可能的，因为在某个时刻能够获得CPU的只有唯一的一个线程，多个线程共享了CPU的执行时间。使用多线程实现并发编程为程序带来的好处是不言而喻的，最主要的体现在提升程序的性能和改善用户体验，今天我们使用的软件几乎都用到了多线程技术，这一点可以利用系统自带的进程监控工具（如macOS中的“活动监视器”、Windows中的“任务管理器”）来证实，如下图所示。 当然多线程也并不是没有坏处，站在其他进程的角度，多线程的程序对其他程序并不友好，因为它占用了更多的CPU执行时间，导致其他程序无法获得足够的CPU执行时间；另一方面，站在开发者的角度，编写和调试多线程的程序都对开发者有较高的要求，对于初学者来说更加困难。 Python既支持多进程又支持多线程，因此使用Python实现并发编程主要有3种方式：多进程、多线程、多进程+多线程。 Python中的多进程Unix和Linux操作系统上提供了fork()系统调用来创建进程，调用fork()函数的是父进程，创建出的是子进程，子进程是父进程的一个拷贝，但是子进程拥有自己的PID。fork()函数非常特殊它会返回两次，父进程中可以通过fork()函数的返回值得到子进程的PID，而子进程中的返回值永远都是0。Python的os模块提供了fork()函数。由于Windows系统没有fork()调用，因此要实现跨平台的多进程编程，可以使用multiprocessing模块的Process类来创建子进程，而且该模块还提供了更高级的封装，例如批量启动进程的进程池（Pool）、用于进程间通信的队列（Queue）和管道（Pipe）等。 下面用一个下载文件的例子来说明使用多进程和不使用多进程到底有什么差别，先看看下面的代码。 123456789101112131415161718192021from random import randintfrom time import time, sleepdef download_task(filename): print('开始下载%s...' % filename) time_to_download = randint(5, 10) sleep(time_to_download) print('%s下载完成! 耗费了%d秒' % (filename, time_to_download))def main(): start = time() download_task('Python从入门到住院.pdf') download_task('Peking Hot.avi') end = time() print('总共耗费了%.2f秒.' % (end - start))if __name__ == '__main__': main() 下面是运行程序得到的一次运行结果。 12345开始下载Python从入门到住院.pdf...Python从入门到住院.pdf下载完成! 耗费了6秒开始下载Peking Hot.avi...Peking Hot.avi下载完成! 耗费了7秒总共耗费了13.01秒. 从上面的例子可以看出，如果程序中的代码只能按顺序一点点的往下执行，那么即使执行两个毫不相关的下载任务，也需要先等待一个文件下载完成后才能开始下一个下载任务，很显然这并不合理也没有效率。接下来我们使用多进程的方式将两个下载任务放到不同的进程中，代码如下所示。 12345678910111213141516171819202122232425262728from multiprocessing import Processfrom os import getpidfrom random import randintfrom time import time, sleepdef download_task(filename): print('启动下载进程，进程号[%d].' % getpid()) print('开始下载%s...' % filename) time_to_download = randint(5, 10) sleep(time_to_download) print('%s下载完成! 耗费了%d秒' % (filename, time_to_download))def main(): start = time() p1 = Process(target=download_task, args=('Python从入门到住院.pdf', )) p1.start() p2 = Process(target=download_task, args=('Peking Hot.avi', )) p2.start() p1.join() p2.join() end = time() print('总共耗费了%.2f秒.' % (end - start))if __name__ == '__main__': main() 在上面的代码中，我们通过Process类创建了进程对象，通过target参数我们传入一个函数来表示进程启动后要执行的代码，后面的args是一个元组，它代表了传递给函数的参数。Process对象的start方法用来启动进程，而join方法表示等待进程执行结束。运行上面的代码可以明显发现两个下载任务“同时”启动了，而且程序的执行时间将大大缩短，不再是两个任务的时间总和。下面是程序的一次执行结果。 1234567启动下载进程，进程号[1530].开始下载Python从入门到住院.pdf...启动下载进程，进程号[1531].开始下载Peking Hot.avi...Peking Hot.avi下载完成! 耗费了7秒Python从入门到住院.pdf下载完成! 耗费了10秒总共耗费了10.01秒. 我们也可以使用subprocess模块中的类和函数来创建和启动子进程，然后通过管道来和子进程通信，这些内容我们不在此进行讲解，有兴趣的读者可以自己了解这些知识。接下来我们将重点放在如何实现两个进程间的通信。我们启动两个进程，一个输出Ping，一个输出Pong，两个进程输出的Ping和Pong加起来一共10个。听起来很简单吧，但是如果这样写可是错的哦。 123456789101112131415161718192021from multiprocessing import Processfrom time import sleepcounter = 0def sub_task(string): global counter while counter &lt; 10: print(string, end='', flush=True) counter += 1 sleep(0.01) def main(): Process(target=sub_task, args=('Ping', )).start() Process(target=sub_task, args=('Pong', )).start()if __name__ == '__main__': main() 看起来没毛病，但是最后的结果是Ping和Pong各输出了10个，Why？当我们在程序中创建进程的时候，子进程复制了父进程及其所有的数据结构，每个子进程有自己独立的内存空间，这也就意味着两个子进程中各有一个counter变量，所以结果也就可想而知了。要解决这个问题比较简单的办法是使用multiprocessing模块中的Queue类，它是可以被多个进程共享的队列，底层是通过管道和信号量（semaphore）机制来实现的，有兴趣的读者可以自己尝试一下。 Python中的多线程在Python早期的版本中就引入了thread模块（现在名为_thread）来实现多线程编程，然而该模块过于底层，而且很多功能都没有提供，因此目前的多线程开发我们推荐使用threading模块，该模块对多线程编程提供了更好的面向对象的封装。我们把刚才下载文件的例子用多线程的方式来实现一遍。 1234567891011121314151617181920212223242526from random import randintfrom threading import Threadfrom time import time, sleepdef download(filename): print('开始下载%s...' % filename) time_to_download = randint(5, 10) sleep(time_to_download) print('%s下载完成! 耗费了%d秒' % (filename, time_to_download))def main(): start = time() t1 = Thread(target=download, args=('Python从入门到住院.pdf',)) t1.start() t2 = Thread(target=download, args=('Peking Hot.avi',)) t2.start() t1.join() t2.join() end = time() print('总共耗费了%.3f秒' % (end - start))if __name__ == '__main__': main() 我们可以直接使用threading模块的Thread类来创建线程，但是我们之前讲过一个非常重要的概念叫“继承”，我们可以从已有的类创建新类，因此也可以通过继承Thread类的方式来创建自定义的线程类，然后再创建线程对象并启动线程。代码如下所示。 1234567891011121314151617181920212223242526272829303132from random import randintfrom threading import Threadfrom time import time, sleepclass DownloadTask(Thread): def __init__(self, filename): super().__init__() self._filename = filename def run(self): print('开始下载%s...' % self._filename) time_to_download = randint(5, 10) sleep(time_to_download) print('%s下载完成! 耗费了%d秒' % (self._filename, time_to_download))def main(): start = time() t1 = DownloadTask('Python从入门到住院.pdf') t1.start() t2 = DownloadTask('Peking Hot.avi') t2.start() t1.join() t2.join() end = time() print('总共耗费了%.2f秒.' % (end - start))if __name__ == '__main__': main() 因为多个线程可以共享进程的内存空间，因此要实现多个线程间的通信相对简单，大家能想到的最直接的办法就是设置一个全局变量，多个线程共享这个全局变量即可。但是当多个线程共享同一个变量（我们通常称之为“资源”）的时候，很有可能产生不可控的结果从而导致程序失效甚至崩溃。如果一个资源被多个线程竞争使用，那么我们通常称之为“临界资源”，对“临界资源”的访问需要加上保护，否则资源会处于“混乱”的状态。下面的例子演示了100个线程向同一个银行账户转账（转入1元钱）的场景，在这个例子中，银行账户就是一个临界资源，在没有保护的情况下我们很有可能会得到错误的结果。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from time import sleepfrom threading import Threadclass Account(object): def __init__(self): self._balance = 0 def deposit(self, money): # 计算存款后的余额 new_balance = self._balance + money # 模拟受理存款业务需要0.01秒的时间 sleep(0.01) # 修改账户余额 self._balance = new_balance @property def balance(self): return self._balanceclass AddMoneyThread(Thread): def __init__(self, account, money): super().__init__() self._account = account self._money = money def run(self): self._account.deposit(self._money)def main(): account = Account() threads = [] # 创建100个存款的线程向同一个账户中存钱 for _ in range(100): t = AddMoneyThread(account, 1) threads.append(t) t.start() # 等所有存款的线程都执行完毕 for t in threads: t.join() print('账户余额为: ￥%d元' % account.balance)if __name__ == '__main__': main() 运行上面的程序，结果让人大跌眼镜，100个线程分别向账户中转入1元钱，结果居然远远小于100元。之所以出现这种情况是因为我们没有对银行账户这个“临界资源”加以保护，多个线程同时向账户中存钱时，会一起执行到new_balance = self._balance + money这行代码，多个线程得到的账户余额都是初始状态下的0，所以都是0上面做了+1的操作，因此得到了错误的结果。在这种情况下，“锁”就可以派上用场了。我们可以通过“锁”来保护“临界资源”，只有获得“锁”的线程才能访问“临界资源”，而其他没有得到“锁”的线程只能被阻塞起来，直到获得“锁”的线程释放了“锁”，其他线程才有机会获得“锁”，进而访问被保护的“临界资源”。下面的代码演示了如何使用“锁”来保护对银行账户的操作，从而获得正确的结果。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from time import sleepfrom threading import Thread, Lockclass Account(object): def __init__(self): self._balance = 0 self._lock = Lock() def deposit(self, money): # 先获取锁才能执行后续的代码 self._lock.acquire() try: new_balance = self._balance + money sleep(0.01) self._balance = new_balance finally: # 在finally中执行释放锁的操作保证正常异常锁都能释放 self._lock.release() @property def balance(self): return self._balanceclass AddMoneyThread(Thread): def __init__(self, account, money): super().__init__() self._account = account self._money = money def run(self): self._account.deposit(self._money)def main(): account = Account() threads = [] for _ in range(100): t = AddMoneyThread(account, 1) threads.append(t) t.start() for t in threads: t.join() print('账户余额为: ￥%d元' % account.balance)if __name__ == '__main__': main() 比较遗憾的一件事情是Python的多线程并不能发挥CPU的多核特性，这一点只要启动几个执行死循环的线程就可以得到证实了。之所以如此，是因为Python的解释器有一个“全局解释器锁”（GIL）的东西，任何线程执行前必须先获得GIL锁，然后每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行，这是一个历史遗留问题，但是即便如此，就如我们之前举的例子，使用多线程在提升执行效率和改善用户体验方面仍然是有积极意义的。 多进程还是多线程无论是多进程还是多线程，只要数量一多，效率肯定上不去，为什么呢？我们打个比方，假设你不幸正在准备中考，每天晚上需要做语文、数学、英语、物理、化学这5科的作业，每项作业耗时1小时。如果你先花1小时做语文作业，做完了，再花1小时做数学作业，这样，依次全部做完，一共花5小时，这种方式称为单任务模型。如果你打算切换到多任务模型，可以先做1分钟语文，再切换到数学作业，做1分钟，再切换到英语，以此类推，只要切换速度足够快，这种方式就和单核CPU执行多任务是一样的了，以旁观者的角度来看，你就正在同时写5科作业。 但是，切换作业是有代价的，比如从语文切到数学，要先收拾桌子上的语文书本、钢笔（这叫保存现场），然后，打开数学课本、找出圆规直尺（这叫准备新环境），才能开始做数学作业。操作系统在切换进程或者线程时也是一样的，它需要先保存当前执行的现场环境（CPU寄存器状态、内存页等），然后，把新任务的执行环境准备好（恢复上次的寄存器状态，切换内存页等），才能开始执行。这个切换过程虽然很快，但是也需要耗费时间。如果有几千个任务同时进行，操作系统可能就主要忙着切换任务，根本没有多少时间去执行任务了，这种情况最常见的就是硬盘狂响，点窗口无反应，系统处于假死状态。所以，多任务一旦多到一个限度，反而会使得系统性能急剧下降，最终导致所有任务都做不好。 是否采用多任务的第二个考虑是任务的类型，可以把任务分为计算密集型和I/O密集型。计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如对视频进行编码解码或者格式转换等等，这种任务全靠CPU的运算能力，虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低。计算密集型任务由于主要消耗CPU资源，这类任务用Python这样的脚本语言去执行效率通常很低，最能胜任这类任务的是C语言，我们之前提到了Python中有嵌入C/C++代码的机制。 除了计算密集型任务，其他的涉及到网络、存储介质I/O的任务都可以视为I/O密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待I/O操作完成（因为I/O的速度远远低于CPU和内存的速度）。对于I/O密集型任务，如果启动多任务，就可以减少I/O等待时间从而让CPU高效率的运转。有一大类的任务都属于I/O密集型任务，这其中包括了我们很快会涉及到的网络应用和Web应用。 说明： 上面的内容和例子来自于廖雪峰官方网站的《Python教程》，因为对作者文中的某些观点持有不同的看法，对原文的文字描述做了适当的调整。 单线程+异步I/O现代操作系统对I/O操作的改进中最为重要的就是支持异步I/O。如果充分利用操作系统提供的异步I/O支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为事件驱动模型。Nginx就是支持异步I/O的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。用Node.js开发的服务器端程序也使用了这种工作模式，这也是当下实现多任务编程的一种趋势。 在Python语言中，单线程+异步I/O的编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。协程最大的优势就是极高的执行效率，因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销。协程的第二个优势就是不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不用加锁，只需要判断状态就好了，所以执行效率比多线程高很多。如果想要充分利用CPU的多核特性，最简单的方法是多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。关于这方面的内容，我稍后会做一个专题来进行讲解。 应用案例例子1：将耗时间的任务放到线程中以获得更好的用户体验。如下所示的界面中，有“下载”和“关于”两个按钮，用休眠的方式模拟点击“下载”按钮会联网下载文件需要耗费10秒的时间，如果不使用“多线程”，我们会发现，当点击“下载”按钮后整个程序的其他部分都被这个耗时间的任务阻塞而无法执行了，这显然是非常糟糕的用户体验，代码如下所示。 123456789101112131415161718192021222324252627282930313233import timeimport tkinterimport tkinter.messageboxdef download(): # 模拟下载任务需要花费10秒钟时间 time.sleep(10) tkinter.messagebox.showinfo('提示', '下载完成!')def show_about(): tkinter.messagebox.showinfo('关于', '作者: 骆昊(v1.0)')def main(): top = tkinter.Tk() top.title('单线程') top.geometry('200x150') top.wm_attributes('-topmost', True) panel = tkinter.Frame(top) button1 = tkinter.Button(panel, text='下载', command=download) button1.pack(side='left') button2 = tkinter.Button(panel, text='关于', command=show_about) button2.pack(side='right') panel.pack(side='bottom') tkinter.mainloop()if __name__ == '__main__': main() 如果使用多线程将耗时间的任务放到一个独立的线程中执行，这样就不会因为执行耗时间的任务而阻塞了主线程，修改后的代码如下所示。 12345678910111213141516171819202122232425262728293031323334353637383940414243import timeimport tkinterimport tkinter.messageboxfrom threading import Threaddef main(): class DownloadTaskHandler(Thread): def run(self): time.sleep(10) tkinter.messagebox.showinfo('提示', '下载完成!') # 启用下载按钮 button1.config(state=tkinter.NORMAL) def download(): # 禁用下载按钮 button1.config(state=tkinter.DISABLED) # 通过daemon参数将线程设置为守护线程(主程序退出就不再保留执行) # 在线程中处理耗时间的下载任务 DownloadTaskHandler(daemon=True).start() def show_about(): tkinter.messagebox.showinfo('关于', '作者: 骆昊(v1.0)') top = tkinter.Tk() top.title('单线程') top.geometry('200x150') top.wm_attributes('-topmost', 1) panel = tkinter.Frame(top) button1 = tkinter.Button(panel, text='下载', command=download) button1.pack(side='left') button2 = tkinter.Button(panel, text='关于', command=show_about) button2.pack(side='right') panel.pack(side='bottom') tkinter.mainloop()if __name__ == '__main__': main() 例子2：使用多进程对复杂任务进行“分而治之”。我们来完成1~100000000求和的计算密集型任务，这个问题本身非常简单，有点循环的知识就能解决，代码如下所示。 12345678910111213141516from time import timedef main(): total = 0 number_list = [x for x in range(1, 100000001)] start = time() for number in number_list: total += number print(total) end = time() print('Execution time: %.3fs' % (end - start))if __name__ == '__main__': main() 在上面的代码中，我故意先去创建了一个列表容器然后填入了100000000个数，这一步其实是比较耗时间的，所以为了公平起见，当我们将这个任务分解到8个进程中去执行的时候，我们暂时也不考虑列表切片操作花费的时间，只是把做运算和合并运算结果的时间统计出来，代码如下所示。 123456789101112131415161718192021222324252627282930313233343536373839from multiprocessing import Process, Queuefrom random import randintfrom time import timedef task_handler(curr_list, result_queue): total = 0 for number in curr_list: total += number result_queue.put(total)def main(): processes = [] number_list = [x for x in range(1, 100000001)] result_queue = Queue() index = 0 # 启动8个进程将数据切片后进行运算 for _ in range(8): p = Process(target=task_handler, args=(number_list[index:index + 12500000], result_queue)) index += 12500000 processes.append(p) p.start() # 开始记录所有进程执行完成花费的时间 start = time() for p in processes: p.join() # 合并执行结果 total = 0 while not result_queue.empty(): total += result_queue.get() print(total) end = time() print('Execution time: ', (end - start), 's', sep='')if __name__ == '__main__': main() 比较两段代码的执行结果（在我目前使用的MacBook上，上面的代码需要大概6秒左右的时间，而下面的代码只需要不到1秒的时间，再强调一次我们只是比较了运算的时间，不考虑列表创建及切片操作花费的时间），使用多进程后由于获得了更多的CPU执行时间以及更好的利用了CPU的多核特性，明显的减少了程序的执行时间，而且计算量越大效果越明显。当然，如果愿意还可以将多个进程部署在不同的计算机上，做成分布式进程，具体的做法就是通过multiprocessing.managers模块中提供的管理器将Queue对象通过网络共享出来（注册到网络上让其他计算机可以访问），这部分内容也留到爬虫的专题再进行讲解。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[12.使用正则表达式]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-12-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[使用正则表达式正则表达式相关知识在编写处理字符串的程序或网页时，经常会有查找符合某些复杂规则的字符串的需要，正则表达式就是用于描述这些规则的工具，换句话说正则表达式是一种工具，它定义了字符串的匹配模式（如何检查一个字符串是否有跟某种模式匹配的部分或者从一个字符串中将与模式匹配的部分提取出来或者替换掉）。如果你在Windows操作系统中使用过文件查找并且在指定文件名时使用过通配符（*和?），那么正则表达式也是与之类似的用来进行文本匹配的工具，只不过比起通配符正则表达式更强大，它能更精确地描述你的需求（当然你付出的代价是书写一个正则表达式比打出一个通配符要复杂得多，要知道任何给你带来好处的东西都是有代价的，就如同学习一门编程语言一样），比如你可以编写一个正则表达式，用来查找所有以0开头，后面跟着2-3个数字，然后是一个连字号“-”，最后是7或8位数字的字符串（像028-12345678或0813-7654321），这不就是国内的座机号码吗。最初计算机是为了做数学运算而诞生的，处理的信息基本上都是数值，而今天我们在日常工作中处理的信息基本上都是文本数据，我们希望计算机能够识别和处理符合某些模式的文本，正则表达式就显得非常重要了。今天几乎所有的编程语言都提供了对正则表达式操作的支持，Python通过标准库中的re模块来支持正则表达式操作。 我们可以考虑下面一个问题：我们从某个地方（可能是一个文本文件，也可能是网络上的一则新闻）获得了一个字符串，希望在字符串中找出手机号和座机号。当然我们可以设定手机号是11位的数字（注意并不是随机的11位数字，因为你没有见过“25012345678”这样的手机号吧）而座机号跟上一段中描述的模式相同，如果不使用正则表达式要完成这个任务就会很麻烦。 关于正则表达式的相关知识，大家可以阅读一篇非常有名的博客叫《正则表达式30分钟入门教程》，读完这篇文章后你就可以看懂下面的表格，这是我们对正则表达式中的一些基本符号进行的扼要总结。 符号 解释 示例 说明 . 匹配任意字符 b.t 可以匹配bat / but / b#t / b1t等 \w 匹配字母/数字/下划线 b\wt 可以匹配bat / b1t / b_t等但不能匹配b#t \s 匹配空白字符（包括\r、\n、\t等） love\syou 可以匹配love you \d 匹配数字 \d\d 可以匹配01 / 23 / 99等 \b 匹配单词的边界 \bThe\b ^ 匹配字符串的开始 ^The 可以匹配The开头的字符串 $ 匹配字符串的结束 .exe$ 可以匹配.exe结尾的字符串 \W 匹配非字母/数字/下划线 b\Wt 可以匹配b#t / b@t等但不能匹配but / b1t / b_t等 \S 匹配非空白字符 love\Syou 可以匹配love#you等但不能匹配love you \D 匹配非数字 \d\D 可以匹配9a / 3# / 0F等 \B 匹配非单词边界 \Bio\B [] 匹配来自字符集的任意单一字符 [aeiou] 可以匹配任一元音字母字符 [^] 匹配不在字符集中的任意单一字符 [^aeiou] 可以匹配任一非元音字母字符 * 匹配0次或多次 \w* + 匹配1次或多次 \w+ ? 匹配0次或1次 \w? {N} 匹配N次 \w{3} {M,} 匹配至少M次 \w{3,} {M,N} 匹配至少M次至多N次 \w{3,6} | 分支 foo|bar 可以匹配foo或者bar (?#) 注释 (exp) 匹配exp并捕获到自动命名的组中 (?&nbsp;&lt;name&gt;exp) 匹配exp并捕获到名为name的组中 (?:exp) 匹配exp但是不捕获匹配的文本 (?=exp) 匹配exp前面的位置 \b\w+(?=ing) 可以匹配I’m dancing中的danc (?&lt;=exp) 匹配exp后面的位置 (?&lt;=\bdanc)\w+\b 可以匹配I love dancing and reading中的第一个ing (?!exp) 匹配后面不是exp的位置 (?&lt;!exp) 匹配前面不是exp的位置 *? 重复任意次，但尽可能少重复 a.*ba.*?b 将正则表达式应用于aabab，前者会匹配整个字符串aabab，后者会匹配aab和ab两个字符串 +? 重复1次或多次，但尽可能少重复 ?? 重复0次或1次，但尽可能少重复 {M,N}? 重复M到N次，但尽可能少重复 {M,}? 重复M次以上，但尽可能少重复 说明： 如果需要匹配的字符是正则表达式中的特殊字符，那么可以使用\进行转义处理，例如想匹配小数点可以写成\.就可以了，因为直接写.会匹配任意字符；同理，想匹配圆括号必须写成\(和\)，否则圆括号被视为正则表达式中的分组。 Python对正则表达式的支持Python提供了re模块来支持正则表达式相关操作，下面是re模块中的核心函数。 函数 说明 compile(pattern, flags=0) 编译正则表达式返回正则表达式对象 match(pattern, string, flags=0) 用正则表达式匹配字符串 成功返回匹配对象 否则返回None search(pattern, string, flags=0) 搜索字符串中第一次出现正则表达式的模式 成功返回匹配对象 否则返回None split(pattern, string, maxsplit=0, flags=0) 用正则表达式指定的模式分隔符拆分字符串 返回列表 sub(pattern, repl, string, count=0, flags=0) 用指定的字符串替换原字符串中与正则表达式匹配的模式 可以用count指定替换的次数 fullmatch(pattern, string, flags=0) match函数的完全匹配（从字符串开头到结尾）版本 findall(pattern, string, flags=0) 查找字符串所有与正则表达式匹配的模式 返回字符串的列表 finditer(pattern, string, flags=0) 查找字符串所有与正则表达式匹配的模式 返回一个迭代器 purge() 清除隐式编译的正则表达式的缓存 re.I / re.IGNORECASE 忽略大小写匹配标记 re.M / re.MULTILINE 多行匹配标记 说明： 上面提到的re模块中的这些函数，实际开发中也可以用正则表达式对象的方法替代对这些函数的使用，如果一个正则表达式需要重复的使用，那么先通过compile函数编译正则表达式并创建出正则表达式对象无疑是更为明智的选择。 下面我们通过一系列的例子来告诉大家在Python中如何使用正则表达式。 例子1：验证输入用户名和QQ号是否有效并给出对应的提示信息。12345678910111213141516171819202122232425"""验证输入用户名和QQ号是否有效并给出对应的提示信息要求：用户名必须由字母、数字或下划线构成且长度在6~20个字符之间，QQ号是5~12的数字且首位不能为0"""import redef main(): username = input('请输入用户名: ') qq = input('请输入QQ号: ') # match函数的第一个参数是正则表达式字符串或正则表达式对象 # 第二个参数是要跟正则表达式做匹配的字符串对象 m1 = re.match(r'^[0-9a-zA-Z_]&#123;6,20&#125;$', username) if not m1: print('请输入有效的用户名.') m2 = re.match(r'^[1-9]\d&#123;4,11&#125;$', qq) if not m2: print('请输入有效的QQ号.') if m1 and m2: print('你输入的信息是有效的!')if __name__ == '__main__': main() 提示： 上面在书写正则表达式时使用了“原始字符串”的写法（在字符串前面加上了r），所谓“原始字符串”就是字符串中的每个字符都是它原始的意义，说得更直接一点就是字符串中没有所谓的转义字符啦。因为正则表达式中有很多元字符和需要进行转义的地方，如果不使用原始字符串就需要将反斜杠写作\\，例如表示数字的\d得书写成\\d，这样不仅写起来不方便，阅读的时候也会很吃力。 例子2：从一段文字中提取出国内手机号码。下面这张图是截止到2017年底，国内三家运营商推出的手机号段。 123456789101112131415161718192021222324252627import redef main(): # 创建正则表达式对象 使用了前瞻和回顾来保证手机号前后不应该出现数字 pattern = re.compile(r'(?&lt;=\D)1[34578]\d&#123;9&#125;(?=\D)') sentence = ''' 重要的事情说8130123456789遍，我的手机号是13512346789这个靓号， 不是15600998765，也是110或119，王大锤的手机号才是15600998765。 ''' # 查找所有匹配并保存到一个列表中 mylist = re.findall(pattern, sentence) print(mylist) print('--------华丽的分隔线--------') # 通过迭代器取出匹配对象并获得匹配的内容 for temp in pattern.finditer(sentence): print(temp.group()) print('--------华丽的分隔线--------') # 通过search函数指定搜索位置找出所有匹配 m = pattern.search(sentence) while m: print(m.group()) m = pattern.search(sentence, m.end())if __name__ == '__main__': main() 说明： 上面匹配国内手机号的正则表达式并不够好，因为像14开头的号码只有145或147，而上面的正则表达式并没有考虑这种情况，要匹配国内手机号，更好的正则表达式的写法是：(?&lt;=\D)(1[38]\d{9}|14[57]\d{8}|15[0-35-9]\d{8}|17[678]\d{8})(?=\D)，国内最近好像有19和16开头的手机号了，但是这个暂时不在我们考虑之列。 例子3：替换字符串中的不良内容123456789101112import redef main(): sentence = '你丫是傻叉吗? 我操你大爷的. Fuck you.' purified = re.sub('[操肏艹]|fuck|shit|傻[比屄逼叉缺吊屌]|煞笔', '*', sentence, flags=re.IGNORECASE) print(purified) # 你丫是*吗? 我*你大爷的. * you.if __name__ == '__main__': main() 说明： re模块的正则表达式相关函数中都有一个flags参数，它代表了正则表达式的匹配标记，可以通过该标记来指定匹配时是否忽略大小写、是否进行多行匹配、是否显示调试信息等。如果需要为flags参数指定多个值，可以使用按位或运算符进行叠加，如flags=re.I | re.M。 例子4：拆分长字符串12345678910111213import redef main(): poem = '窗前明月光，疑是地上霜。举头望明月，低头思故乡。' sentence_list = re.split(r'[，。, .]', poem) while '' in sentence_list: sentence_list.remove('') print(sentence_list) # ['窗前明月光', '疑是地上霜', '举头望明月', '低头思故乡']if __name__ == '__main__': main() 后话如果要从事爬虫类应用的开发，那么正则表达式一定是一个非常好的助手，因为它可以帮助我们迅速的从网页代码中发现某种我们指定的模式并提取出我们需要的信息，当然对于初学者来收，要编写一个正确的适当的正则表达式可能并不是一件容易的事情（当然有些常用的正则表达式可以直接在网上找找），所以实际开发爬虫应用的时候，有很多人会选择Beautiful Soup或Lxml来进行匹配和信息的提取，前者简单方便但是性能较差，后者既好用性能也好，但是安装稍嫌麻烦，这些内容我们会在后期的爬虫专题中为大家介绍。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[11.文件和异常]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-11-%E6%96%87%E4%BB%B6%E5%92%8C%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[文件和异常实际开发中常常会遇到对数据进行持久化操作的场景，而实现数据持久化最直接简单的方式就是将数据保存到文件中。说到“文件”这个词，可能需要先科普一下关于文件系统的知识，但是这里我们并不浪费笔墨介绍这个概念，请大家自行通过维基百科进行了解。 在Python中实现文件的读写操作其实非常简单，通过Python内置的open函数，我们可以指定文件名、操作模式、编码信息等来获得操作文件的对象，接下来就可以对文件进行读写操作了。这里所说的操作模式是指要打开什么样的文件（字符文件还是二进制文件）以及做什么样的操作（读、写还是追加），具体的如下表所示。 操作模式 具体含义 &#39;r&#39; 读取 （默认） &#39;w&#39; 写入（会先截断之前的内容） &#39;x&#39; 写入，如果文件已经存在会产生异常 &#39;a&#39; 追加，将内容写入到已有文件的末尾 &#39;b&#39; 二进制模式 &#39;t&#39; 文本模式（默认） &#39;+&#39; 更新（既可以读又可以写） 下面这张图来自于菜鸟教程网站，它展示了如果根据应用程序的需要来设置操作模式。 读写文本文件读取文本文件时，需要在使用open函数时指定好带路径的文件名（可以使用相对路径或绝对路径）并将文件模式设置为&#39;r&#39;（如果不指定，默认值也是&#39;r&#39;），然后通过encoding参数指定编码（如果不指定，默认值是None，那么在读取文件时使用的是操作系统默认的编码），如果不能保证保存文件时使用的编码方式与encoding参数指定的编码方式是一致的，那么就可能因无法解码字符而导致读取失败。下面的例子演示了如何读取一个纯文本文件。 12345678def main(): f = open('致橡树.txt', 'r', encoding='utf-8') print(f.read()) f.close()if __name__ == '__main__': main() 请注意上面的代码，如果open函数指定的文件并不存在或者无法打开，那么将引发异常状况导致程序崩溃。为了让代码有一定的健壮性和容错性，我们可以使用Python的异常机制对可能在运行时发生状况的代码进行适当的处理，如下所示。 123456789101112131415161718def main(): f = None try: f = open('致橡树.txt', 'r', encoding='utf-8') print(f.read()) except FileNotFoundError: print('无法打开指定的文件!') except LookupError: print('指定了未知的编码!') except UnicodeDecodeError: print('读取文件时解码错误!') finally: if f: f.close()if __name__ == '__main__': main() 在Python中，我们可以将那些在运行时可能会出现状况的代码放在try代码块中，在try代码块的后面可以跟上一个或多个except来捕获可能出现的异常状况。例如在上面读取文件的过程中，文件找不到会引发FileNotFoundError，指定了未知的编码会引发LookupError，而如果读取文件时无法按指定方式解码会引发UnicodeDecodeError，我们在try后面跟上了三个except分别处理这三种不同的异常状况。最后我们使用finally代码块来关闭打开的文件，释放掉程序中获取的外部资源，由于finally块的代码不论程序正常还是异常都会执行到（甚至是调用了sys模块的exit函数退出Python环境，finally块都会被执行，因为exit函数实质上是引发了SystemExit异常），因此我们通常把finally块称为“总是执行代码块”，它最适合用来做释放外部资源的操作。如果不愿意在finally代码块中关闭文件对象释放资源，也可以使用上下文语法，通过with关键字指定文件对象的上下文环境并在离开上下文环境时自动释放文件资源，代码如下所示。 1234567891011121314def main(): try: with open('致橡树.txt', 'r', encoding='utf-8') as f: print(f.read()) except FileNotFoundError: print('无法打开指定的文件!') except LookupError: print('指定了未知的编码!') except UnicodeDecodeError: print('读取文件时解码错误!')if __name__ == '__main__': main() 除了使用文件对象的read方法读取文件之外，还可以使用for-in循环逐行读取或者用readlines方法将文件按行读取到一个列表容器中，代码如下所示。 1234567891011121314151617181920212223import timedef main(): # 一次性读取整个文件内容 with open('致橡树.txt', 'r', encoding='utf-8') as f: print(f.read()) # 通过for-in循环逐行读取 with open('致橡树.txt', mode='r') as f: for line in f: print(line, end='') time.sleep(0.5) print() # 读取文件按行读取到列表中 with open('致橡树.txt') as f: lines = f.readlines() print(lines) if __name__ == '__main__': main() 要将文本信息写入文件文件也非常简单，在使用open函数时指定好文件名并将文件模式设置为&#39;w&#39;即可。注意如果需要对文件内容进行追加式写入，应该将模式设置为&#39;a&#39;。如果要写入的文件不存在会自动创建文件而不是引发异常。下面的例子演示了如何将1-9999之间的素数分别写入三个文件中（1-99之间的素数保存在a.txt中，100-999之间的素数保存在b.txt中，1000-9999之间的素数保存在c.txt中）。 12345678910111213141516171819202122232425262728293031323334353637from math import sqrtdef is_prime(n): """判断素数的函数""" assert n &gt; 0 for factor in range(2, int(sqrt(n)) + 1): if n % factor == 0: return False return True if n != 1 else Falsedef main(): filenames = ('a.txt', 'b.txt', 'c.txt') fs_list = [] try: for filename in filenames: fs_list.append(open(filename, 'w', encoding='utf-8')) for number in range(1, 10000): if is_prime(number): if number &lt; 100: fs_list[0].write(str(number) + '\n') elif number &lt; 1000: fs_list[1].write(str(number) + '\n') else: fs_list[2].write(str(number) + '\n') except IOError as ex: print(ex) print('写文件时发生错误!') finally: for fs in fs_list: fs.close() print('操作完成!')if __name__ == '__main__': main() 读写二进制文件知道了如何读写文本文件要读写二进制文件也就很简单了，下面的代码实现了复制图片文件的功能。 12345678910111213141516def main(): try: with open('guido.jpg', 'rb') as fs1: data = fs1.read() print(type(data)) # &lt;class 'bytes'&gt; with open('吉多.jpg', 'wb') as fs2: fs2.write(data) except FileNotFoundError as e: print('指定的文件无法打开.') except IOError as e: print('读写文件时出现错误.') print('程序执行结束.')if __name__ == '__main__': main() 读写JSON文件通过上面的讲解，我们已经知道如何将文本数据和二进制数据保存到文件中，那么这里还有一个问题，如果希望把一个列表或者一个字典中的数据保存到文件中又该怎么做呢？答案是将数据以JSON格式进行保存。JSON是“JavaScript Object Notation”的缩写，它本来是JavaScript语言中创建对象的一种字面量语法，现在已经被广泛的应用于跨平台跨语言的数据交换，原因很简单，因为JSON也是纯文本，任何系统任何编程语言处理纯文本都是没有问题的。目前JSON基本上已经取代了XML作为异构系统间交换数据的事实标准。关于JSON的知识，更多的可以参考JSON的官方网站，从这个网站也可以了解到每种语言处理JSON数据格式可以使用的工具或三方库，下面是一个JSON的简单例子。 1234567891011&#123; "name": "骆昊", "age": 38, "qq": 957658, "friends": ["王大锤", "白元芳"], "cars": [ &#123;"brand": "BYD", "max_speed": 180&#125;, &#123;"brand": "Audi", "max_speed": 280&#125;, &#123;"brand": "Benz", "max_speed": 320&#125; ]&#125; 可能大家已经注意到了，上面的JSON跟Python中的字典其实是一样一样的，事实上JSON的数据类型和Python的数据类型是很容易找到对应关系的，如下面两张表所示。 JSON Python object dict array list string str number (int / real) int / float true / false True / False null None Python JSON dict object list, tuple array str string int, float, int- &amp; float-derived Enums number True / False true / false None null 我们使用Python中的json模块就可以将字典或列表以JSON格式保存到文件中，代码如下所示。 12345678910111213141516171819202122232425import jsondef main(): mydict = &#123; 'name': '骆昊', 'age': 38, 'qq': 957658, 'friends': ['王大锤', '白元芳'], 'cars': [ &#123;'brand': 'BYD', 'max_speed': 180&#125;, &#123;'brand': 'Audi', 'max_speed': 280&#125;, &#123;'brand': 'Benz', 'max_speed': 320&#125; ] &#125; try: with open('data.json', 'w', encoding='utf-8') as fs: json.dump(mydict, fs) except IOError as e: print(e) print('保存数据完成!')if __name__ == '__main__': main() json模块主要有四个比较重要的函数，分别是： dump - 将Python对象按照JSON格式序列化到文件中 dumps - 将Python对象处理成JSON格式的字符串 load - 将文件中的JSON数据反序列化成对象 loads - 将字符串的内容反序列化成Python对象 这里出现了两个概念，一个叫序列化，一个叫反序列化。自由的百科全书维基百科上对这两个概念是这样解释的：“序列化（serialization）在计算机科学的数据处理中，是指将数据结构或对象状态转换为可以存储或传输的形式，这样在需要的时候能够恢复到原先的状态，而且通过序列化的数据重新获取字节时，可以利用这些字节来产生原始对象的副本（拷贝）。与这个过程相反的动作，即从一系列字节中提取数据结构的操作，就是反序列化（deserialization）”。 目前绝大多数网络数据服务（或称之为网络API）都是基于HTTP协议提供JSON格式的数据，关于HTTP协议的相关知识，可以看看阮一峰老师的《HTTP协议入门》，如果想了解国内的网络数据服务，可以看看聚合数据和阿凡达数据等网站，国外的可以看看{API}Search网站。下面的例子演示了如何使用requests模块（封装得足够好的第三方网络访问模块）访问网络API获取国内新闻，如何通过json模块解析JSON数据并显示新闻标题，这个例子使用了天行数据提供的国内新闻数据接口，其中的APIKey需要自己到该网站申请。 12345678910111213import requestsimport jsondef main(): resp = requests.get('http://api.tianapi.com/guonei/?key=APIKey&amp;num=10') data_model = json.loads(resp.text) for news in data_model['newslist']: print(news['title'])if __name__ == '__main__': main() 在Python中要实现序列化和反序列化除了使用json模块之外，还可以使用pickle和shelve模块，但是这两个模块是使用特有的序列化协议来序列化数据，因此序列化后的数据只能被Python识别。关于这两个模块的相关知识可以自己看看网络上的资料。另外，如果要了解更多的关于Python异常机制的知识，可以看看segmentfault上面的文章《总结：Python中的异常处理》，这篇文章不仅介绍了Python中异常机制的使用，还总结了一系列的最佳实践，很值得一读。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10.图形用户界面和游戏开发]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-10-%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2%E5%92%8C%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[图形用户界面和游戏开发基于tkinter模块的GUIGUI是图形用户界面的缩写，图形化的用户界面对使用过计算机的人来说应该都不陌生，在此也无需进行赘述。Python默认的GUI开发模块是tkinter（在Python 3以前的版本中名为Tkinter），从这个名字就可以看出它是基于Tk的，Tk是一个工具包，最初是为Tcl设计的，后来被移植到很多其他的脚本语言中，它提供了跨平台的GUI控件。当然Tk并不是最新和最好的选择，也没有功能特别强大的GUI控件，事实上，开发GUI应用并不是Python最擅长的工作，如果真的需要使用Python开发GUI应用，wxPython、PyQt、PyGTK等模块都是不错的选择。 基本上使用tkinter来开发GUI应用需要以下5个步骤： 导入tkinter模块中我们需要的东西。 创建一个顶层窗口对象并用它来承载整个GUI应用。 在顶层窗口对象上添加GUI组件。 通过代码将这些GUI组件的功能组织起来。 进入主事件循环(main loop)。 下面的代码演示了如何使用tkinter做一个简单的GUI应用。 12345678910111213141516171819202122232425262728293031323334353637383940414243import tkinterimport tkinter.messageboxdef main(): flag = True # 修改标签上的文字 def change_label_text(): nonlocal flag flag = not flag color, msg = ('red', 'Hello, world!')\ if flag else ('blue', 'Goodbye, world!') label.config(text=msg, fg=color) # 确认退出 def confirm_to_quit(): if tkinter.messagebox.askokcancel('温馨提示', '确定要退出吗?'): top.quit() # 创建顶层窗口 top = tkinter.Tk() # 设置窗口大小 top.geometry('240x160') # 设置窗口标题 top.title('小游戏') # 创建标签对象并添加到顶层窗口 label = tkinter.Label(top, text='Hello, world!', font='Arial -32', fg='red') label.pack(expand=1) # 创建一个装按钮的容器 panel = tkinter.Frame(top) # 创建按钮对象 指定添加到哪个容器中 通过command参数绑定事件回调函数 button1 = tkinter.Button(panel, text='修改', command=change_label_text) button1.pack(side='left') button2 = tkinter.Button(panel, text='退出', command=confirm_to_quit) button2.pack(side='right') panel.pack(side='bottom') # 开启主事件循环 tkinter.mainloop()if __name__ == '__main__': main() 需要说明的是，GUI应用通常是事件驱动式的，之所以要进入主事件循环就是要监听鼠标、键盘等各种事件的发生并执行对应的代码对事件进行处理，因为事件会持续的发生，所以需要这样的一个循环一直运行着等待下一个事件的发生。另一方面，Tk为控件的摆放提供了三种布局管理器，通过布局管理器可以对控件进行定位，这三种布局管理器分别是：Placer（开发者提供控件的大小和摆放位置）、Packer（自动将控件填充到合适的位置）和Grid（基于网格坐标来摆放控件），此处不进行赘述。 使用Pygame进行游戏开发Pygame是一个开源的Python模块，专门用于多媒体应用（如电子游戏）的开发，其中包含对图像、声音、视频、事件、碰撞等的支持。Pygame建立在SDL的基础上，SDL是一套跨平台的多媒体开发库，用C语言实现，被广泛的应用于游戏、模拟器、播放器等的开发。而Pygame让游戏开发者不再被底层语言束缚，可以更多的关注游戏的功能和逻辑。 下面我们来完成一个简单的小游戏，游戏的名字叫“大球吃小球”，当然完成这个游戏并不是重点，学会使用Pygame也不是重点，最重要的我们要在这个过程中体会如何使用前面讲解的面向对象程序设计，学会用这种编程思想去解决现实中的问题。 制作游戏窗口123456789101112131415161718192021import pygamedef main(): # 初始化导入的pygame中的模块 pygame.init() # 初始化用于显示的窗口并设置窗口尺寸 screen = pygame.display.set_mode((800, 600)) # 设置当前窗口的标题 pygame.display.set_caption('大球吃小球') running = True # 开启一个事件循环处理发生的事件 while running: # 从消息队列中获取事件并对事件进行处理 for event in pygame.event.get(): if event.type == pygame.QUIT: running = Falseif __name__ == '__main__': main() 在窗口中绘图可以通过pygame中draw模块的函数在窗口上绘图，可以绘制的图形包括：线条、矩形、多边形、圆、椭圆、圆弧等。需要说明的是，屏幕坐标系是将屏幕左上角设置为坐标原点(0, 0)，向右是x轴的正向，向下是y轴的正向，在表示位置或者设置尺寸的时候，我们默认的单位都是像素。所谓像素就是屏幕上的一个点，你可以用浏览图片的软件试着将一张图片放大若干倍，就可以看到这些点。pygame中表示颜色用的是色光三原色表示法，即通过一个元组或列表来指定颜色的RGB值，每个值都在0~255之间，因为是每种原色都用一个8位（bit）的值来表示，三种颜色相当于一共由24位构成，这也就是常说的“24位颜色表示法”。 123456789101112131415161718192021222324252627import pygamedef main(): # 初始化导入的pygame中的模块 pygame.init() # 初始化用于显示的窗口并设置窗口尺寸 screen = pygame.display.set_mode((800, 600)) # 设置当前窗口的标题 pygame.display.set_caption('大球吃小球') # 设置窗口的背景色(颜色是由红绿蓝三原色构成的元组) screen.fill((242, 242, 242)) # 绘制一个圆(参数分别是: 屏幕, 颜色, 圆心位置, 半径, 0表示填充圆) pygame.draw.circle(screen, (255, 0, 0,), (100, 100), 30, 0) # 刷新当前窗口(渲染窗口将绘制的图像呈现出来) pygame.display.flip() running = True # 开启一个事件循环处理发生的事件 while running: # 从消息队列中获取事件并对事件进行处理 for event in pygame.event.get(): if event.type == pygame.QUIT: running = Falseif __name__ == '__main__': main() 加载图像如果需要直接加载图像到窗口上，可以使用pygame中image模块的函数来加载图像，再通过之前获得的窗口对象的blit方法渲染图像，代码如下所示。 1234567891011121314151617181920212223242526272829import pygamedef main(): # 初始化导入的pygame中的模块 pygame.init() # 初始化用于显示的窗口并设置窗口尺寸 screen = pygame.display.set_mode((800, 600)) # 设置当前窗口的标题 pygame.display.set_caption('大球吃小球') # 设置窗口的背景色(颜色是由红绿蓝三原色构成的元组) screen.fill((255, 255, 255)) # 通过指定的文件名加载图像 ball_image = pygame.image.load('/images/Python100/ball.png') # 在窗口上渲染图像 screen.blit(ball_image, (50, 50)) # 刷新当前窗口(渲染窗口将绘制的图像呈现出来) pygame.display.flip() running = True # 开启一个事件循环处理发生的事件 while running: # 从消息队列中获取事件并对事件进行处理 for event in pygame.event.get(): if event.type == pygame.QUIT: running = Falseif __name__ == '__main__': main() 实现动画效果说到动画这个词大家都不会陌生，事实上要实现动画效果，本身的原理也非常简单，就是将不连续的图片连续的播放，只要每秒钟达到了一定的帧数，那么就可以做出比较流畅的动画效果。如果要让上面代码中的小球动起来，可以将小球的位置用变量来表示，并在循环中修改小球的位置再刷新整个窗口即可。 1234567891011121314151617181920212223242526272829import pygamedef main(): # 初始化导入的pygame中的模块 pygame.init() # 初始化用于显示的窗口并设置窗口尺寸 screen = pygame.display.set_mode((800, 600)) # 设置当前窗口的标题 pygame.display.set_caption('大球吃小球') # 定义变量来表示小球在屏幕上的位置 x, y = 50, 50 running = True # 开启一个事件循环处理发生的事件 while running: # 从消息队列中获取事件并对事件进行处理 for event in pygame.event.get(): if event.type == pygame.QUIT: running = False screen.fill((255, 255, 255)) pygame.draw.circle(screen, (255, 0, 0,), (x, y), 30, 0) pygame.display.flip() # 每隔50毫秒就改变小球的位置再刷新窗口 pygame.time.delay(50) x, y = x + 5, y + 5if __name__ == '__main__': main() 碰撞检测通常一个游戏中会有很多对象出现，而这些对象之间的“碰撞”在所难免，比如炮弹击中了飞机、箱子撞到了地面等。碰撞检测在绝大多数的游戏中都是一个必须得处理的至关重要的问题，pygame的sprite（动画精灵）模块就提供了对碰撞检测的支持，这里我们暂时不介绍sprite模块提供的功能，因为要检测两个小球有没有碰撞其实非常简单，只需要检查球心的距离有没有小于两个球的半径之和。为了制造出更多的小球，我们可以通过对鼠标事件的处理，在点击鼠标的位置创建颜色、大小和移动速度都随机的小球，当然要做到这一点，我们可以把之前学习到的面向对象的知识应用起来。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465from enum import Enum, uniquefrom math import sqrtfrom random import randintimport pygame@uniqueclass Color(Enum): """颜色""" RED = (255, 0, 0) GREEN = (0, 255, 0) BLUE = (0, 0, 255) BLACK = (0, 0, 0) WHITE = (255, 255, 255) GRAY = (242, 242, 242) @staticmethod def random_color(): """获得随机颜色""" r = randint(0, 255) g = randint(0, 255) b = randint(0, 255) return (r, g, b)class Ball(object): """球""" def __init__(self, x, y, radius, sx, sy, color=Color.RED): """初始化方法""" self.x = x self.y = y self.radius = radius self.sx = sx self.sy = sy self.color = color self.alive = True def move(self, screen): """移动""" self.x += self.sx self.y += self.sy if self.x - self.radius &lt;= 0 or \ self.x + self.radius &gt;= screen.get_width(): self.sx = -self.sx if self.y - self.radius &lt;= 0 or \ self.y + self.radius &gt;= screen.get_height(): self.sy = -self.sy def eat(self, other): """吃其他球""" if self.alive and other.alive and self != other: dx, dy = self.x - other.x, self.y - other.y distance = sqrt(dx ** 2 + dy ** 2) if distance &lt; self.radius + other.radius \ and self.radius &gt; other.radius: other.alive = False self.radius = self.radius + int(other.radius * 0.146) def draw(self, screen): """在窗口上绘制球""" pygame.draw.circle(screen, self.color, (self.x, self.y), self.radius, 0) 事件处理可以在事件循环中对鼠标事件进行处理，通过事件对象的type属性可以判定事件类型，再通过pos属性就可以获得鼠标点击的位置。如果要处理键盘事件也是在这个地方，做法与处理鼠标事件类似。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def main(): # 定义用来装所有球的容器 balls = [] # 初始化导入的pygame中的模块 pygame.init() # 初始化用于显示的窗口并设置窗口尺寸 screen = pygame.display.set_mode((800, 600)) # 设置当前窗口的标题 pygame.display.set_caption('大球吃小球') running = True # 开启一个事件循环处理发生的事件 while running: # 从消息队列中获取事件并对事件进行处理 for event in pygame.event.get(): if event.type == pygame.QUIT: running = False # 处理鼠标事件的代码 if event.type == pygame.MOUSEBUTTONDOWN and event.button == 1: # 获得点击鼠标的位置 x, y = event.pos radius = randint(10, 100) sx, sy = randint(-10, 10), randint(-10, 10) color = Color.random_color() # 在点击鼠标的位置创建一个球(大小、速度和颜色随机) ball = Ball(x, y, radius, sx, sy, color) # 将球添加到列表容器中 balls.append(ball) screen.fill((255, 255, 255)) # 取出容器中的球 如果没被吃掉就绘制 被吃掉了就移除 for ball in balls: if ball.alive: ball.draw(screen) else: balls.remove(ball) pygame.display.flip() # 每隔50毫秒就改变球的位置再刷新窗口 pygame.time.delay(50) for ball in balls: ball.move(screen) # 检查球有没有吃到其他的球 for other in balls: ball.eat(other)if __name__ == '__main__': main() 上面的两段代码合在一起，我们就完成了“大球吃小球”的游戏（如下图所示），准确的说它算不上一个游戏，但是做一个小游戏的基本知识我们已经通过这个例子告诉大家了，有了这些知识已经可以开始你的小游戏开发之旅了。其实上面的代码中还有很多值得改进的地方，比如刷新窗口以及让球移动起来的代码并不应该放在事件循环中，等学习了多线程的知识后，用一个后台线程来处理这些事可能是更好的选择。如果希望获得更好的用户体验，我们还可以在游戏中加入背景音乐以及在球与球发生碰撞时播放音效，利用pygame的mixer和music模块，我们可以很容易的做到这一点，大家可以自行了解这方面的知识。事实上，想了解更多的关于pygame的知识，最好的教程是pygame的官方网站，如果英语没毛病就可以赶紧去看看啦。 如果想开发3D游戏，pygame就显得力不从心了，对3D游戏开发如果有兴趣的读者不妨看看Panda3D。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[09.面向对象进阶]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-09-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[面向对象进阶在前面的章节我们已经了解了面向对象的入门知识，知道了如何定义类，如何创建对象以及如何给对象发消息。为了能够更好的使用面向对象编程思想进行程序开发，我们还需要对Python中的面向对象编程进行更为深入的了解。 @property装饰器之前我们讨论过Python中属性和方法访问权限的问题，虽然我们不建议将属性设置为私有的，但是如果直接将属性暴露给外界也是有问题的，比如我们没有办法检查赋给属性的值是否有效。我们之前的建议是将属性命名以单下划线开头，通过这种方式来暗示属性是受保护的，不建议外界直接访问，那么如果想访问属性可以通过属性的getter（访问器）和setter（修改器）方法进行对应的操作。如果要做到这点，就可以考虑使用@property包装器来包装getter和setter方法，使得对属性的访问既安全又方便，代码如下所示。 1234567891011121314151617181920212223242526272829303132333435363738class Person(object): def __init__(self, name, age): self._name = name self._age = age # 访问器 - getter方法 @property def name(self): return self._name # 访问器 - getter方法 @property def age(self): return self._age # 修改器 - setter方法 @age.setter def age(self, age): self._age = age def play(self): if self._age &lt;= 16: print('%s正在玩飞行棋.' % self._name) else: print('%s正在玩斗地主.' % self._name)def main(): person = Person('王大锤', 12) person.play() person.age = 22 person.play() # person.name = '白元芳' # AttributeError: can't set attributeif __name__ == '__main__': main() __slots__魔法我们讲到这里，不知道大家是否已经意识到，Python是一门动态语言。通常，动态语言允许我们在程序运行时给对象绑定新的属性或方法，当然也可以对已经绑定的属性和方法进行解绑定。但是如果我们需要限定自定义类型的对象只能绑定某些属性，可以通过在类中定义__slots__变量来进行限定。需要注意的是__slots__的限定只对当前类的对象生效，对子类并不起任何作用。 12345678910111213141516171819202122232425262728293031323334class Person(object): # 限定Person对象只能绑定_name, _age和_gender属性 __slots__ = ('_name', '_age', '_gender') def __init__(self, name, age): self._name = name self._age = age @property def name(self): return self._name @property def age(self): return self._age @age.setter def age(self, age): self._age = age def play(self): if self._age &lt;= 16: print('%s正在玩飞行棋.' % self._name) else: print('%s正在玩斗地主.' % self._name)def main(): person = Person('王大锤', 22) person.play() person._gender = '男' # AttributeError: 'Person' object has no attribute '_is_gay' # person._is_gay = True 静态方法和类方法之前，我们在类中定义的方法都是对象方法，也就是说这些方法都是发送给对象的消息。实际上，我们写在类中的方法并不需要都是对象方法，例如我们定义一个“三角形”类，通过传入三条边长来构造三角形，并提供计算周长和面积的方法，但是传入的三条边长未必能构造出三角形对象，因此我们可以先写一个方法来验证三条边长是否可以构成三角形，这个方法很显然就不是对象方法，因为在调用这个方法时三角形对象尚未创建出来（因为都不知道三条边能不能构成三角形），所以这个方法是属于三角形类而并不属于三角形对象的。我们可以使用静态方法来解决这类问题，代码如下所示。 123456789101112131415161718192021222324252627282930313233343536373839from math import sqrtclass Triangle(object): def __init__(self, a, b, c): self._a = a self._b = b self._c = c @staticmethod def is_valid(a, b, c): return a + b &gt; c and b + c &gt; a and a + c &gt; b def perimeter(self): return self._a + self._b + self._c def area(self): half = self.perimeter() / 2 return sqrt(half * (half - self._a) * (half - self._b) * (half - self._c))def main(): a, b, c = 3, 4, 5 # 静态方法和类方法都是通过给类发消息来调用的 if Triangle.is_valid(a, b, c): t = Triangle(a, b, c) print(t.perimeter()) # 也可以通过给类发消息来调用对象方法但是要传入接收消息的对象作为参数 # print(Triangle.perimeter(t)) print(t.area()) # print(Triangle.area(t)) else: print('无法构成三角形.')if __name__ == '__main__': main() 和静态方法比较类似，Python还可以在类中定义类方法，类方法的第一个参数约定名为cls，它代表的是当前类相关的信息的对象（类本身也是一个对象，有的地方也称之为类的元数据对象），通过这个参数我们可以获取和类相关的信息并且可以创建出类的对象，代码如下所示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445from time import time, localtime, sleepclass Clock(object): """数字时钟""" def __init__(self, hour=0, minute=0, second=0): self._hour = hour self._minute = minute self._second = second @classmethod def now(cls): ctime = localtime(time()) return cls(ctime.tm_hour, ctime.tm_min, ctime.tm_sec) def run(self): """走字""" self._second += 1 if self._second == 60: self._second = 0 self._minute += 1 if self._minute == 60: self._minute = 0 self._hour += 1 if self._hour == 24: self._hour = 0 def show(self): """显示时间""" return '%02d:%02d:%02d' % \ (self._hour, self._minute, self._second)def main(): # 通过类方法创建对象并获取系统时间 clock = Clock.now() while True: print(clock.show()) sleep(1) clock.run()if __name__ == '__main__': main() 类之间的关系简单的说，类和类之间的关系有三种：is-a、has-a和use-a关系。 is-a关系也叫继承或泛化，比如学生和人的关系、手机和电子产品的关系都属于继承关系。 has-a关系通常称之为关联，比如部门和员工的关系，汽车和引擎的关系都属于关联关系；关联关系如果是整体和部分的关联，那么我们称之为聚合关系；如果整体进一步负责了部分的生命周期（整体和部分是不可分割的，同时同在也同时消亡），那么这种就是最强的关联关系，我们称之为合成关系。 use-a关系通常称之为依赖，比如司机有一个驾驶的行为（方法），其中（的参数）使用到了汽车，那么司机和汽车的关系就是依赖关系。 我们可以使用一种叫做UML（统一建模语言）的东西来进行面向对象建模，其中一项重要的工作就是把类和类之间的关系用标准化的图形符号描述出来。关于UML我们在这里不做详细的介绍，有兴趣的读者可以自行阅读《UML面向对象设计基础》一书。 利用类之间的这些关系，我们可以在已有类的基础上来完成某些操作，也可以在已有类的基础上创建新的类，这些都是实现代码复用的重要手段。复用现有的代码不仅可以减少开发的工作量，也有利于代码的管理和维护，这是我们在日常工作中都会使用到的技术手段。 继承和多态刚才我们提到了，可以在已有类的基础上创建新类，这其中的一种做法就是让一个类从另一个类那里将属性和方法直接继承下来，从而减少重复代码的编写。提供继承信息的我们称之为父类，也叫超类或基类；得到继承信息的我们称之为子类，也叫派生类或衍生类。子类除了继承父类提供的属性和方法，还可以定义自己特有的属性和方法，所以子类比父类拥有的更多的能力，在实际开发中，我们经常会用子类对象去替换掉一个父类对象，这是面向对象编程中一个常见的行为，对应的原则称之为里氏替换原则。下面我们先看一个继承的例子。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class Person(object): """人""" def __init__(self, name, age): self._name = name self._age = age @property def name(self): return self._name @property def age(self): return self._age @age.setter def age(self, age): self._age = age def play(self): print('%s正在愉快的玩耍.' % self._name) def watch_av(self): if self._age &gt;= 18: print('%s正在观看爱情动作片.' % self._name) else: print('%s只能观看《熊出没》.' % self._name)class Student(Person): """学生""" def __init__(self, name, age, grade): super().__init__(name, age) self._grade = grade @property def grade(self): return self._grade @grade.setter def grade(self, grade): self._grade = grade def study(self, course): print('%s的%s正在学习%s.' % (self._grade, self._name, course))class Teacher(Person): """老师""" def __init__(self, name, age, title): super().__init__(name, age) self._title = title @property def title(self): return self._title @title.setter def title(self, title): self._title = title def teach(self, course): print('%s%s正在讲%s.' % (self._name, self._title, course))def main(): stu = Student('王大锤', 15, '初三') stu.study('数学') stu.watch_av() t = Teacher('骆昊', 38, '砖家') t.teach('Python程序设计') t.watch_av()if __name__ == '__main__': main() 子类在继承了父类的方法后，可以对父类已有的方法给出新的实现版本，这个动作称之为方法重写（override）。通过方法重写我们可以让父类的同一个行为在子类中拥有不同的实现版本，当我们调用这个经过子类重写的方法时，不同的子类对象会表现出不同的行为，这个就是多态（poly-morphism）。 12345678910111213141516171819202122232425262728293031323334353637from abc import ABCMeta, abstractmethodclass Pet(object, metaclass=ABCMeta): """宠物""" def __init__(self, nickname): self._nickname = nickname @abstractmethod def make_voice(self): """发出声音""" passclass Dog(Pet): """狗""" def make_voice(self): print('%s: 汪汪汪...' % self._nickname)class Cat(Pet): """猫""" def make_voice(self): print('%s: 喵...喵...' % self._nickname)def main(): pets = [Dog('旺财'), Cat('凯蒂'), Dog('大黄')] for pet in pets: pet.make_voice()if __name__ == '__main__': main() 在上面的代码中，我们将Pet类处理成了一个抽象类，所谓抽象类就是不能够创建对象的类，这种类的存在就是专门为了让其他类去继承它。Python从语法层面并没有像Java或C#那样提供对抽象类的支持，但是我们可以通过abc模块的ABCMeta元类和abstractmethod包装器来达到抽象类的效果，如果一个类中存在抽象方法那么这个类就不能够实例化（创建对象）。上面的代码中，Dog和Cat两个子类分别对Pet类中的make_voice抽象方法进行了重写并给出了不同的实现版本，当我们在main函数中调用该方法时，这个方法就表现出了多态行为（同样的方法做了不同的事情）。 综合案例案例1：奥特曼打小怪兽123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185from abc import ABCMeta, abstractmethodfrom random import randint, randrangeclass Fighter(object, metaclass=ABCMeta): """战斗者""" # 通过__slots__魔法限定对象可以绑定的成员变量 __slots__ = ('_name', '_hp') def __init__(self, name, hp): """初始化方法 :param name: 名字 :param hp: 生命值 """ self._name = name self._hp = hp @property def name(self): return self._name @property def hp(self): return self._hp @hp.setter def hp(self, hp): self._hp = hp if hp &gt;= 0 else 0 @property def alive(self): return self._hp &gt; 0 @abstractmethod def attack(self, other): """攻击 :param other: 被攻击的对象 """ passclass Ultraman(Fighter): """奥特曼""" __slots__ = ('_name', '_hp', '_mp') def __init__(self, name, hp, mp): """初始化方法 :param name: 名字 :param hp: 生命值 :param mp: 魔法值 """ super().__init__(name, hp) self._mp = mp def attack(self, other): other.hp -= randint(15, 25) def huge_attack(self, other): """究极必杀技(打掉对方至少50点或四分之三的血) :param other: 被攻击的对象 :return: 使用成功返回True否则返回False """ if self._mp &gt;= 50: self._mp -= 50 injury = other.hp * 3 // 4 injury = injury if injury &gt;= 50 else 50 other.hp -= injury return True else: self.attack(other) return False def magic_attack(self, others): """魔法攻击 :param others: 被攻击的群体 :return: 使用魔法成功返回True否则返回False """ if self._mp &gt;= 20: self._mp -= 20 for temp in others: if temp.alive: temp.hp -= randint(10, 15) return True else: return False def resume(self): """恢复魔法值""" incr_point = randint(1, 10) self._mp += incr_point return incr_point def __str__(self): return '~~~%s奥特曼~~~\n' % self._name + \ '生命值: %d\n' % self._hp + \ '魔法值: %d\n' % self._mpclass Monster(Fighter): """小怪兽""" __slots__ = ('_name', '_hp') def attack(self, other): other.hp -= randint(10, 20) def __str__(self): return '~~~%s小怪兽~~~\n' % self._name + \ '生命值: %d\n' % self._hpdef is_any_alive(monsters): """判断有没有小怪兽是活着的""" for monster in monsters: if monster.alive &gt; 0: return True return Falsedef select_alive_one(monsters): """选中一只活着的小怪兽""" monsters_len = len(monsters) while True: index = randrange(monsters_len) monster = monsters[index] if monster.alive &gt; 0: return monsterdef display_info(ultraman, monsters): """显示奥特曼和小怪兽的信息""" print(ultraman) for monster in monsters: print(monster, end='')def main(): u = Ultraman('骆昊', 1000, 120) m1 = Monster('狄仁杰', 250) m2 = Monster('白元芳', 500) m3 = Monster('王大锤', 750) ms = [m1, m2, m3] fight_round = 1 while u.alive and is_any_alive(ms): print('========第%02d回合========' % fight_round) m = select_alive_one(ms) # 选中一只小怪兽 skill = randint(1, 10) # 通过随机数选择使用哪种技能 if skill &lt;= 6: # 60%的概率使用普通攻击 print('%s使用普通攻击打了%s.' % (u.name, m.name)) u.attack(m) print('%s的魔法值恢复了%d点.' % (u.name, u.resume())) elif skill &lt;= 9: # 30%的概率使用魔法攻击(可能因魔法值不足而失败) if u.magic_attack(ms): print('%s使用了魔法攻击.' % u.name) else: print('%s使用魔法失败.' % u.name) else: # 10%的概率使用究极必杀技(如果魔法值不足则使用普通攻击) if u.huge_attack(m): print('%s使用究极必杀技虐了%s.' % (u.name, m.name)) else: print('%s使用普通攻击打了%s.' % (u.name, m.name)) print('%s的魔法值恢复了%d点.' % (u.name, u.resume())) if m.alive &gt; 0: # 如果选中的小怪兽没有死就回击奥特曼 print('%s回击了%s.' % (m.name, u.name)) m.attack(u) display_info(u, ms) # 每个回合结束后显示奥特曼和小怪兽的信息 fight_round += 1 print('\n========战斗结束!========\n') if u.alive &gt; 0: print('%s奥特曼胜利!' % u.name) else: print('小怪兽胜利!')if __name__ == '__main__': main() 案例2：扑克游戏123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110import randomclass Card(object): """一张牌""" def __init__(self, suite, face): self._suite = suite self._face = face @property def face(self): return self._face @property def suite(self): return self._suite def __str__(self): if self._face == 1: face_str = 'A' elif self._face == 11: face_str = 'J' elif self._face == 12: face_str = 'Q' elif self._face == 13: face_str = 'K' else: face_str = str(self._face) return '%s%s' % (self._suite, face_str) def __repr__(self): return self.__str__()class Poker(object): """一副牌""" def __init__(self): self._cards = [Card(suite, face) for suite in '♠♥♣♦' for face in range(1, 14)] self._current = 0 @property def cards(self): return self._cards def shuffle(self): """洗牌(随机乱序)""" self._current = 0 random.shuffle(self._cards) @property def next(self): """发牌""" card = self._cards[self._current] self._current += 1 return card @property def has_next(self): """还有没有牌""" return self._current &lt; len(self._cards)class Player(object): """玩家""" def __init__(self, name): self._name = name self._cards_on_hand = [] @property def name(self): return self._name @property def cards_on_hand(self): return self._cards_on_hand def get(self, card): """摸牌""" self._cards_on_hand.append(card) def arrange(self, card_key): """玩家整理手上的牌""" self._cards_on_hand.sort(key=card_key)# 排序规则-先根据花色再根据点数排序def get_key(card): return (card.suite, card.face)def main(): p = Poker() p.shuffle() players = [Player('东邪'), Player('西毒'), Player('南帝'), Player('北丐')] for _ in range(13): for player in players: player.get(p.next) for player in players: print(player.name + ':', end=' ') player.arrange(get_key) print(player.cards_on_hand)if __name__ == '__main__': main() 说明： 大家可以自己尝试在上面代码的基础上写一个简单的扑克游戏，例如21点(Black Jack)，游戏的规则可以自己在网上找一找。 案例3：工资结算系统123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899"""某公司有三种类型的员工 分别是部门经理、程序员和销售员需要设计一个工资结算系统 根据提供的员工信息来计算月薪部门经理的月薪是每月固定15000元程序员的月薪按本月工作时间计算 每小时150元销售员的月薪是1200元的底薪加上销售额5%的提成"""from abc import ABCMeta, abstractmethodclass Employee(object, metaclass=ABCMeta): """员工""" def __init__(self, name): """ 初始化方法 :param name: 姓名 """ self._name = name @property def name(self): return self._name @abstractmethod def get_salary(self): """ 获得月薪 :return: 月薪 """ passclass Manager(Employee): """部门经理""" def get_salary(self): return 15000.0class Programmer(Employee): """程序员""" def __init__(self, name, working_hour=0): super().__init__(name) self._working_hour = working_hour @property def working_hour(self): return self._working_hour @working_hour.setter def working_hour(self, working_hour): self._working_hour = working_hour if working_hour &gt; 0 else 0 def get_salary(self): return 150.0 * self._working_hourclass Salesman(Employee): """销售员""" def __init__(self, name, sales=0): super().__init__(name) self._sales = sales @property def sales(self): return self._sales @sales.setter def sales(self, sales): self._sales = sales if sales &gt; 0 else 0 def get_salary(self): return 1200.0 + self._sales * 0.05def main(): emps = [ Manager('刘备'), Programmer('诸葛亮'), Manager('曹操'), Salesman('荀彧'), Salesman('吕布'), Programmer('张辽'), Programmer('赵云') ] for emp in emps: if isinstance(emp, Programmer): emp.working_hour = int(input('请输入%s本月工作时间: ' % emp.name)) elif isinstance(emp, Salesman): emp.sales = float(input('请输入%s本月销售额: ' % emp.name)) # 同样是接收get_salary这个消息但是不同的员工表现出了不同的行为(多态) print('%s本月工资为: ￥%s元' % (emp.name, emp.get_salary()))if __name__ == '__main__': main()]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[08.面向对象编程基础]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-08-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[面向对象编程基础活在当下的程序员应该都听过“面向对象编程”一词，也经常有人问能不能用一句话解释下什么是“面向对象编程”，我们先来看看比较正式的说法。 “把一组数据结构和处理它们的方法组成对象（object），把相同行为的对象归纳为类（class），通过类的封装（encapsulation）隐藏内部细节，通过继承（inheritance）实现类的特化（specialization）和泛化（generalization），通过多态（polymorphism）实现基于对象类型的动态分派。” 这样一说是不是更不明白了。所以我们还是看看更通俗易懂的说法，下面这段内容来自于知乎。 说明： 以上的内容来自于网络，不代表作者本人的观点和看法，与作者本人立场无关，相关责任不由作者承担。 之前我们说过“程序是指令的集合”，我们在程序中书写的语句在执行时会变成一条或多条指令然后由CPU去执行。当然为了简化程序的设计，我们引入了函数的概念，把相对独立且经常重复使用的代码放置到函数中，在需要使用这些功能的时候只要调用函数即可；如果一个函数的功能过于复杂和臃肿，我们又可以进一步将函数继续切分为子函数来降低系统的复杂性。但是说了这么多，不知道大家是否发现，所谓编程就是程序员按照计算机的工作方式控制计算机完成各种任务。但是，计算机的工作方式与正常人类的思维模式是不同的，如果编程就必须得抛弃人类正常的思维方式去迎合计算机，编程的乐趣就少了很多，“每个人都应该学习编程”这样的豪言壮语就只能说说而已。当然，这些还不是最重要的，最重要的是当我们需要开发一个复杂的系统时，代码的复杂性会让开发和维护工作都变得举步维艰，所以在上世纪60年代末期，“软件危机”、“软件工程”等一系列的概念开始在行业中出现。 当然，程序员圈子内的人都知道，现实中并没有解决上面所说的这些问题的“银弹”，真正让软件开发者看到希望的是上世纪70年代诞生的Smalltalk编程语言中引入的面向对象的编程思想（面向对象编程的雏形可以追溯到更早期的Simula语言）。按照这种编程理念，程序中的数据和操作数据的函数是一个逻辑上的整体，我们称之为“对象”，而我们解决问题的方式就是创建出需要的对象并向对象发出各种各样的消息，多个对象的协同工作最终可以让我们构造出复杂的系统来解决现实中的问题。 说明： 当然面向对象也不是解决软件开发中所有问题的最后的“银弹”，所以今天的高级程序设计语言几乎都提供了对多种编程范式的支持，Python也不例外。 类和对象简单的说，类是对象的蓝图和模板，而对象是类的实例。这个解释虽然有点像用概念在解释概念，但是从这句话我们至少可以看出，类是抽象的概念，而对象是具体的东西。在面向对象编程的世界中，一切皆为对象，对象都有属性和行为，每个对象都是独一无二的，而且对象一定属于某个类（型）。当我们把一大堆拥有共同特征的对象的静态特征（属性）和动态特征（行为）都抽取出来后，就可以定义出一个叫做“类”的东西。 定义类在Python中可以使用class关键字定义类，然后在类中通过之前学习过的函数来定义方法，这样就可以将对象的动态特征描述出来，代码如下所示。 123456789101112131415161718class Student(object): # __init__是一个特殊方法用于在创建对象时进行初始化操作 # 通过这个方法我们可以为学生对象绑定name和age两个属性 def __init__(self, name, age): self.name = name self.age = age def study(self, course_name): print('%s正在学习%s.' % (self.name, course_name)) # PEP 8要求标识符的名字用全小写多个单词用下划线连接 # 但是部分程序员和公司更倾向于使用驼峰命名法(驼峰标识) def watch_movie(self): if self.age &lt; 18: print('%s只能观看《熊出没》.' % self.name) else: print('%s正在观看岛国爱情大电影.' % self.name) 说明： 写在类中的函数，我们通常称之为（对象的）方法，这些方法就是对象可以接收的消息。 创建和使用对象当我们定义好一个类之后，可以通过下面的方式来创建对象并给对象发消息。 1234567891011121314def main(): # 创建学生对象并指定姓名和年龄 stu1 = Student('骆昊', 38) # 给对象发study消息 stu1.study('Python程序设计') # 给对象发watch_av消息 stu1.watch_movie() stu2 = Student('王大锤', 15) stu2.study('思想品德') stu2.watch_movie()if __name__ == '__main__': main() 访问可见性问题对于上面的代码，有C++、Java、C#等编程经验的程序员可能会问，我们给Student对象绑定的name和age属性到底具有怎样的访问权限（也称为可见性）。因为在很多面向对象编程语言中，我们通常会将对象的属性设置为私有的（private）或受保护的（protected），简单的说就是不允许外界访问，而对象的方法通常都是公开的（public），因为公开的方法就是对象能够接受的消息。在Python中，属性和方法的访问权限只有两种，也就是公开的和私有的，如果希望属性是私有的，在给属性命名时可以用两个下划线作为开头，下面的代码可以验证这一点。 1234567891011121314151617181920class Test: def __init__(self, foo): self.__foo = foo def __bar(self): print(self.__foo) print('__bar')def main(): test = Test('hello') # AttributeError: 'Test' object has no attribute '__bar' test.__bar() # AttributeError: 'Test' object has no attribute '__foo' print(test.__foo)if __name__ == "__main__": main() 但是，Python并没有从语法上严格保证私有属性或方法的私密性，它只是给私有的属性和方法换了一个名字来“妨碍”对它们的访问，事实上如果你知道更换名字的规则仍然可以访问到它们，下面的代码就可以验证这一点。之所以这样设定，可以用这样一句名言加以解释，就是“We are all consenting adults here”。因为绝大多数程序员都认为开放比封闭要好，而且程序员要自己为自己的行为负责。 123456789101112131415161718class Test: def __init__(self, foo): self.__foo = foo def __bar(self): print(self.__foo) print('__bar')def main(): test = Test('hello') test._Test__bar() print(test._Test__foo)if __name__ == "__main__": main() 在实际开发中，我们并不建议将属性设置为私有的，因为这会导致子类无法访问（后面会讲到）。所以大多数Python程序员会遵循一种命名惯例就是让属性名以单下划线开头来表示属性是受保护的，本类之外的代码在访问这样的属性时应该要保持慎重。这种做法并不是语法上的规则，单下划线开头的属性和方法外界仍然是可以访问的，所以更多的时候它是一种暗示或隐喻，关于这一点可以看看我的《Python - 那些年我们踩过的那些坑》文章中的讲解。 面向对象的支柱面向对象有三大支柱：封装、继承和多态。后面两个概念在下一个章节中进行详细的说明，这里我们先说一下什么是封装。我自己对封装的理解是“隐藏一切可以隐藏的实现细节，只向外界暴露（提供）简单的编程接口”。我们在类中定义的方法其实就是把数据和对数据的操作封装起来了，在我们创建了对象之后，只需要给对象发送一个消息（调用方法）就可以执行方法中的代码，也就是说我们只需要知道方法的名字和传入的参数（方法的外部视图），而不需要知道方法内部的实现细节（方法的内部视图）。 练习练习1：定义一个类描述数字时钟123456789101112131415161718192021222324252627282930313233343536373839404142434445from time import sleepclass Clock(object): """数字时钟""" def __init__(self, hour=0, minute=0, second=0): """初始化方法 :param hour: 时 :param minute: 分 :param second: 秒 """ self._hour = hour self._minute = minute self._second = second def run(self): """走字""" self._second += 1 if self._second == 60: self._second = 0 self._minute += 1 if self._minute == 60: self._minute = 0 self._hour += 1 if self._hour == 24: self._hour = 0 def show(self): """显示时间""" return '%02d:%02d:%02d' % \ (self._hour, self._minute, self._second)def main(): clock = Clock(23, 59, 58) while True: print(clock.show()) sleep(1) clock.run()if __name__ == '__main__': main() 练习2：定义一个类描述平面上的点并提供移动点和计算到另一个点距离的方法。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from math import sqrtclass Point(object): def __init__(self, x=0, y=0): """初始化方法 :param x: 横坐标 :param y: 纵坐标 """ self.x = x self.y = y def move_to(self, x, y): """移动到指定位置 :param x: 新的横坐标 "param y: 新的纵坐标 """ self.x = x self.y = y def move_by(self, dx, dy): """移动指定的增量 :param dx: 横坐标的增量 "param dy: 纵坐标的增量 """ self.x += dx self.y += dy def distance_to(self, other): """计算与另一个点的距离 :param other: 另一个点 """ dx = self.x - other.x dy = self.y - other.y return sqrt(dx ** 2 + dy ** 2) def __str__(self): return '(%s, %s)' % (str(self.x), str(self.y))def main(): p1 = Point(3, 5) p2 = Point() print(p1) print(p2) p2.move_by(-1, 2) print(p2) print(p1.distance_to(p2))if __name__ == '__main__': main() 说明： 本章中的插图来自于Grady Booch等著作的《面向对象分析与设计》一书，该书是讲解面向对象编程的经典著作，有兴趣的读者可以购买和阅读这本书来了解更多的面向对象的相关知识。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[07.字符串和常用数据结构]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-07-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[字符串和常用数据结构使用字符串第二次世界大战促使了现代电子计算机的诞生，最初的目的用计算机来快速的完成导弹弹道的计算，因此在计算机刚刚诞生的那个年代，计算机处理的信息基本上都是数值型的信息，而世界上的第一台电子计算机ENIAC每秒钟能够完成约5000次浮点运算。随着时间的推移，虽然数值运算仍然是计算机日常工作中最为重要的事情之一，但是今天的计算机更多的时间需要处理的数据可能都是以文本的方式存在的，如果我们希望通过Python程序操作本这些文本信息，就必须要先了解字符串类型以及与它相关的知识。 所谓字符串，就是由零个或多个字符组成的有限序列，一般记为。 我们可以通过下面的代码来了解字符串的使用。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def main(): str1 = 'hello, world!' # 通过len函数计算字符串的长度 print(len(str1)) # 13 # 获得字符串首字母大写的拷贝 print(str1.capitalize()) # Hello, world! # 获得字符串变大写后的拷贝 print(str1.upper()) # HELLO, WORLD! # 从字符串中查找子串所在位置 print(str1.find('or')) # 8 print(str1.find('shit')) # -1 # 与find类似但找不到子串时会引发异常 # print(str1.index('or')) # print(str1.index('shit')) # 检查字符串是否以指定的字符串开头 print(str1.startswith('He')) # False print(str1.startswith('hel')) # True # 检查字符串是否以指定的字符串结尾 print(str1.endswith('!')) # True # 将字符串以指定的宽度居中并在两侧填充指定的字符 print(str1.center(50, '*')) # 将字符串以指定的宽度靠右放置左侧填充指定的字符 print(str1.rjust(50, ' ')) str2 = 'abc123456' # 从字符串中取出指定位置的字符(下标运算) print(str2[2]) # c # 字符串切片(从指定的开始索引到指定的结束索引) print(str2[2:5]) # c12 print(str2[2:]) # c123456 print(str2[2::2]) # c246 print(str2[::2]) # ac246 print(str2[::-1]) # 654321cba print(str2[-3:-1]) # 45 # 检查字符串是否由数字构成 print(str2.isdigit()) # False # 检查字符串是否以字母构成 print(str2.isalpha()) # False # 检查字符串是否以数字和字母构成 print(str2.isalnum()) # True str3 = ' jackfrued@126.com ' print(str3) # 获得字符串修剪左右两侧空格的拷贝 print(str3.strip())if __name__ == '__main__': main() 除了字符串，Python还内置了多种类型的数据结构，如果要在程序中保存和操作数据，绝大多数时候可以利用现有的数据结构来实现，最常用的包括列表、元组、集合和字典。 使用列表下面的代码演示了如何定义列表、使用下标访问列表元素以及添加和删除元素的操作。 12345678910111213141516171819202122232425262728293031323334def main(): list1 = [1, 3, 5, 7, 100] print(list1) list2 = ['hello'] * 5 print(list2) # 计算列表长度(元素个数) print(len(list1)) # 下标(索引)运算 print(list1[0]) print(list1[4]) # print(list1[5]) # IndexError: list index out of range print(list1[-1]) print(list1[-3]) list1[2] = 300 print(list1) # 添加元素 list1.append(200) list1.insert(1, 400) list1 += [1000, 2000] print(list1) print(len(list1)) # 删除元素 list1.remove(3) if 1234 in list1: list1.remove(1234) del list1[0] print(list1) # 清空列表元素 list1.clear() print(list1)if __name__ == '__main__': main() 和字符串一样，列表也可以做切片操作，通过切片操作我们可以实现对列表的复制或者将列表中的一部分取出来创建出新的列表，代码如下所示。 1234567891011121314151617181920212223def main(): fruits = ['grape', 'apple', 'strawberry', 'waxberry'] fruits += ['pitaya', 'pear', 'mango'] # 循环遍历列表元素 for fruit in fruits: print(fruit.title(), end=' ') print() # 列表切片 fruits2 = fruits[1:4] print(fruits2) # fruit3 = fruits # 没有复制列表只创建了新的引用 # 可以通过完整切片操作来复制列表 fruits3 = fruits[:] print(fruits3) fruits4 = fruits[-3:-1] print(fruits4) # 可以通过反向切片操作来获得倒转后的列表的拷贝 fruits5 = fruits[::-1] print(fruits5)if __name__ == '__main__': main() 下面的代码实现了对列表的排序操作。 12345678910111213141516171819def main(): list1 = ['orange', 'apple', 'zoo', 'internationalization', 'blueberry'] list2 = sorted(list1) # sorted函数返回列表排序后的拷贝不会修改传入的列表 # 函数的设计就应该像sorted函数一样尽可能不产生副作用 list3 = sorted(list1, reverse=True) # 通过key关键字参数指定根据字符串长度进行排序而不是默认的字母表顺序 list4 = sorted(list1, key=len) print(list1) print(list2) print(list3) print(list4) # 给列表对象发出排序消息直接在列表对象上进行排序 list1.sort(reverse=True) print(list1)if __name__ == '__main__': main() 我们还可以使用列表的生成式语法来创建列表，代码如下所示。 12345678910111213141516171819202122232425import sysdef main(): f = [x for x in range(1, 10)] print(f) f = [x + y for x in 'ABCDE' for y in '1234567'] print(f) # 用列表的生成表达式语法创建列表容器 # 用这种语法创建列表之后元素已经准备就绪所以需要耗费较多的内存空间 f = [x ** 2 for x in range(1, 1000)] print(sys.getsizeof(f)) # 查看对象占用内存的字节数 print(f) # 请注意下面的代码创建的不是一个列表而是一个生成器对象 # 通过生成器可以获取到数据但它不占用额外的空间存储数据 # 每次需要数据的时候就通过内部的运算得到数据(需要花费额外的时间) f = (x ** 2 for x in range(1, 1000)) print(sys.getsizeof(f)) # 相比生成式生成器不占用存储数据的空间 print(f) for val in f: print(val)if __name__ == '__main__': main() 除了上面提到的生成器语法，Python中还有另外一种定义生成器的方式，就是通过yield关键字将一个普通函数改造成生成器函数。下面的代码演示了如何实现一个生成斐波拉切数列的生成器。所谓斐波拉切数列可以通过下面递归的方法来进行定义： 1234567891011121314def fib(n): a, b = 0, 1 for _ in range(n): a, b = b, a + b yield adef main(): for val in fib(20): print(val)if __name__ == '__main__': main() 使用元组Python 的元组与列表类似，不同之处在于元组的元素不能修改，在前面的代码中我们已经不止一次使用过元组了。顾名思义，我们把多个元素组合到一起就形成了一个元组，所以它和列表一样可以保存多条数据。下面的代码演示了如何定义和使用元组。 123456789101112131415161718192021222324252627282930def main(): # 定义元组 t = ('骆昊', 38, True, '四川成都') print(t) # 获取元组中的元素 print(t[0]) print(t[3]) # 遍历元组中的值 for member in t: print(member) # 重新给元组赋值 # t[0] = '王大锤' # TypeError # 变量t重新引用了新的元组原来的元组将被垃圾回收 t = ('王大锤', 20, True, '云南昆明') print(t) # 将元组转换成列表 person = list(t) print(person) # 列表是可以修改它的元素的 person[0] = '李小龙' person[1] = 25 print(person) # 将列表转换成元组 fruits_list = ['apple', 'banana', 'orange'] fruits_tuple = tuple(fruits_list) print(fruits_tuple)if __name__ == '__main__': main() 这里有一个非常值得探讨的问题，我们已经有了列表这种数据结构，为什么还需要元组这样的类型呢？ 元组中的元素是无法修改的，事实上我们在项目中尤其是多线程环境（后面会讲到）中可能更喜欢使用的是那些不变对象（一方面因为对象状态不能修改，所以可以避免由此引起的不必要的程序错误，简单的说就是一个不变的对象要比可变的对象更加容易维护；另一方面因为没有任何一个线程能够修改不变对象的内部状态，一个不变对象自动就是线程安全的，这样就可以省掉处理同步化的开销。一个不变对象可以方便的被共享访问）。所以结论就是：如果不需要对元素进行添加、删除、修改的时候，可以考虑使用元组，当然如果一个方法要返回多个值，使用元组也是不错的选择。 元组在创建时间和占用的空间上面都优于列表。我们可以使用sys模块的getsizeof函数来检查存储同样的元素的元组和列表各自占用了多少内存空间，这个很容易做到。我们也可以在ipython中使用魔法指令%timeit来分析创建同样内容的元组和列表所花费的时间，下图是我的macOS系统上测试的结果。 使用集合Python中的集合跟数学上的集合是一致的，不允许有重复元素，而且可以进行交集、并集、差集等运算。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def main(): set1 = &#123;1, 2, 3, 3, 3, 2&#125; print(set1) print('Length =', len(set1)) set2 = set(range(1, 10)) print(set2) set1.add(4) set1.add(5) set2.update([11, 12]) print(set1) print(set2) set2.discard(5) # remove的元素如果不存在会引发KeyError if 4 in set2: set2.remove(4) print(set2) # 遍历集合容器 for elem in set2: print(elem ** 2, end=' ') print() # 将元组转换成集合 set3 = set((1, 2, 3, 3, 2, 1)) print(set3.pop()) print(set3) # 集合的交集、并集、差集、对称差运算 print(set1 &amp; set2) # print(set1.intersection(set2)) print(set1 | set2) # print(set1.union(set2)) print(set1 - set2) # print(set1.difference(set2)) print(set1 ^ set2) # print(set1.symmetric_difference(set2)) # 判断子集和超集 print(set2 &lt;= set1) # print(set2.issubset(set1)) print(set3 &lt;= set1) # print(set3.issubset(set1)) print(set1 &gt;= set2) # print(set1.issuperset(set2)) print(set1 &gt;= set3) # print(set1.issuperset(set3))if __name__ == '__main__': main() 说明： Python中允许通过一些特殊的方法来为某种类型或数据结构自定义运算符（后面的章节中会讲到），上面的代码中我们对集合进行运算的时候可以调用集合对象的方法，也可以直接使用对应的运算符，例如&amp;运算符跟intersection方法的作用就是一样的，但是使用运算符让代码更加直观。 使用字典字典是另一种可变容器模型，类似于我们生活中使用的字典，它可以存储任意类型对象，与列表、集合不同的是，字典的每个元素都是由一个键和一个值组成的“键值对”，键和值通过冒号分开。下面的代码演示了如何定义和使用字典。 1234567891011121314151617181920212223242526272829def main(): scores = &#123;'骆昊': 95, '白元芳': 78, '狄仁杰': 82&#125; # 通过键可以获取字典中对应的值 print(scores['骆昊']) print(scores['狄仁杰']) # 对字典进行遍历(遍历的其实是键再通过键取对应的值) for elem in scores: print('%s\t---&gt;\t%d' % (elem, scores[elem])) # 更新字典中的元素 scores['白元芳'] = 65 scores['诸葛王朗'] = 71 scores.update(冷面=67, 方启鹤=85) print(scores) if '武则天' in scores: print(scores['武则天']) print(scores.get('武则天')) # get方法也是通过键获取对应的值但是可以设置默认值 print(scores.get('武则天', 60)) # 删除字典中的元素 print(scores.popitem()) print(scores.popitem()) print(scores.pop('骆昊', 100)) # 清空字典 scores.clear() print(scores)if __name__ == '__main__': main() 练习练习1：在屏幕上显示跑马灯文字1234567891011121314151617import osimport timedef main(): content = '北京欢迎你为你开天辟地…………' while True: # 清理屏幕上的输出 os.system('cls') # os.system('clear') print(content) # 休眠200毫秒 time.sleep(0.2) content = content[1:] + content[0]if __name__ == '__main__': main() 练习2：设计一个函数产生指定长度的验证码，验证码由大小写字母和数字构成。123456789101112131415161718import randomdef generate_code(code_len=4): """ 生成指定长度的验证码 :param code_len: 验证码的长度(默认4个字符) :return: 由大小写英文字母和数字构成的随机验证码 """ all_chars = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ' last_pos = len(all_chars) - 1 code = '' for _ in range(code_len): index = random.randint(0, last_pos) code += all_chars[index] return code 练习3：设计一个函数返回给定文件名的后缀名。1234567891011121314def get_suffix(filename, has_dot=False): """ 获取文件名的后缀名 :param filename: 文件名 :param has_dot: 返回的后缀名是否需要带点 :return: 文件的后缀名 """ pos = filename.rfind('.') if 0 &lt; pos &lt; len(filename) - 1: index = pos if has_dot else pos + 1 return filename[index:] else: return '' 练习4：设计一个函数返回传入的列表中最大和第二大的元素的值。123456789def max2(x): m1, m2 = (x[0], x[1]) if x[0] &gt; x[1] else (x[1], x[0]) for index in range(2, len(x)): if x[index] &gt; m1: m2 = m1 m1 = x[index] elif x[index] &gt; m2: m2 = x[index] return m1, m2 练习5：计算指定的年月日是这一年的第几天1234567891011121314151617181920212223242526272829303132333435363738def is_leap_year(year): """ 判断指定的年份是不是闰年 :param year: 年份 :return: 闰年返回True平年返回False """ return year % 4 == 0 and year % 100 != 0 or year % 400 == 0def which_day(year, month, date): """ 计算传入的日期是这一年的第几天 :param year: 年 :param month: 月 :param date: 日 :return: 第几天 """ days_of_month = [ [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31], [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31] ][is_leap_year(year)] total = 0 for index in range(month - 1): total += days_of_month[index] return total + datedef main(): print(which_day(1980, 11, 28)) print(which_day(1981, 12, 31)) print(which_day(2018, 1, 1)) print(which_day(2016, 3, 1))if __name__ == '__main__': main() 练习6：打印杨辉三角。12345678910111213141516def main(): num = int(input('Number of rows: ')) yh = [[]] * num for row in range(len(yh)): yh[row] = [None] * (row + 1) for col in range(len(yh[row])): if col == 0 or col == row: yh[row][col] = 1 else: yh[row][col] = yh[row - 1][col] + yh[row - 1][col - 1] print(yh[row][col], end='\t') print()if __name__ == '__main__': main() 综合案例案例1：双色球选号12345678910111213141516171819202122232425262728293031323334from random import randrange, randint, sampledef display(balls): """ 输出列表中的双色球号码 """ for index, ball in enumerate(balls): if index == len(balls) - 1: print('|', end=' ') print('%02d' % ball, end=' ') print()def random_select(): """ 随机选择一组号码 """ red_balls = [x for x in range(1, 34)] selected_balls = [] selected_balls = sample(red_balls, 6) selected_balls.sort() selected_balls.append(randint(1, 16)) return selected_ballsdef main(): n = int(input('机选几注: ')) for _ in range(n): display(random_select())if __name__ == '__main__': main() 说明： 上面使用random模块的sample函数来实现从列表中选择不重复的n个元素。 综合案例2：约瑟夫环问题123456789101112131415161718192021222324"""《幸运的基督徒》有15个基督徒和15个非基督徒在海上遇险，为了能让一部分人活下来不得不将其中15个人扔到海里面去，有个人想了个办法就是大家围成一个圈，由某个人开始从1报数，报到9的人就扔到海里面，他后面的人接着从1开始报数，报到9的人继续扔到海里面，直到扔掉15个人。由于上帝的保佑，15个基督徒都幸免于难，问这些人最开始是怎么站的，哪些位置是基督徒哪些位置是非基督徒。"""def main(): persons = [True] * 30 counter, index, number = 0, 0, 0 while counter &lt; 15: if persons[index]: number += 1 if number == 9: persons[index] = False counter += 1 number = 0 index += 1 index %= 30 for person in persons: print('基' if person else '非', end='')if __name__ == '__main__': main() 综合案例3：井字棋游戏123456789101112131415161718192021222324252627282930313233343536373839404142import osdef print_board(board): print(board['TL'] + '|' + board['TM'] + '|' + board['TR']) print('-+-+-') print(board['ML'] + '|' + board['MM'] + '|' + board['MR']) print('-+-+-') print(board['BL'] + '|' + board['BM'] + '|' + board['BR'])def main(): init_board = &#123; 'TL': ' ', 'TM': ' ', 'TR': ' ', 'ML': ' ', 'MM': ' ', 'MR': ' ', 'BL': ' ', 'BM': ' ', 'BR': ' ' &#125; begin = True while begin: curr_board = init_board.copy() begin = False turn = 'x' counter = 0 os.system('clear') print_board(curr_board) while counter &lt; 9: move = input('轮到%s走棋, 请输入位置: ' % turn) if curr_board[move] == ' ': counter += 1 curr_board[move] = turn if turn == 'x': turn = 'o' else: turn = 'x' os.system('clear') print_board(curr_board) choice = input('再玩一局?(yes|no)') begin = choice == 'yes'if __name__ == '__main__': main() 说明： 最后这个案例来自《Python编程快速上手:让繁琐工作自动化》一书（这本书对有编程基础想迅速使用Python将日常工作自动化的人来说还是不错的选择），对代码做了一点点的调整。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[06.函数和模块的使用]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-06-%E5%87%BD%E6%95%B0%E5%92%8C%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[函数和模块的使用在讲解本章节的内容之前，我们先来研究一道数学题，请说出下面的方程有多少组正整数解。 事实上，上面的问题等同于将8个苹果分成四组每组至少一个苹果有多少种方案。想到这一点问题的答案就呼之欲出了。 可以用Python的程序来计算出这个值，代码如下所示。 12345678910111213141516"""输入M和N计算C(M,N)"""m = int(input('m = '))n = int(input('n = '))fm = 1for num in range(1, m + 1): fm *= numfn = 1for num in range(1, n + 1): fn *= numfmn = 1for num in range(1, m - n + 1): fmn *= numprint(fm // fn // fmn) 函数的作用不知道大家是否注意到，在上面的代码中，我们做了3次求阶乘，这样的代码实际上就是重复代码。编程大师Martin Fowler先生曾经说过：“代码有很多种坏味道，重复是最坏的一种！”，要写出高质量的代码首先要解决的就是重复代码的问题。对于上面的代码来说，我们可以将计算阶乘的功能封装到一个称之为“函数”的功能模块中，在需要计算阶乘的地方，我们只需要“调用”这个“函数”就可以了。 定义函数在Python中可以使用def关键字来定义函数，和变量一样每个函数也有一个响亮的名字，而且命名规则跟变量的命名规则是一致的。在函数名后面的圆括号中可以放置传递给函数的参数，这一点和数学上的函数非常相似，程序中函数的参数就相当于是数学上说的函数的自变量，而函数执行完成后我们可以通过return关键字来返回一个值，这相当于数学上说的函数的因变量。 在了解了如何定义函数后，我们可以对上面的代码进行重构，所谓重构就是在不影响代码执行结果的前提下对代码的结构进行调整，重构之后的代码如下所示。 1234567891011121314151617def factorial(num): """ 求阶乘 :param num: 非负整数 :return: num的阶乘 """ result = 1 for n in range(1, num + 1): result *= n return resultm = int(input('m = '))n = int(input('n = '))# 当需要计算阶乘的时候不用再写循环求阶乘而是直接调用已经定义好的函数print(factorial(m) // factorial(n) // factorial(m - n)) 说明： Python的math模块中其实已经有一个factorial函数了，事实上要计算阶乘可以直接使用这个现成的函数而不用自己定义。下面例子中的某些函数其实Python中也是内置了，我们这里是为了讲解函数的定义和使用才把它们又实现了一遍，实际开发中不建议做这种低级的重复性的工作。 函数的参数函数是绝大多数编程语言中都支持的一个代码的“构建块”，但是Python中的函数与其他语言中的函数还是有很多不太相同的地方，其中一个显著的区别就是Python对函数参数的处理。在Python中，函数的参数可以有默认值，也支持使用可变参数，所以Python并不需要像其他语言一样支持函数的重载，因为我们在定义一个函数的时候可以让它有多种不同的使用方式，下面是两个小例子。 123456789101112131415161718192021222324252627282930from random import randintdef roll_dice(n=2): """ 摇色子 :param n: 色子的个数 :return: n颗色子点数之和 """ total = 0 for _ in range(n): total += randint(1, 6) return totaldef add(a=0, b=0, c=0): return a + b + c# 如果没有指定参数那么使用默认值摇两颗色子print(roll_dice())# 摇三颗色子print(roll_dice(3))print(add())print(add(1))print(add(1, 2))print(add(1, 2, 3))# 传递参数时可以不按照设定的顺序进行传递print(add(c=50, a=100, b=200)) 我们给上面两个函数的参数都设定了默认值，这也就意味着如果在调用函数的时候如果没有传入对应参数的值时将使用该参数的默认值，所以在上面的代码中我们可以用各种不同的方式去调用add函数，这跟其他很多语言中函数重载的效果是一致的。 其实上面的add函数还有更好的实现方案，因为我们可能会对0个或多个参数进行加法运算，而具体有多少个参数是由调用者来决定，我们作为函数的设计者对这一点是一无所知的，因此在不确定参数个数的时候，我们可以使用可变参数，代码如下所示。 1234567891011121314# 在参数名前面的*表示args是一个可变参数# 即在调用add函数时可以传入0个或多个参数def add(*args): total = 0 for val in args: total += val return totalprint(add())print(add(1))print(add(1, 2))print(add(1, 2, 3))print(add(1, 3, 5, 7, 9)) 用模块管理函数对于任何一种编程语言来说，给变量、函数这样的标识符起名字都是一个让人头疼的问题，因为我们会遇到命名冲突这种尴尬的情况。最简单的场景就是在同一个.py文件中定义了两个同名函数，由于Python没有函数重载的概念，那么后面的定义会覆盖之前的定义，也就意味着两个函数同名函数实际上只有一个是存在的。 12345678910def foo(): print('hello, world!')def foo(): print('goodbye, world!')# 下面的代码会输出什么呢？foo() 当然上面的这种情况我们很容易就能避免，但是如果项目是由多人协作进行团队开发的时候，团队中可能有多个程序员都定义了名为foo的函数，那么怎么解决这种命名冲突呢？答案其实很简单，Python中每个文件就代表了一个模块（module），我们在不同的模块中可以有同名的函数，在使用函数的时候我们通过import关键字导入指定的模块就可以区分到底要使用的是哪个模块中的foo函数，代码如下所示。 module1.py 12def foo(): print('hello, world!') module2.py 12def foo(): print('goodbye, world!') test.py 123456789from module1 import foo# 输出hello, world!foo()from module2 import foo# 输出goodbye, world!foo() 也可以按照如下所示的方式来区分到底要使用哪一个foo函数。 test.py 12345import module1 as m1import module2 as m2m1.foo()m2.foo() 但是如果将代码写成了下面的样子，那么程序中调用的是最后导入的那个foo，因为后导入的foo覆盖了之前导入的foo。 test.py 12345from module1 import foofrom module2 import foo# 输出goodbye, world!foo() test.py 12345from module2 import foofrom module1 import foo# 输出hello, world!foo() 需要说明的是，如果我们导入的模块除了定义函数之外还中有可以执行代码，那么Python解释器在导入这个模块时就会执行这些代码，事实上我们可能并不希望如此，因此如果我们在模块中编写了执行代码，最好是将这些执行代码放入如下所示的条件中，这样的话除非直接运行该模块，if条件下的这些代码是不会执行的，因为只有直接执行的模块的名字才是“__main__”。 module3.py 123456789101112131415def foo(): passdef bar(): pass# __name__是Python中一个隐含的变量它代表了模块的名字# 只有被Python解释器直接执行的模块的名字才是__main__if __name__ == '__main__': print('call foo()') foo() print('call bar()') bar() test.py 123import module3# 导入module3时 不会执行模块中if条件成立时的代码 因为模块的名字是module3而不是__main__ 练习练习1：实现计算求最大公约数和最小公倍数的函数。123456789def gcd(x, y): (x, y) = (y, x) if x &gt; y else (x, y) for factor in range(x, 0, -1): if x % factor == 0 and y % factor == 0: return factordef lcm(x, y): return x * y // gcd(x, y) 练习2：实现判断一个数是不是回文数的函数。1234567def is_palindrome(num): temp = num total = 0 while temp &gt; 0: total = total * 10 + temp % 10 temp //= 10 return total == num 练习3：实现判断一个数是不是素数的函数。12345def is_prime(num): for factor in range(2, num): if num % factor == 0: return False return True if num != 1 else False 练习4：写一个程序判断输入的正整数是不是回文素数。1234if __name__ == '__main__': num = int(input('请输入正整数: ')) if is_palindrome(num) and is_prime(num): print('%d是回文素数' % num) 通过上面的程序可以看出，当我们将代码中重复出现的和相对独立的功能抽取成函数后，我们可以组合使用这些函数来解决更为复杂的问题，这也是我们为什么要定义和使用函数的一个非常重要的原因。 最后，我们来讨论一下Python中有关变量作用域的问题。 1234567891011121314151617def foo(): b = 'hello' def bar(): # Python中可以在函数内部再定义函数 c = True print(a) print(b) print(c) bar() # print(c) # NameError: name 'c' is not definedif __name__ == '__main__': a = 100 # print(b) # NameError: name 'b' is not defined foo() 上面的代码能够顺利的执行并且打印出100和“hello”，但我们注意到了，在bar函数的内部并没有定义a和b两个变量，那么a和b是从哪里来的。我们在上面代码的if分支中定义了一个变量a，这是一个全局变量（global variable），属于全局作用域，因为它没有定义在任何一个函数中。在上面的foo函数中我们定义了变量b，这是一个定义在函数中的局部变量（local variable），属于局部作用域，在foo函数的外部并不能访问到它；但对于foo函数内部的bar函数来说，变量b属于嵌套作用域，在bar函数中我们是可以访问到它的。bar函数中的变量c属于局部作用域，在bar函数之外是无法访问的。事实上，Python查找一个变量时会按照“局部作用域”、“嵌套作用域”、“全局作用域”和“内置作用域”的顺序进行搜索，前三者我们在上面的代码中已经看到了，所谓的“内置作用域”就是Python内置的那些隐含标识符min、len等都属于内置作用域）。 再看看下面这段代码，我们希望通过函数调用修改全局变量a的值，但实际上下面的代码是做不到的。 123456789def foo(): a = 200 print(a) # 200if __name__ == '__main__': a = 100 foo() print(a) # 100 在调用foo函数后，我们发现a的值仍然是100，这是因为当我们在函数foo中写a = 200的时候，是重新定义了一个名字为a的局部变量，它跟全局作用域的a并不是同一个变量，因为局部作用域中有了自己的变量a，因此foo函数不再搜索全局作用域中的a。如果我们希望在foo函数中修改全局作用域中的a，代码如下所示。 12345678910def foo(): global a a = 200 print(a) # 200if __name__ == '__main__': a = 100 foo() print(a) # 200 我们可以使用global关键字来指示foo函数中的变量a来自于全局作用域，如果全局作用域中没有a，那么下面一行的代码就会定义变量a并将其置于全局作用域。同理，如果我们希望函数内部的函数能够修改嵌套作用域中的变量，可以使用nonlocal关键字来指示变量来自于嵌套作用域，请大家自行试验。 在实际开发中，我们应该尽量减少对全局变量的使用，因为全局变量的作用域和影响过于广泛，可能会发生意料之外的修改和使用，除此之外全局变量比局部变量拥有更长的生命周期，可能导致对象占用的内存长时间无法被垃圾回收。事实上，减少对全局变量的使用，也是降低代码之间耦合度的一个重要举措，同时也是对迪米特法则的践行。减少全局变量的使用就意味着我们应该尽量让变量的作用域在函数的内部，但是如果我们希望将一个局部变量的生命周期延长，使其在函数调用结束后依然可以访问，这时候就需要使用闭包，这个我们在后续的内容中进行讲解。 说明： 很多人经常会将“闭包”一词和“匿名函数”混为一谈，但实际上它们是不同的概念，如果想提前了解这个概念，推荐看看维基百科或者知乎上对这个概念的讨论。 说了那么多，其实结论很简单，从现在开始我们可以将Python代码按照下面的格式进行书写，这一点点的改进其实就是在我们理解了函数和作用域的基础上跨出的巨大的一步。 1234567def main(): # Todo: Add your code here passif __name__ == '__main__': main()]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05.构造程序逻辑]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-05-%E6%9E%84%E9%80%A0%E7%A8%8B%E5%BA%8F%E9%80%BB%E8%BE%91%2F</url>
    <content type="text"><![CDATA[构造程序逻辑分支和循环结构会帮助我们将程序中逻辑建立起来，将来我们的程序无论简单复杂，都是由顺序结构、分支结构、循环结构构成的。对于编程语言的初学者来说，首先要锻炼的是将人类自然语言描述的解决问题的步骤和方法翻译成代码的能力，其次就是熟练的运用之前学过的运算符、表达式以及最近的两个章节讲解的分支结构和循环结构的知识。有了这些基本的能力才能够通过计算机程序去解决各种各样的现实问题。所以，开始做练习吧！ 练习清单 寻找“水仙花数”。 寻找“完美数”。 “百钱百鸡”问题。 生成“斐波拉切数列”。 Craps赌博游戏。 提示：练习的参考答案在code/Day05目录下。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04.循环结构]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-04-%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[循环结构应用场景如果在程序中我们需要重复的执行某条或某些指令，例如用程序控制机器人踢足球，如果机器人持球而且还没有进入射门范围，那么我们就要一直发出让机器人向球门方向奔跑的指令。当然你可能已经注意到了，刚才的描述中其实不仅仅有需要重复的动作，还有我们上一个章节讲到的分支结构。再举一个简单的例子，比如在我们的程序中要实现每隔1秒中在屏幕上打印一个&quot;hello, world&quot;这样的字符串并持续一个小时，我们肯定不能够将print(&#39;hello, world&#39;)这句代码写上3600遍，如果真的需要这样做，那么编程的工作就太无聊了。因此，我们还需要了解一下循环结构，有了循环结构我们就可以轻松的控制某件事或者某些事重复、重复、再重复的去执行。 在Python中构造循环结构有两种做法，一种是for-in循环，一种是while循环。 for-in循环如果明确的知道循环执行的次数或者要对一个容器进行迭代（后面会讲到），那么我们推荐使用for-in循环，例如下面代码中计算1~100求和的结果（$\displaystyle \sum \limits_{n=1}^{100}n$）。 1234567891011"""用for循环实现1~100求和Version: 0.1Author: 骆昊"""sum = 0for x in range(101): sum += xprint(sum) 需要说明的是上面代码中的range类型，range可以用来产生一个不变的数值序列，而且这个序列通常都是用在循环中的，例如： range(101)可以产生一个0到100的整数序列。 range(1, 100)可以产生一个1到99的整数序列。 range(1, 100, 2)可以产生一个1到99的奇数序列，其中的2是步长，即数值序列的增量。 知道了这一点，我们可以用下面的代码来实现1~100之间的偶数求和。 1234567891011"""用for循环实现1~100之间的偶数求和Version: 0.1Author: 骆昊"""sum = 0for x in range(2, 101, 2): sum += xprint(sum) 也可以通过在循环中使用分支结构的方式来实现相同的功能，代码如下所示。 123456789101112"""用for循环实现1~100之间的偶数求和Version: 0.1Author: 骆昊"""sum = 0for x in range(1, 101): if x % 2 == 0: sum += xprint(sum) while循环如果要构造不知道具体循环次数的循环结构，我们推荐使用while循环。while循环通过一个能够产生或转换出bool值的表达式来控制循环，表达式的值为True循环继续，表达式的值为False循环结束。下面我们通过一个“猜数字”的小游戏（计算机出一个1~100之间的随机数，人输入自己猜的数字，计算机给出对应的提示信息，直到人猜出计算机出的数字）来看看如何使用while循环。 1234567891011121314151617181920212223242526"""猜数字游戏计算机出一个1~100之间的随机数由人来猜计算机根据人猜的数字分别给出提示大一点/小一点/猜对了Version: 0.1Author: 骆昊"""import randomanswer = random.randint(1, 100)counter = 0while True: counter += 1 number = int(input('请输入: ')) if number &lt; answer: print('大一点') elif number &gt; answer: print('小一点') else: print('恭喜你猜对了!') breakprint('你总共猜了%d次' % counter)if counter &gt; 7: print('你的智商余额明显不足') 说明： 上面的代码中使用了break关键字来提前终止循环，需要注意的是break只能终止它所在的那个循环，这一点在使用嵌套的循环结构（下面会讲到）需要引起注意。除了break之外，还有另一个关键字是continue，它可以用来放弃本次循环后续的代码直接让循环进入下一轮。 和分支结构一样，循环结构也是可以嵌套的，也就是说在循环中还可以构造循环结构。下面的例子演示了如何通过嵌套的循环来输出一个九九乘法表。 1234567891011"""输出乘法口诀表(九九表)Version: 0.1Author: 骆昊"""for i in range(1, 10): for j in range(1, i + 1): print('%d*%d=%d' % (i, j, i * j), end='\t') print() 练习练习1：输入一个数判断是不是素数。1234567891011121314151617181920"""输入一个正整数判断它是不是素数Version: 0.1Author: 骆昊Date: 2018-03-01"""from math import sqrtnum = int(input('请输入一个正整数: '))end = int(sqrt(num))is_prime = Truefor x in range(2, end + 1): if num % x == 0: is_prime = False breakif is_prime and num != 1: print('%d是素数' % num)else: print('%d不是素数' % num) 练习2：输入两个正整数，计算最大公约数和最小公倍数。1234567891011121314151617"""输入两个正整数计算最大公约数和最小公倍数Version: 0.1Author: 骆昊Date: 2018-03-01"""x = int(input('x = '))y = int(input('y = '))if x &gt; y: x, y = y, xfor factor in range(x, 0, -1): if x % factor == 0 and y % factor == 0: print('%d和%d的最大公约数是%d' % (x, y, factor)) print('%d和%d的最小公倍数是%d' % (x, y, x * y // factor)) break 练习3：打印三角形图案。12345678910111213141516171819202122232425262728293031323334353637383940414243444546"""打印各种三角形图案*************** * ** *** ********* * *** ***** ****************Version: 0.1Author: 骆昊"""row = int(input('请输入行数: '))for i in range(row): for _ in range(i + 1): print('*', end='') print()for i in range(row): for j in range(row): if j &lt; row - i - 1: print(' ', end='') else: print('*', end='') print()for i in range(row): for _ in range(row - i - 1): print(' ', end='') for _ in range(2 * i + 1): print('*', end='') print()]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03.分支结构]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-03-%E5%88%86%E6%94%AF%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[分支结构应用场景迄今为止，我们写的Python代码都是一条一条语句顺序执行，这种代码结构通常称之为顺序结构。然而仅有顺序结构并不能解决所有的问题，比如我们设计一个游戏，游戏第一关的通关条件是玩家获得1000分，那么在完成本局游戏后，我们要根据玩家得到分数来决定究竟是进入第二关，还是告诉玩家“Game Over”，这里就会产生两个分支，而且这两个分支只有一个会被执行。类似的场景还有很多，我们将这种结构称之为“分支结构”或“选择结构”。给大家一分钟的时间，你应该可以想到至少5个以上这样的例子，赶紧试一试。 if语句的使用在Python中，要构造分支结构可以使用if、elif和else关键字。所谓关键字就是有特殊含义的单词，像if和else就是专门用于构造分支结构的关键字，很显然你不能够使用它作为变量名（事实上，用作其他的标识符也是不可以）。下面的例子中演示了如何构造一个分支结构。 12345678910111213141516"""用户身份验证Version: 0.1Author: 骆昊"""username = input('请输入用户名: ')password = input('请输入口令: ')# 如果希望输入口令时 终端中没有回显 可以使用getpass模块的getpass函数# import getpass# password = getpass.getpass('请输入口令: ')if username == 'admin' and password == '123456': print('身份验证成功!')else: print('身份验证失败!') 唯一需要说明的是和C/C++、Java等语言不同，Python中没有用花括号来构造代码块而是使用了缩进的方式来设置代码的层次结构，如果if条件成立的情况下需要执行多条语句，只要保持多条语句具有相同的缩进就可以了，换句话说连续的代码如果又保持了相同的缩进那么它们属于同一个代码块，相当于是一个执行的整体。 当然如果要构造出更多的分支，可以使用if…elif…else…结构，例如下面的分段函数求值。 12345678910111213141516171819"""分段函数求值 3x - 5 (x &gt; 1)f(x) = x + 2 (-1 &lt;= x &lt;= 1) 5x + 3 (x &lt; -1)Version: 0.1Author: 骆昊"""x = float(input('x = '))if x &gt; 1: y = 3 * x - 5elif x &gt;= -1: y = x + 2else: y = 5 * x + 3print('f(%.2f) = %.2f' % (x, y)) 当然根据实际开发的需要，分支结构是可以嵌套的，例如判断是否通关以后还要根据你获得的宝物或者道具的数量对你的表现给出等级（比如点亮两颗或三颗星星），那么我们就需要在if的内部构造出一个新的分支结构，同理elif和else中也可以再构造新的分支，我们称之为嵌套的分支结构，也就是说上面的代码也可以写成下面的样子。 12345678910111213141516171819"""分段函数求值 3x - 5 (x &gt; 1)f(x) = x + 2 (-1 &lt;= x &lt;= 1) 5x + 3 (x &lt; -1)Version: 0.1Author: 骆昊"""x = float(input('x = '))if x &gt; 1: y = 3 * x - 5else: if x &gt;= -1: y = x + 2 else: y = 5 * x + 3print('f(%.2f) = %.2f' % (x, y)) 说明： 大家可以自己感受一下这两种写法到底是哪一种更好。在之前我们提到的Python之禅中有这么一句话“Flat is better than nested.”，之所以提倡代码“扁平化”是因为嵌套结构的嵌套层次多了之后会严重的影响代码的可读性，所以能使用扁平化的结构时就不要使用嵌套。 练习练习1：英制单位与公制单位互换123456789101112131415"""英制单位英寸和公制单位厘米互换Version: 0.1Author: 骆昊"""value = float(input('请输入长度: '))unit = input('请输入单位: ')if unit == 'in' or unit == '英寸': print('%f英寸 = %f厘米' % (value, value * 2.54))elif unit == 'cm' or unit == '厘米': print('%f厘米 = %f英寸' % (value, value / 2.54))else: print('请输入有效的单位') 练习2：掷骰子决定做什么1234567891011121314151617181920212223"""掷骰子决定做什么事情Version: 0.1Author: 骆昊"""from random import randintface = randint(1, 6)if face == 1: result = '唱首歌'elif face == 2: result = '跳个舞'elif face == 3: result = '学狗叫'elif face == 4: result = '做俯卧撑'elif face == 5: result = '念绕口令'else: result = '讲冷笑话'print(result) 说明： 上面的代码中使用了random模块的randint函数生成指定范围的随机数来模拟掷骰子。 练习3：百分制成绩转等级制123456789101112131415161718192021222324"""百分制成绩转等级制成绩90分以上 --&gt; A80分~89分 --&gt; B70分~79分 --&gt; C60分~69分 --&gt; D60分以下 --&gt; EVersion: 0.1Author: 骆昊"""score = float(input('请输入成绩: '))if score &gt;= 90: grade = 'A'elif score &gt;= 80: grade = 'B'elif score &gt;= 70: grade = 'C'elif score &gt;= 60: grade = 'D'else: grade = 'E'print('对应的等级是:', grade) 练习4：输入三条边长如果能构成三角形就计算周长和面积1234567891011121314151617181920"""判断输入的边长能否构成三角形如果能则计算出三角形的周长和面积Version: 0.1Author: 骆昊"""import matha = float(input('a = '))b = float(input('b = '))c = float(input('c = '))if a + b &gt; c and a + c &gt; b and b + c &gt; a: print('周长: %f' % (a + b + c)) p = (a + b + c) / 2 area = math.sqrt(p * (p - a) * (p - b) * (p - c)) print('面积: %f' % (area))else: print('不能构成三角形') 说明： 上面的代码中使用了math模块的sqrt函数来计算平方根。用边长计算三角形面积的公式叫做海伦公式。 练习5：个人所得税计算器。12345678910111213141516171819202122232425262728293031323334353637"""输入月收入和五险一金计算个人所得税Version: 0.1Author: 骆昊"""salary = float(input('本月收入: '))insurance = float(input('五险一金: '))diff = salary - insurance - 3500if diff &lt;= 0: rate = 0 deduction = 0elif diff &lt; 1500: rate = 0.03 deduction = 0elif diff &lt; 4500: rate = 0.1 deduction = 105elif diff &lt; 9000: rate = 0.2 deduction = 555elif diff &lt; 35000: rate = 0.25 deduction = 1005elif diff &lt; 55000: rate = 0.3 deduction = 2755elif diff &lt; 80000: rate = 0.35 deduction = 5505else: rate = 0.45 deduction = 13505tax = abs(diff * rate - deduction)print('个人所得税: ￥%.2f元' % tax)print('实际到手收入: ￥%.2f元' % (diff + 3500 - tax)) 说明： 上面的代码中使用了Python内置的abs()函数取绝对值来处理-0的问题。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02.语言元素]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-02-%E8%AF%AD%E8%A8%80%E5%85%83%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[语言元素指令和程序计算机的硬件系统通常由五大部件构成，包括：运算器、控制器、存储器、输入设备和输出设备。其中，运算器和控制器放在一起就是我们通常所说的中央处理器，它的功能是执行各种运算和控制指令以及处理计算机软件中的数据。我们通常所说的程序实际上就是指令的集合，我们程序就是将一系列的指令按照某种方式组织到一起，然后通过这些指令去控制计算机做我们想让它做的事情。今天我们大多数时候使用的计算机，虽然它们的元器件做工越来越精密，处理能力越来越强大，但究其本质来说仍然属于“冯·诺依曼结构”的计算机。“冯·诺依曼结构”有两个关键点，一是指出要将存储设备与中央处理器分开，二是提出了将数据以二进制方式编码。二进制是一种“逢二进一”的计数法，跟我们人类使用的“逢十进一”的计数法没有实质性的区别，人类因为有十根手指所以使用了十进制（因为在数数时十根手指用完之后就只能进位了，当然凡事都有例外，玛雅人可能是因为长年光着脚的原因把脚趾头也算上了，于是他们使用了二十进制的计数法，在这种计数法的指导下玛雅人的历法就与我们平常使用的历法不一样，而按照玛雅人的历法，2012年是上一个所谓的“太阳纪”的最后一年，而2013年则是新的“太阳纪”的开始，后来这件事情被以讹传讹的方式误传为”2012年是玛雅人预言的世界末日“这种荒诞的说法，今天我们可以大胆的猜测，玛雅文明之所以发展缓慢估计也与使用了二十进制有关）。对于计算机来说，二进制在物理器件上来说是最容易实现的（高电压表示1，低电压表示0），于是在“冯·诺依曼结构”的计算机都使用了二进制。虽然我们并不需要每个程序员都能够使用二进制的思维方式来工作，但是了解二进制以及它与我们生活中的十进制之间的转换关系，以及二进制与八进制和十六进制的转换关系还是有必要的。如果你对这一点不熟悉，可以自行使用维基百科或者百度百科科普一下。 提示：近期关于量子计算机的研究已经被推倒了风口浪尖，量子计算机基于量子力学进行运算，使用量子瞬移的方式来传递信息。2018年6月，Intel宣布开发出新款量子芯片并通过了在接近绝对零度环境下的测试；2019年1月，IBM向全世界发布了首款商业化量子计算机。 变量和类型在程序设计中，变量是一种存储数据的载体。计算机中的变量是实际存在的数据或者说是存储器中存储数据的一块内存空间，变量的值可以被读取和修改，这是所有计算和控制的基础。计算机能处理的数据有很多种类型，除了数值之外还可以处理文本、图形、音频、视频等各种各样的数据，那么不同的数据就需要定义不同的存储类型。Python中的数据类型很多，而且也允许我们自定义新的数据类型（这一点在后面会讲到），我们先介绍几种常用的数据类型。 整型：Python中可以处理任意大小的整数（Python 2.x中有int和long两种类型的整数，但这种区分对Python来说意义不大，因此在Python 3.x中整数只有int这一种了），而且支持二进制（如0b100，换算成十进制是4）、八进制（如0o100，换算成十进制是64）、十进制（100）和十六进制（0x100，换算成十进制是256）的表示法。 浮点型：浮点数也就是小数，之所以称为浮点数，是因为按照科学记数法表示时，一个浮点数的小数点位置是可变的，浮点数除了数学写法（如123.456）之外还支持科学计数法（如1.23456e2）。 字符串型：字符串是以单引号或双引号括起来的任意文本，比如&#39;hello&#39;和&quot;hello&quot;,字符串还有原始字符串表示法、字节字符串表示法、Unicode字符串表示法，而且可以书写成多行的形式（用三个单引号或三个双引号开头，三个单引号或三个双引号结尾）。 布尔型：布尔值只有True、False两种值，要么是True，要么是False，在Python中，可以直接用True、False表示布尔值（请注意大小写），也可以通过布尔运算计算出来（例如3 &lt; 5会产生布尔值True，而2 == 1会产生布尔值False）。 复数型：形如3+5j，跟数学上的复数表示一样，唯一不同的是虚部的i换成了j。 变量命名对于每个变量我们需要给它取一个名字，就如同我们每个人都有属于自己的响亮的名字一样。在Python中，变量命名需要遵循以下这些必须遵守硬性规则和强烈建议遵守的非硬性规则。 硬性规则： 变量名由字母（广义的Unicode字符，不包括特殊字符）、数字和下划线构成，数字不能开头。 大小写敏感（大写的a和小写的A是两个不同的变量）。 不要跟关键字（有特殊含义的单词，后面会讲到）和系统保留字（如函数、模块等的名字）冲突。 PEP 8要求： 用小写字母拼写，多个单词用下划线连接。 受保护的实例属性用单个下划线开头（后面会讲到）。 私有的实例属性用两个下划线开头（后面会讲到）。 当然，作为一个专业的程序员，给变量（事实上应该是所有的标识符）命名时做到见名知意也是非常重要的。 变量的使用下面通过几个例子来说明变量的类型和变量使用。 12345678910111213141516"""使用变量保存数据并进行算术运算Version: 0.1Author: 骆昊"""a = 321b = 123print(a + b)print(a - b)print(a * b)print(a / b)print(a // b)print(a % b)print(a ** b) 123456789101112131415161718"""使用input()函数获取键盘输入使用int()进行类型转换用占位符格式化输出的字符串Version: 0.1Author: 骆昊"""a = int(input('a = '))b = int(input('b = '))print('%d + %d = %d' % (a, b, a + b))print('%d - %d = %d' % (a, b, a - b))print('%d * %d = %d' % (a, b, a * b))print('%d / %d = %f' % (a, b, a / b))print('%d // %d = %d' % (a, b, a // b))print('%d %% %d = %d' % (a, b, a % b))print('%d ** %d = %d' % (a, b, a ** b)) 123456789101112131415161718"""使用type()检查变量的类型Version: 0.1Author: 骆昊Date: 2018-02-27"""a = 100b = 12.345c = 1 + 5jd = 'hello, world'e = Trueprint(type(a))print(type(b))print(type(c))print(type(d))print(type(e)) 在对变量类型进行转换时可以使用Python的内置函数（准确的说下面列出的并不是真正意义上的函数，而是后面我们要讲到的创建对象的构造方法）。 int()：将一个数值或字符串转换成整数，可以指定进制。 float()：将一个字符串转换成浮点数。 str()：将指定的对象转换成字符串形式，可以指定编码。 chr()：将整数转换成该编码对应的字符串（一个字符）。 ord()：将字符串（一个字符）转换成对应的编码（整数）。 运算符Python支持多种运算符，下表大致按照优先级从高到低的顺序列出了所有的运算符，我们会陆续使用到它们。 运算符 描述 [] [:] 下标，切片 ** 指数 ~ + - 按位取反, 正负号 * / % // 乘，除，模，整除 + - 加，减 &gt;&gt; &lt;&lt; 右移，左移 &amp; 按位与 ^ | 按位异或，按位或 &lt;= &lt; &gt; &gt;= 小于等于，小于，大于，大于等于 == != 等于，不等于 is is not 身份运算符 in not in 成员运算符 not or and 逻辑运算符 = += -= *= /= %= //= **= &amp;= ` =^=&gt;&gt;=&lt;&lt;=` 说明： 在实际开发中，如果搞不清楚运算符的优先级，可以使用括号来确保运算的执行顺序。 下面的例子演示了运算符的使用。 123456789101112131415161718192021222324252627282930"""运算符的使用Version: 0.1Author: 骆昊"""a = 5b = 10c = 3d = 4e = 5a += ba -= ca *= da /= eprint("a = ", a)flag1 = 3 &gt; 2flag2 = 2 &lt; 1flag3 = flag1 and flag2flag4 = flag1 or flag2flag5 = not flag1print("flag1 = ", flag1)print("flag2 = ", flag2)print("flag3 = ", flag3)print("flag4 = ", flag4)print("flag5 = ", flag5)print(flag1 is True)print(flag2 is not False) 练习练习1：华氏温度转摄氏温度。1234567891011"""将华氏温度转换为摄氏温度F = 1.8C + 32Version: 0.1Author: 骆昊"""f = float(input('请输入华氏温度: '))c = (f - 32) / 1.8print('%.1f华氏度 = %.1f摄氏度' % (f, c)) 练习2：输入圆的半径计算计算周长和面积。1234567891011121314"""输入半径计算圆的周长和面积Version: 0.1Author: 骆昊"""import mathradius = float(input('请输入圆的半径: '))perimeter = 2 * math.pi * radiusarea = math.pi * radius * radiusprint('周长: %.2f' % perimeter)print('面积: %.2f' % area) 练习3：输入年份判断是不是闰年。123456789101112"""输入年份 如果是闰年输出True 否则输出FalseVersion: 0.1Author: 骆昊"""year = int(input('请输入年份: '))# 如果代码太长写成一行不便于阅读 可以使用\或()折行is_leap = (year % 4 == 0 and year % 100 != 0 or year % 400 == 0)print(is_leap)]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01.初识Python]]></title>
    <url>%2F2014%2F07%2F28%2Flang-python-Python-100-Days-01-%E5%88%9D%E8%AF%86Python%2F</url>
    <content type="text"><![CDATA[初识PythonPython简介Python的历史 1989年圣诞节：Guido von Rossum开始写Python语言的编译器。 1991年2月：第一个Python编译器（同时也是解释器）诞生，它是用C语言实现的（后面又出现了Java和C#实现的版本Jython和IronPython，以及PyPy、Brython、Pyston等其他实现），可以调用C语言的库函数。在最早的版本中，Python已经提供了对“类”，“函数”，“异常处理”等构造块的支持，同时提供了“列表”和“字典”等核心数据类型，同时支持以模块为基础来构造应用程序。 1994年1月：Python 1.0正式发布。 2000年10月16日：Python 2.0发布，增加了实现完整的垃圾回收，提供了对Unicode的支持。与此同时，Python的整个开发过程更加透明，社区对开发进度的影响逐渐扩大，生态圈开始慢慢形成。 2008年12月3日：Python 3.0发布，它并不完全兼容之前的Python代码，不过因为目前还有不少公司在项目和运维中使用Python 2.x版本，所以Python 3.x的很多新特性后来也被移植到Python 2.6/2.7版本中。 目前我们使用的Python 3.7.x的版本是在2018年发布的，Python的版本号分为三段，形如A.B.C。其中A表示大版本号，一般当整体重写，或出现不向后兼容的改变时，增加A；B表示功能更新，出现新功能时增加B；C表示小的改动（例如：修复了某个Bug），只要有修改就增加C。如果对Python的历史感兴趣，可以阅读名为《Python简史》的博文。 Python的优缺点Python的优点很多，简单的可以总结为以下几点。 简单和明确，做一件事只有一种方法。 学习曲线低，跟其他很多语言相比，Python更容易上手。 开放源代码，拥有强大的社区和生态圈。 解释型语言，天生具有平台可移植性。 支持两种主流的编程范式（面向对象编程和函数式编程）都提供了支持。 可扩展性和可嵌入性，可以调用C/C++代码，也可以在C/C++中调用Python。 代码规范程度高，可读性强，适合有代码洁癖和强迫症的人群。 Python的缺点主要集中在以下几点。 执行效率稍低，因此计算密集型任务可以由C/C++编写。 代码无法加密，但是现在很多公司都不销售卖软件而是销售服务，这个问题会被淡化。 在开发时可以选择的框架太多（如Web框架就有100多个），有选择的地方就有错误。 Python的应用领域目前Python在Web应用开发、云基础设施、DevOps、网络爬虫开发、数据分析挖掘、机器学习等领域都有着广泛的应用，因此也产生了Web后端开发、数据接口开发、自动化运维、自动化测试、科学计算和可视化、数据分析、量化交易、机器人开发、图像识别和处理等一系列的职位。 搭建编程环境Windows环境可以在Python官方网站下载到Python的Windows安装程序（exe文件），需要注意的是如果在Windows 7环境下安装Python 3.x，需要先安装Service Pack 1补丁包（可以通过一些工具软件自动安装系统补丁的功能来安装），安装过程建议勾选“Add Python 3.x to PATH”（将Python 3.x添加到PATH环境变量）并选择自定义安装，在设置“Optional Features”界面最好将“pip”、“tcl/tk”、“Python test suite”等项全部勾选上。强烈建议选择自定义的安装路径并保证路径中没有中文。安装完成会看到“Setup was successful”的提示。如果稍后运行Python程序时，出现因为缺失一些动态链接库文件而导致Python解释器无法工作的问题，可以按照下面的方法加以解决。 如果系统显示api-ms-win-crt*.dll文件缺失，可以参照《api-ms-win-crt*.dll缺失原因分析和解决方法》一文讲解的方法进行处理或者直接在微软官网下载Visual C++ Redistributable for Visual Studio 2015文件进行修复；如果是因为更新Windows的DirectX之后导致某些动态链接库文件缺失问题，可以下载一个DirectX修复工具进行修复。 Linux环境Linux环境自带了Python 2.x版本，但是如果要更新到3.x的版本，可以在Python的官方网站下载Python的源代码并通过源代码构建安装的方式进行安装，具体的步骤如下所示。 安装依赖库（因为没有这些依赖库可能在源代码构件安装时因为缺失底层依赖库而失败）。 1yum -y install wget gcc zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel libffi-devel 下载Python源代码并解压缩到指定目录。 123wget https://www.python.org/ftp/python/3.7.3/Python-3.7.3.tgzxz -d Python-3.7.3.tar.xztar -xvf Python-3.7.3.tar 切换至Python源代码目录并执行下面的命令进行配置和安装。 123cd Python-3.7.3./configure --prefix=/usr/local/python37 --enable-optimizationsmake &amp;&amp; make install 修改用户主目录下名为.bash_profile的文件，配置PATH环境变量并使其生效。 12cd ~vim .bash_profile 12345# ... 此处省略上面的代码 ...export PATH=$PATH:/usr/local/python37/bin# ... 此处省略下面的代码 ... 激活环境变量。 1source .bash_profile macOS环境macOS也自带了Python 2.x版本，可以通过Python的官方网站提供的安装文件（pkg文件）安装Python 3.x的版本。默认安装完成后，可以通过在终端执行python命令来启动2.x版本的Python解释器，可以通过执行python3命令来启动3.x版本的Python解释器。 从终端运行Python程序确认Python的版本可以Windows的命令行提示符中键入下面的命令。 1python --version 或者是在Linux或macOS系统的终端中键入下面的命令。 1python3 --version 当然也可以先输入python或python3进入交互式环境，再执行以下的代码检查Python的版本。 1234import sysprint(sys.version_info)print(sys.version) 编写Python源代码可以用文本编辑工具（推荐使用Sublime、Atom、Visual Studio Code等高级文本编辑工具）编写Python源代码并用py作为后缀名保存该文件，代码内容如下所示。 1print('hello, world!') 运行程序切换到源代码所在的目录并执行下面的命令，看看屏幕上是否输出了”hello, world!”。 1python hello.py 或 1python3 hello.py 代码中的注释注释是编程语言的一个重要组成部分，用于在源代码中解释代码的作用从而增强程序的可读性和可维护性，当然也可以将源代码中不需要参与运行的代码段通过注释来去掉，这一点在调试程序的时候经常用到。注释在随源代码进入预处理器或编译时会被移除，不会在目标代码中保留也不会影响程序的执行结果。 单行注释 - 以#和空格开头的部分 多行注释 - 三个引号开头，三个引号结尾 12345678910111213"""第一个Python程序 - hello, world!向伟大的Dennis M. Ritchie先生致敬Version: 0.1Author: 骆昊"""print('hello, world!')# print("你好,世界！")print('你好', '世界')print('hello', 'world', sep=', ', end='!')print('goodbye, world', end='!\n') 其他工具介绍IDLE - 自带的集成开发工具IDLE是安装Python环境时自带的集成开发工具，如下图所示。但是由于IDLE的用户体验并不是那么好所以很少在实际开发中被采用。 IPython - 更好的交互式编程工具IPython是一种基于Python的交互式解释器。相较于原生的Python交互式环境，IPython提供了更为强大的编辑和交互功能。可以通过Python的包管理工具pip安装IPython和Jupyter，具体的操作如下所示。 1pip install ipython 或 1pip3 install ipython 安装成功后，可以通过下面的ipython命令启动IPython，如下图所示。 当然，我们也可以通过安装Jupyter工具并运行名为notebook的程序在浏览器窗口中进行交互式代码编写操作。 1pip install jupyter 或 1pip3 intall jupyter 然后执行下面的命令： 1jupyter notebook Sublime / Visual Studio Code - 高级文本编辑器 首先可以通过官方网站下载安装程序安装Sublime 3或Sublime 2。 安装包管理工具。 通过快捷键Ctrl+`或者在View菜单中选择Show Console打开控制台，输入下面的代码。 Sublime 3 1import urllib.request,os;pf='Package Control.sublime-package';ipp=sublime.installed_packages_path();urllib.request.install_opener(urllib.request.build_opener(urllib.request.ProxyHandler()));open(os.path.join(ipp,pf),'wb').write(urllib.request.urlopen('http://sublime.wbond.net/'+pf.replace(' ','%20')).read()) Sublime 2 1import urllib2,os;pf='Package Control.sublime-package';ipp=sublime.installed_packages_path();os.makedirs(ipp)ifnotos.path.exists(ipp)elseNone;urllib2.install_opener(urllib2.build_opener(urllib2.ProxyHandler()));open(os.path.join(ipp,pf),'wb').write(urllib2.urlopen('http://sublime.wbond.net/'+pf.replace(' ','%20')).read());print('Please restart Sublime Text to finish installation') 手动安装浏览器输入 https://sublime.wbond.net/Package%20Control.sublime-package 下载这个文件下载好以后，打开sublime text，选择菜单Preferences-&gt;Browse Packages… 打开安装目录此时会进入到一个叫做Packages的目录下，点击进入上一层目录Sublime Text3，在此目录下有一个文件夹叫做Installed Packages，把刚才下载的文件放到这里就可以了。然后重启sublime text3，观察Preferences菜单最下边是否有Package Settings 和Package Control两个选项，如果有，则代表安装成功了。 安装插件。通过Preference菜单的Package Control或快捷键Ctrl+Shift+P打开命令面板，在面板中输入Install Package就可以找到安装插件的工具，然后再查找需要的插件。我们推荐大家安装以下几个插件： SublimeCodeIntel - 代码自动补全工具插件。 Emmet - 前端开发代码模板插件。 Git - 版本控制工具插件。 Python PEP8 Autoformat - PEP8规范自动格式化插件。 ConvertToUTF8 - 将本地编码转换为UTF-8。 说明：事实上Visual Studio Code可能是更好的选择，它不用花钱并提供了更为完整和强大的功能，有兴趣的读者可以自行研究。 PyCharm - Python开发神器PyCharm的安装、配置和使用在《玩转PyCharm》进行了介绍，有兴趣的读者可以选择阅读。 练习 在Python交互环境中查看下面的代码结果，并试着将这些内容翻译成中文。 1import this 说明：当前键入上面的命令后会在交互式环境中看到如下所示的输出，这段内容被称为“Python之禅”，里面讲述的道理不仅仅适用于Python，也适用于其他编程语言。 学习使用turtle在屏幕上绘制图形。 说明：turtle是Python内置的一个非常有趣的模块，特别适用于让小朋友体会什么是编程，它最早是Logo语言的一部分，Logo语言是Wally Feurzig和Seymour Papert在1966发明的编程语言. 123456789101112import turtleturtle.pensize(4)turtle.pencolor('red')turtle.forward(100)turtle.right(90)turtle.forward(100)turtle.right(90)turtle.forward(100)turtle.right(90)turtle.forward(100)turtle.mainloop()]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-时间]]></title>
    <url>%2F2014%2F07%2F23%2Flang-python-2015-07-23-python-%E6%97%B6%E9%97%B4%2F</url>
    <content type="text"><![CDATA[说明下面是time的应用，另外datetime中也有对时间的处理距离1970年01月01日的时间差import time; # 引入time模块 ticks = time.time() print &quot;当前时间戳为:&quot;, ticks获取当地时间元组localtime = time.localtime(time.time()) print &quot;本地时间为 :&quot;, localtime #time.struct_time(tm_year=2016, tm_mon=4, tm_mday=7,tm_hour=10, tm_min=3, tm_sec=27, tm_wday=3, tm_yday=98,tm_isdst=0)获取格式化的时间localtime = time.asctime( time.localtime(time.time()) ) print &quot;本地时间为 :&quot;, localtime #本地时间为 : Thu Apr 7 10:05:21 2016格式化日期# 格式化成2016-03-20 11:45:39形式 print time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, time.localtime()) # 格式化成Sat Mar 28 22:24:24 2016形式 print time.strftime(&quot;%a %b %d %H:%M:%S %Y&quot;, time.localtime()) # 将格式字符串转换为时间戳 a = &quot;Sat Mar 28 22:24:24 2016&quot; print time.mktime(time.strptime(a,&quot;%a %b %d %H:%M:%S %Y&quot;))]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-数据类型]]></title>
    <url>%2F2014%2F07%2F22%2Flang-python-2015-07-22-python-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[数字（int，long，float浮点型，complex复数）var1 = 1 var2 = 12345L var3 = 0.0 var4 = 9.322e-36j int(s) #字符串转int，long、float、complex同数学函数（自带+第三方+随机数+三角函数）Python 中数学运算常用的函数基本都在 math 模块、cmath 模块中。 Python math 模块提供了许多对浮点数的数学运算函数。 Python cmath 模块包含了一些用于复数运算的函数。字符串str = &apos;X u G u o z h o n g&apos; 0 1 2 3 4 5 6 7 8 9 -3-2-1 str str[-2] # n str[1:3] # uGu str[7:] # ong str(x) #转字符串类型，同repr(x) eval(str) #将字符串转换为代码执行转义字符 \行尾表示续行符，其他地方表示转义，例如\n表示换行字符串格式化print &quot;My name is %s and weight is %d kg!&quot; % (&apos;Zara&apos;, 21) 内建函数string.format() string.strip([obj]) #在 string 上执行 lstrip()和 rstrip()列表list = [ &apos;xgz&apos;, 786 , 2.23, &apos;john&apos;, 70.2 ] tinylist = [123, &apos;frank&apos;] print list # 输出完整列表 print list[0] # 输出列表的第一个元素 print list[1:3] # 输出第二个至第三个元素 print list[2:] # 输出从第三个开始至列表末尾的所有元素 print tinylist * 2 # 输出列表两次 print list + tinylist # 打印组合的列表 list(s) #转化为列表 set() 函数创建一个无序不重复元素集添加及删除list.append(&apos;Google&apos;) ## 使用 append() 添加元素 del list[2] #删除求长度len(list)元组就是把列表的[]变成()同时变为只读列表 tuple(s)字典dict = {} dict[&apos;one&apos;] = &quot;This is one&quot; dict[2] = &quot;This is two&quot; tinydict = {&apos;name&apos;: &apos;john&apos;,&apos;code&apos;:6734, &apos;dept&apos;: &apos;sales&apos;}​​ print dict[‘one’] # 输出键为’one’ 的值​ print dict[2] # 输出键为 2 的值​ print tinydict # 输出完整的字典​ print tinydict.keys() # 输出所有键​ print tinydict.values() # 输出所有值​​ dict(d) #创建一个字典。d 必须是一个序列 (key,value)元组。 修改及删除字典dict[&apos;Age&apos;] = 8; # 更新 del dict[&apos;Name&apos;]; # 删除键是&apos;Name&apos;的条目 dict.clear(); # 清空词典所有条目 del dict ; # 删除词典求长度len(dict)成员运算符in not in身份运算符is is not]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript正则]]></title>
    <url>%2F2012%2F11%2F28%2Flang-Javascript-%E4%B8%80%E4%BA%9B%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%9A%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[一些匹配方法去除首尾的12//去除首尾的‘/’input = input.replace(/^\/*|\/*$/g,''); javascript:; 、javascript:void(0)12'javascript:;'.match(/^(javascript\s*\:|#)/);//["javascript:", "javascript:", index: 0, input: "javascript:;"] 匹配123var str = "access_token=dcb90862-29fb-4b03-93ff-5f0a8f546250; refresh_token=702f4815-a0ff-456c-82ce-24e4d7d619e6; account_uid=1361177947320160506170322436";str.match(/account_uid=([^\=]+(\;)|(.*))/ig);//=&gt; ["account_uid=1361177947320160506170322436"] 匹配一些字符123var str = 'asdf html-webpack-plugin for "index/index.html" asdfasdf';str.match(/html-webpack-plugin for \"(.*)\"/ig);console.log(RegExp.$1) //=&gt;index/index.html 关键字符替换12'css/[hash:8].index-index.css'.replace(/\[(?:(\w+):)?(contenthash|hash)(?::([a-z]+\d*))?(?::(\d+))?\]/ig,'(.*)');//=&gt; css/(.*).index-index.css 替换参数中的值123456789var str = '&lt;!DOCTYPE html&gt;&lt;html manifest="../../cache.manifest" lang="en"&gt;&lt;head&gt;&lt;meta charset="UTF-8"&gt;';str.replace(/&lt;html[^&gt;]*manifest="([^"]*)"[^&gt;]*&gt;/,function(word)&#123; return word.replace(/manifest="([^"]*)"/,'manifest="'+url+'"');&#125;).replace(/&lt;html(\s?[^\&gt;]*\&gt;)/,function(word)&#123; if(word.indexOf('manifest')) return word; return word.replace('&lt;html','&lt;html manifest="'+url+'"');&#125;);//原：&lt;!DOCTYPE html&gt;&lt;html manifest="../../cache.manifest" lang="en"&gt;&lt;head&gt;&lt;meta charset="UTF-8"&gt;//替换成=&gt; &lt;!DOCTYPE html&gt;&lt;html manifest="cache.manifest" lang="en"&gt;&lt;head&gt;&lt;meta charset="UTF-8"&gt; 匹配括号内容1234'max_length(12)'.match(/^(.+?)\((.+)\)$/)// ["max_length(12)", "max_length", "12", index: 0, input: "max_length(12)"]'hello(world)code(js)javascirpt'.match(/\((\w*)+?\)/gmi);// =&gt; ["(world)", "(js)"] 调换123var name = "Doe, John"; name.replace(/(\w+)\s*, \s*(\w+)/, "$2 $1"); //=&gt; "John Doe" 字符串截取1234var str = 'asfdf === sdfaf ##'str.match(/[^===]+(?=[===])/g) // 截取 ===之前的内容str.replace(/\n/g,'') // 替换字符串中的 \n 换行字符 浏览器版本12navigator.userAgent.match(/chrome\/([\d]+)\.([\d]+)\.([\d]+)\.([\d]+)/i);//=&gt; ["Chrome/64.0.3282.167", "64", "0", "3282", "167", index: 87, input: "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) Ap…L, like Gecko) Chrome/64.0.3282.167 Safari/537.36"] 验证小数点后几位验证12345// 精确到1位小数/^[1-9][0-9]*$|^[1-9][0-9]*\.[0-9]$|^0\.[0-9]$/.test(1.2);// 精确到2位小数/^[0-9]+(.[0-9]&#123;2&#125;)?$/.test(1.221); 密码强度正则12345// 必须是包含大小写字母和数字的组合，不能使用特殊字符，长度在8-10之间。/^(?=.*\d)(?=.*[a-z])(?=.*[A-Z]).&#123;8,10&#125;$/.test("weeeeeeeW2");//密码强度正则，最少6位，包括至少1个大写字母，1个小写字母，1个数字，1个特殊字符/^.*(?=.&#123;6,&#125;)(?=.*\d)(?=.*[A-Z])(?=.*[a-z])(?=.*[!@#$%^&amp;*? ]).*$/.test("diaoD123#");//输出 true 校验中文123/^[\u4e00-\u9fa5]&#123;0,&#125;$/.test("但是d"); //false/^[\u4e00-\u9fa5]&#123;0,&#125;$/.test("但是"); //true/^[\u4e00-\u9fa5]&#123;0,&#125;$/.test("但是"); //true 包含中文正则1/[\u4E00-\u9FA5]/.test("但是d") //true 由数字、26个英文字母或下划线组成的字符串1/^\w+$/.test("ds2_@#"); // false 身份证号正则123//身份证号（18位）正则/^[1-9]\d&#123;5&#125;(18|19|([23]\d))\d&#123;2&#125;((0[1-9])|(10|11|12))(([0-2][1-9])|10|20|30|31)\d&#123;3&#125;[0-9Xx]$/.test("42112319870115371X");//输出 false 校验日期“yyyy-mm-dd“ 格式的日期校验，已考虑平闰年。 123456789101112131415//日期正则，简单判定,未做月份及日期的判定var dP1 = /^\d&#123;4&#125;(\-)\d&#123;1,2&#125;\1\d&#123;1,2&#125;$/;//输出 trueconsole.log(dP1.test("2017-05-11"));//输出 trueconsole.log(dP1.test("2017-15-11"));//日期正则，复杂判定var dP2 = /^(?:(?!0000)[0-9]&#123;4&#125;-(?:(?:0[1-9]|1[0-2])-(?:0[1-9]|1[0-9]|2[0-8])|(?:0[13-9]|1[0-2])-(?:29|30)|(?:0[13578]|1[02])-31)|(?:[0-9]&#123;2&#125;(?:0[48]|[2468][048]|[13579][26])|(?:0[48]|[2468][048]|[13579][26])00)-02-29)$/;//输出 trueconsole.log(dP2.test("2017-02-11"));//输出 falseconsole.log(dP2.test("2017-15-11"));//输出 falseconsole.log(dP2.test("2017-02-29"));// true 校验文件后缀1234567 var strRegex = "(.jpg|.gif|.txt)"; var re=new RegExp(strRegex); if (re.test(str))&#123; &#125;/(.jpg|.gif)+(\?|\#|$)/.test('a/b/c.jpgsss'); //=&gt; false/(.jpg|.gif)+(\?|\#|$)/.test('a/b/c.jpg?'); //=&gt; true 用户名正则123//用户名正则，4到16位（字母，数字，下划线，减号）/^[a-zA-Z0-9_-]&#123;4,16&#125;$/.test("diaodiao");//输出 true 整数正则1234567/^\d+$/.test("42"); //正整数正则 -&gt; 输出 true/^-\d+$/.test("-42"); //负整数正则 -&gt; 输出 true/^-?\d+$/.test("-42"); //整数正则 -&gt; 输出 true/^[0-9]+$/.test(25.5455) //正整数正则 -&gt; 输出 false// 浮点数/^(?:[-+])?(?:[0-9]+)?(?:\.[0-9]*)?(?:[eE][\+\-]?(?:[0-9]+))?$/.test(0.2) 数字正则可以是整数也可以是浮点数 123/^\d*\.?\d+$/.test("42.2"); //正数正则 -&gt; 输出 true/^-\d*\.?\d+$/.test("-42.2"); //负数正则 -&gt; 输出 true/^-?\d*\.?\d+$/.test("-42.2"); //数字正则 -&gt; 输出 true Email正则1234567891011//Email正则/^([A-Za-z0-9_\-\.])+\@([A-Za-z0-9_\-\.])+\.([A-Za-z]&#123;2,4&#125;)$/.test("wowohoo@qq.com");//输出 true// 1.邮箱以a-z、A-Z、0-9开头，最小长度为1.// 2.如果左侧部分包含-、_、.则这些特殊符号的前面必须包一位数字或字母。// 3.@符号是必填项// 4.右则部分可分为两部分，第一部分为邮件提供商域名地址，第二部分为域名后缀，现已知的最短为2位。// 最长的为6为。// 5.邮件提供商域可以包含特殊字符-、_、./^[a-z0-9]+([._\\-]*[a-z0-9])*@([a-z0-9]+[-a-z0-9]*[a-z0-9]+.)&#123;1,63&#125;[a-z0-9]+$/.test("wowohoo@qq.com"); 传真号码12// 国家代码(2到3位)-区号(2到3位)-电话号码(7到8位)-分机号(3位)/^(([0\+]\d&#123;2,3&#125;-)?(0\d&#123;2,3&#125;)-)(\d&#123;7,8&#125;)(-(\d&#123;3,&#125;))?$/.test('021-5055455') 手机号码正则1234567891011//手机号正则/^1[34578]\d&#123;9&#125;$/.test("13611778887");//输出 true//* 13段：130、131、132、133、134、135、136、137、138、139//* 14段：145、147//* 15段：150、151、152、153、155、156、157、158、159//* 17段：170、176、177、178//* 18段：180、181、182、183、184、185、186、187、188、189//* 国际码 如：中国(+86)/^((\+?[0-9]&#123;1,4&#125;)|(\(\+86\)))?(13[0-9]|14[57]|15[012356789]|17[03678]|18[0-9])\d&#123;8&#125;$/.test("13611778887"); URL正则1234567891011//URL正则/^((https?|ftp|file):\/\/)?([\da-z\.-]+)\.([a-z\.]&#123;2,6&#125;)([\/\w \.-]*)*\/?$/.test("http://wangchujiang.com");//输出 true//获取url中域名、协议正则 'http://xxx.xx/xxx','https://xxx.xx/xxx','//xxx.xx/xxx'/^(http(?:|s)\:)*\/\/([^\/]+)/.test("http://www.baidu.com");/^((http|https):\/\/(\w+:&#123;0,1&#125;\w*@)?(\S+)|)(:[0-9]+)?(\/|\/([\w#!:.?+=&amp;%@!\-\/]))?$/.test('https://www.baidu.com/s?wd=@#%$^&amp;%$#')// 必须有协议 /^[a-zA-Z]+:\/\//.test("http://www.baidu.com"); 域名正则表达式12/^([a-zA-Z0-9]([a-zA-Z0-9\-]&#123;0,61&#125;[a-zA-Z0-9])?\.)+[a-zA-Z]&#123;2,6&#125;$/.test('blog.csdn.net');// 输出 true Mac地址匹配12/^([0-9a-fA-F][0-9a-fA-F]:)&#123;5&#125;([0-9a-fA-F][0-9a-fA-F])$/.test('dc:a9:04:77:37:20');// 输出 true 浮点数正则表达式12/[-+]?(?:\b[0-9]+(?:\.[0-9]*)?|\.[0-9]+\b)(?:[eE][-+]?[0-9]+\b)?/.test(+334.4443434343e3);//输出 true IPv4地址正则123//ipv4地址正则/^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.)&#123;3&#125;(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$/.test("192.168.130.199");//输出 true 日期格式化yyyy-MM-dd正则12/(19|20)\d\d([- /.])(0[1-9]|1[012])\2(0[1-9]|[12][0-9]|3[01])/.test('2019-09-12')//输出 true 十六进制颜色正则123//RGB Hex颜色正则/^#?([a-fA-F0-9]&#123;6&#125;|[a-fA-F0-9]&#123;3&#125;)$/.test("#b8b8b8");//输出 true QQ号码正则12//QQ号正则，5至11位/^[1-9][0-9]&#123;4,10&#125;$/.test("398188661");//输出 true 微信号正则12//微信号正则，6至20位，以字母开头，字母，数字，减号，下划线/^[a-zA-Z]([-_a-zA-Z0-9]&#123;5,19&#125;)+$/.test("jslite"); //输出 true 车牌号正则12//车牌号正则/^[京津沪渝冀豫云辽黑湘皖鲁新苏浙赣鄂桂甘晋蒙陕吉闽贵粤青藏川宁琼使领A-Z]&#123;1&#125;[A-Z]&#123;1&#125;[A-Z0-9]&#123;4&#125;[A-Z0-9挂学警港澳]&#123;1&#125;$/.test("沪B99116") //输出 true 颜色值校验12// HEX 颜色正则/^#?([0-9a-fA-F]&#123;3&#125;|[0-9a-fA-F]&#123;6&#125;)$/.test("#ccb2b2") 工具 Regular Expression Library Regular expression visualizer using railroad diagrams Online regex tester. Full highlighting of regex syntax. Very useful. Javascript regex used. 正则表达式在线测试工具 Regulex：JavaScript正则表达式展示器]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[轻量高效的开源JavaScript插件和库]]></title>
    <url>%2F2012%2F11%2F28%2Flang-Javascript-%E8%BD%BB%E9%87%8F%E9%AB%98%E6%95%88%E7%9A%84%E5%BC%80%E6%BA%90JavaScript%E6%8F%92%E4%BB%B6%E5%92%8C%E5%BA%93%2F</url>
    <content type="text"><![CDATA[图片 baguetteBox.js - 是一个简单易用的响应式图像灯箱效果脚本。demo Lightgallery.js - 是一个功能齐全的JavaScript图像灯箱插件。demo viewerjs - 是一个图像预览插件。demo cropperjs - 是一个图片编辑器插件。demo photo-editor - 是一个本地图片编辑器插件。demo blazy.js - 是一个懒惰加载插件。demo tesseract.js - 可以从图像中获取几乎任何语言的单词。demo 布局 SuperEmbed.js - 是一个Javascript库，可检测出网页上的内嵌视频并使他们能够变成响应式元素。demo ScrollReveal - ScrollReveal插件使用户能够无比轻松地创建桌面和移动浏览器的网页滚动动画。demo Bricks.js - 是一款超快的用于固定宽度元素的“砖石”布局生成器。demo 轮播图 Swipe - 准确的触摸滑块。demo Lory - 是一个由 Vanilla JavaScript 编写的拥有触摸功能的简约滑块。demo baguetteBox.js - 是一个简单、易用的响应式 Lightbox 图片库，它支持移动端上触滑动手势操作，无依赖。demo 弹出层 Popper.js - 是一个轻量级的库用于管理工具提示和弹窗效果。demo SweetAlert2 - 是一个颜值很高而且可以自定义的警告弹出窗口插件，可以代替Javascript的弹出窗口。demo artDialog - 是一个经典、优雅的网页对话框控件。demo layer - 是一个web弹层组件。demo 音频视频 Loud Links - 是一个轻量级的JavaScript库用于添加交互声音到您的站点。demo flv.js - B 站 HTML5 播放器内核开源。 Loud Links - 是一个轻量级 JavaScript 库，用于向您的网站添加交互音频。。demo 编辑器 MediumEditor - 仿Medium.com的所见即所得在线编辑器工具栏。demo Substance - 是一个基于Web的内容自定义编辑器。demo flatpickr - 是一个轻量级的代码高亮库，适用于任何编程语言。demo pen - 是一个Markdown编辑器工具。demo aceAce（Ajax.org Cloud9 Editor）。demo CodeMirror浏览器端的代码编辑器。demo esprima用于综合分析的 ECMAScript 解析器。demo quill一个带有 API 的跨浏览器富文本编辑器。(demo) ckeditor-releases 适用于每个人的 web 文本编辑器。demo editor 一个 markdown 编辑器，但仍在开发中。demo EpicEditor 一个可嵌入的 js Markdown的编辑器，拥有全屏编辑、即时预览、自动保存草稿和离线支持等功能。demo jsoneditor 查看、编辑和格式化 JSON 的 web 工具。demo vim.js 拥有持久化 ~/.vimrc 的 Vim 编辑器的 JavaScript 移植版本。demo Squire HTML5 富文本编辑器。demo TinyMCE JavaScript 富文本编辑器。demo trix 由 Basecamp 制作，适用于每天写作的富文本编辑器。demo Editor.md 由 Basecamp 制作，适用于每天写作的富文本编辑器。demo 字符串 selecting - 一个允许你获取用户选定文本的库。 string.js - 额外的 JavaScript 字符串方法。demo he - 健壮的 HTML 实体编码/解码器。 multiline - 多行字符串。 query-string - 解析和字符串化 URL 查询字符串。 URI.js - URL 操作库。demo jsurl - 轻量的 URL 操作库。 sprintf.js - 实现字符串格式化。 url-pattern - 让 url 和其它字符串进行比正则表达式匹配更简单。字符串和数据可相互转化。 Numeral.js - 格式化和操作数字的 JS 库。 demo 表单 validator.js - 轻量级的JavaScript表单验证，字符串验证。demo List.js - 是一个轻量级的为列表、表格或其他任何HTMLL标签增加了搜索，排序，过滤器和灵活性等元素。demo Algolia Places - 是一个能让你在网页轻易实现搜索栏自动完成功能。demo Cleave.js - 是一个会在你输入时格式化你的&lt;input/&gt;标签里面的内容。demo validator.js - 是一个简单、轻量级，但功能强大的 Validator 组件。demo axios - 是一个基于浏览器和node的HTTP请求库，绿色环保只有12kb。 存储 store.js - 本地存储localstorage的封装，提供简单的API。demo cookie.js - 对操作cookie的封装，提供简单的AIP 兼容IE6。demo store.js - 为所有浏览器封装了LocalStorage，隐秘地使用localStorage、globalStorage和用户数据。 localForage - 改善后的离线存储。封装了IndexedDB、WebSQL和localStorage。demo cross-storage - 获得权限后，能跨域名本地存储。 basket.js - 用 localStorage 加载和缓存脚本的资源加载器。demo bag.js - 可以缓存脚本和加载资源，增加了键值对接口和对localStorage/websql/indexedDB 的支持。 basil.js - 智能的&nbsp;JavaScript 数据持久层库。 Cookies - 客户端 Cookie 操作库。 DB.js - 基于 Promise 的、封装了 IndexedDB 的库。demo lawnchair.js - 简单的客户端 JSON 存储。demo 动画 anime.js - 是一个灵活轻便的JavaScript动画库。demo three.js - 是一个JS 3D库。demo loaders.css - CSS 动画加载效果。demo Hover.css - 一款基于 CSS3 的悬停特效合集。demo Effeckt.css - 一个包含众多精妙的 CSS3 切换和动画效果库。demo Magic Animations - 一个独特的 CSS3 动画特效包。demo Transformicons - 一个结合 SVG、CSS 和 HTML 技术，让图标、按钮和符号具有变种（特殊）动画效果的库。demo SpinKit - 一款 CSS 加载动画合集，可高度自定义动画效果。demo d3-ease - 这是一个让动画更为平滑的 Easing 库。 ScrollMagic - 一个用来创建魔幻滚动交互的 JavaScript 库，可以像使用进度条一样使用滚动条。demo ScrollReveal - 一款页面滚动显示动画，可以播放一次也可以播放无限次，能让页面更加有趣，更吸引用户眼球。。demo RELLAX.js - 是一款主打轻量级的纯 JavaScript 视差效果库。demo CountUp.js - 可以用来快速创建以一种更有趣的动画方式显示数值数据。demo Dynamics.js - 可以创建物理运动动画效果 JavaScript 库。demo Mojs - 一个拥有极简的声明式 API ，能够轻松掌控运动轨迹，为运动图形而生的工具库。demo React FLIP Move - 一个翻转移动的库，旨在解决当列表的顺序发生变化时，项目列表动画化的问题。demo tween.js - 是一个JS 平滑动画库。demo Typed.js - 是一个JS 打字动画库。demo vivus - JavaScript库，使SVG绘制动画。demo Choreographer-js - 是一个用于处理复杂动画的简单库。demo minirefresh - 优雅的H5下拉刷新。零依赖。demo 时间 Day.js - 是一个轻量的 JavaScript 时间日期处理库。 dayjs 只有2KB日期库，替代Moment.js，具有相同的现代API。 moment - 是一个日期处理类库,用于解析、检验、操作、以及显示日期。demo date-fns - 现代JavaScript日期实用程序库。demo luxon - 在JS中使用日期和时间的库。demo timesheet.js - 是一个时间展示片段插件。demo date.js - 是一个格式化时间、过去时间展示、解决因时区变更插件。 timeago.js - 格式化时间显示多久以前的插件。 demo rome - 可定制的日期（和时间）选择器。无依赖，可选 UI。 demo moment-timezone - 基于 moment.js 的时区库。demo date - 拥有人性化的 Date() 方法。demo ms.js - 小巧的毫秒转换工具。 其它 hotkeys - 是一个强健的 Javascript 库用于捕获键盘输入和输入的组合键。demo isMobile - 一个检测移动设备的简单JS库。 clipboard.js - 现代复制到剪贴板。没有Flash，gzip压缩只有3KB 。demo translater.js - 这是一个利用HTML注释的页面翻译解决方案。demo scrollama - 为 Scrollytelling 开发的现代、轻量级的 JavaScript 库。demo Push.js - 是一个跨浏览器的Javascript桌面通知插件。demo onlinenetwork - js判断是否断网了。 iNotify - 是一个实现浏览器的 title 闪烁、滚动、声音提示、chrome、等系统通知。demo tesseract.js - 是一个文字识别转换，可以运行在浏览器和服务器NodeJS上。demo Leaflet.js - 是一个开源的移动友好交互式地图 JavaScript 库。体积仅有 33 KB。demo CurrencyFormatter.js - 是一款简单纯JS格式化155种不同国家货币格式库，gzip压缩后仅7KB。demo Feature.js - 是一个快速、简单、轻量级的浏览器功能检测库。它没有任何的依赖，并且 gzip 压缩后仅有1kb。demo screenfull.js - 极小、跨平台的 JavaScript 全屏插件。demo 图表 TOAST UI Chart - 提供了直方图、折线图、散点图、饼图、热力图等多种类型格式的图表，兼容 IE8+ demo Chart.js - Simple HTML5 Charts using the &lt;canvas&gt; tag demo ECharts - A powerful, interactive charting and visualization library for browser demo D3 - Bring data to life with SVG, Canvas and HTML. demo Frappe Charts - Bring data to life with SVG, Canvas and HTML. demo Chartist.js - Simple responsive charts. demo Recharts - Simple responsive charts. demo 加载器 requirejs - JS模块化工具 SeaJS - JavaScript模块加载框架 loadjs - JavaScript模块加载框架 ESL - 浏览器端AMD标准加载器 构建工具 webpack - 前端构建工具 Gulp - 自动化构建工具 Babel - 下一代JavasScript语法编译器 PostCSS - 利用JS插件转换CSS样式的工具 Grunt - JavaScript世界的构建工具 rollup.js - JS模块打包器 webpack-dashboard - webpack开发服务器的CLI仪表板 traceur-compiler - 支持ES6的JS编译器 brunch - 超快的HTML5构建工具 Helium-css - 显示网站中未使用的CSS 测试 mocha - JavaScript 测试框架 ESLint - JavaScript代码检查工具 JSHint - JavaScript语法和风格检查工具 casperjs - 开源的导航脚本处理和测试工具 Nightwatch - 用户界面自动化测试框架 istanbul - JS代码覆盖工具 intern - JavaScript测试系统 benchmark.js - 强大的JavaScript基准库 loadtest - HTTP或WebSockets URL的负载测试 JSCover - JavaScript代码覆盖测量工具 包管理器 yarn - 新的 Hadoop 资源管理器 bower - web包管理器 npm - NodeJS包安装的管理模块 ndm - npm桌面管理器 CDN https://cdnjs.com http://cdnjs.net http://www.jsdelivr.com http://unpkg.com http://rawgit.com http://staticfile.org http://www.bootcdn.cn http://cdn.baomitu.com http://lib.sinaapp.com http://cdn.code.baidu.com http://jscdn.upai.com https://www.asp.net/ajax/cdn http://libs.sun0769.com https://css.net https://developers.google.com/speed/libraries/]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[连接的种类]]></title>
    <url>%2F2011%2F06%2F06%2Fdb-mysql-question-9-1-5-%E8%BF%9E%E6%8E%A5%E7%9A%84%E7%A7%8D%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[题目**：连接的种类参考答案：查询分析器中执行： 123456789--建表table1,table2：create table table1(id int,name varchar(10))create table table2(id int,score int)insert into table1 select 1,&apos;lee&apos;insert into table1 select 2,&apos;zhang&apos;insert into table1 select 4,&apos;wang&apos;insert into table2 select 1,90insert into table2 select 2,100insert into table2 select 3,70 如表: 12345678-------------------------------------------------table1 | table2 |-------------------------------------------------id name |id score |1 lee |1 90|2 zhang| 2 100|4 wang| 3 70|------------------------------------------------- 以下均在查询分析器中执行一、外连接1.概念：包括左向外联接、右向外联接或完整外部联接 2.左连接：left join 或 left outer join(1)左向外联接的结果集包括 LEFT OUTER 子句中指定的左表的所有行，而不仅仅是联接列所匹配的行。如果左表的某行在右表中没有匹配行，则在相关联的结果集行中右表的所有选择列表列均为空值(null)。(2)sql 语句 12345678select * from table1 left join table2 on table1.id=table2.id-------------结果-------------idnameidscore------------------------------1lee1902zhang21004wangNULLNULL------------------------------ 注释：包含table1的所有子句，根据指定条件返回table2相应的字段，不符合的以null显示 3.右连接：right join 或 right outer join(1)右向外联接是左向外联接的反向联接。将返回右表的所有行。如果右表的某行在左表中没有匹配行，则将为左表返回空值。(2)sql 语句 12345678select * from table1 right join table2 on table1.id=table2.id-------------结果-------------idnameidscore------------------------------1lee1902zhang2100NULLNULL370------------------------------ 注释：包含table2的所有子句，根据指定条件返回table1相应的字段，不符合的以null显示 4.完整外部联接:full join 或 full outer join(1)完整外部联接返回左表和右表中的所有行。当某行在另一个表中没有匹配行时，则另一个表的选择列表列包含空值。如果表之间有匹配行，则整个结果集行包含基表的数据值。(2)sql 语句 123456789select * from table1 full join table2 on table1.id=table2.id-------------结果-------------idnameidscore------------------------------1lee1902zhang21004wangNULLNULLNULLNULL370------------------------------ 注释：返回左右连接的和（见上左、右连接） 二、内连接1.概念：内联接是用比较运算符比较要联接列的值的联接 2.内连接：join 或 inner join 3.sql 语句 1234567select * from table1 join table2 on table1.id=table2.id-------------结果-------------idnameidscore------------------------------1lee1902zhang2100------------------------------ 注释：只返回符合条件的table1和table2的列 4.等价（与下列执行效果相同） 12A:select a.*,b.* from table1 a,table2 b where a.id=b.idB:select * from table1 cross join table2 where table1.id=table2.id (注：cross join后加条件只能用where,不能用on) 三、交叉连接(完全) 1.概念：没有 WHERE 子句的交叉联接将产生联接所涉及的表的笛卡尔积。第一个表的行数乘以第二个表的行数等于笛卡尔积结果集的大小。（table1和table2交叉连接产生3*3=9条记录） 2.交叉连接：cross join (不带条件where…) 3.sql语句 1234567891011121314select * from table1 cross join table2-------------结果-------------idnameidscore------------------------------1lee1902zhang1904wang1901lee21002zhang21004wang21001lee3702zhang3704wang370------------------------------ 注释：返回3*3=9条记录，即笛卡尔积 4.等价（与下列执行效果相同） 1A:select * from table1,table2]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[存储过程与触发器的区别]]></title>
    <url>%2F2011%2F06%2F06%2Fdb-mysql-question-9-1-8-%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E4%B8%8E%E8%A7%A6%E5%8F%91%E5%99%A8%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[题目：存储过程与触发器的区别参考答案：触发器与存储过程非常相似，触发器也是SQL语句集，两者唯一的区别是触发器不能用EXECUTE语句调用，而是在用户执行Transact-SQL语句时自动触发（激活）执行。触发器是在一个修改了指定表中的数据时执行的存储过程。通常通过创建触发器来强制实现不同表中的逻辑相关数据的引用完整性和一致性。由于用户不能绕过触发器，所以可以用它来强制实施复杂的业务规则，以确保数据的完整性。触发器不同于存储过程，触发器主要是通过事件执行触发而被执行的，而存储过程可以通过存储过程名称名字而直接调用。当对某一表进行诸如UPDATE、INSERT、DELETE这些操作时，SQLSERVER就会自动执行触发器所定义的SQL语句，从而确保对数据的处理必须符合这些SQL语句所定义的规则。]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库优化的思路]]></title>
    <url>%2F2011%2F06%2F06%2Fdb-mysql-question-9-1-7-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BC%98%E5%8C%96%E7%9A%84%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[题目：数据库优化的思路参考答案：这个我借鉴了慕课上关于数据库优化的课程。 1.SQL语句优化1）应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描。2）应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num is null可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：select id from t where num=0========================= liueleven的评论 =================================不是非我杠精，关于null,isNull,isNotNull其实是要看成本的，是否回表等因素总和考虑，才会决定是要走索引还是走全表扫描 也给大家找了一个作者的博文（MySQL中IS NULL、IS NOT NULL、!=不能用索引？胡扯！），仅供参考！！！ [zhiyong0804d的意见]之所以未把第二条删除还是考虑可能很多人都被误导了。那这样的组织能让大家兼听则明。 3）很多时候用 exists 代替 in 是一个好的选择4）用Where子句替换HAVING 子句 因为HAVING 只会在检索出所有记录之后才对结果集进行过滤 2.索引优化看上文索引 3.数据库结构优化1）范式优化： 比如消除冗余（节省空间。。）2）反范式优化：比如适当加冗余等（减少join）3）拆分表： 分区将数据在物理上分隔开，不同分区的数据可以制定保存在处于不同磁盘上的数据文件里。这样，当对这个表进行查询时，只需要在表分区中进行扫描，而不必进行全表扫描，明显缩短了查询时间，另外处于不同磁盘的分区也将对这个表的数据传输分散在不同的磁盘I/O，一个精心设置的分区可以将数据传输对磁盘I/O竞争均匀地分散开。对数据量大的时时表可采取此方法。可按月自动建表分区。4）拆分其实又分垂直拆分和水平拆分： 案例： 简单购物系统暂设涉及如下表： 1.产品表（数据量10w，稳定） 2.订单表（数据量200w，且有增长趋势） 3.用户表 （数据量100w，且有增长趋势） 以mysql为例讲述下水平拆分和垂直拆分，mysql能容忍的数量级在百万静态数据可以到千万 垂直拆分：解决问题：表与表之间的io竞争 不解决问题：单表中数据量增长出现的压力 方案： 把产品表和用户表放到一个server上 订单表单独放到一个server上 水平拆分： 解决问题：单表中数据量增长出现的压力 不解决问题：表与表之间的io争夺方案： 用户表通过性别拆分为男用户表和女用户表 订单表通过已完成和完成中拆分为已完成订单和未完成订单 产品表 未完成订单放一个server上 已完成订单表盒男用户表放一个server上 女用户表放一个server上(女的爱购物 哈哈) 4.服务器硬件优化这个么多花钱咯！]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql执行原理]]></title>
    <url>%2F2011%2F04%2F05%2Fdb-mysql-%E4%B8%80%E6%9D%A1sql%E8%AF%AD%E5%8F%A5%E5%9C%A8mysql%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84%2F</url>
    <content type="text"><![CDATA[本篇文章会分析下一个 sql 语句在 MySQL 中的执行流程，包括 sql 的查询在 MySQL 内部会怎么流转，sql 语句的更新是怎么完成的。 在分析之前我会先带着你看看 MySQL 的基础架构，知道了 MySQL 由那些组件组成已经这些组件的作用是什么，可以帮助我们理解和解决这些问题。 一 MySQL 基础架构分析1.1 MySQL 基本架构概览下图是 MySQL 的一个简要架构图，从下图你可以很清晰的看到用户的 SQL 语句在 MySQL 内部是如何执行的。 先简单介绍一下下图涉及的一些组件的基本作用帮助大家理解这幅图，在 1.2 节中会详细介绍到这些组件的作用。 连接器： 身份认证和权限相关(登录 MySQL 的时候)。 查询缓存: 执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。 分析器: 没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的 SQL 语句要干嘛，再检查你的 SQL 语句语法是否正确。 优化器： 按照 MySQL 认为最优的方案去执行。 执行器: 执行语句，然后从存储引擎返回数据。 简单来说 MySQL 主要分为 Server 层和存储引擎层： Server 层：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binglog 日志模块。 存储引擎： 主要负责数据的存储和读取，采用可以替换的插件式架构，支持 InnoDB、MyISAM、Memory 等多个存储引擎，其中 InnoDB 引擎有自有的日志模块 redolog 模块。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始就被当做默认存储引擎了。 1.2 Server 层基本组件介绍1) 连接器连接器主要和身份认证和权限相关的功能相关，就好比一个级别很高的门卫一样。 主要负责用户登录数据库，进行用户的身份认证，包括校验账户密码，权限等操作，如果用户账户密码已通过，连接器会到权限表中查询该用户的所有权限，之后在这个连接里的权限逻辑判断都是会依赖此时读取到的权限数据，也就是说，后续只要这个连接不断开，即时管理员修改了该用户的权限，该用户也是不受影响的。 2) 查询缓存(MySQL 8.0 版本后移除)查询缓存主要用来缓存我们所执行的 SELECT 语句以及该语句的结果集。 连接建立后，执行查询语句的时候，会先查询缓存，MySQL 会先校验这个 sql 是否执行过，以 Key-Value 的形式缓存在内存中，Key 是查询预计，Value 是结果集。如果缓存 key 被命中，就会直接返回给客户端，如果没有命中，就会执行后续的操作，完成后也会把结果缓存起来，方便下一次调用。当然在真正执行缓存查询的时候还是会校验用户的权限，是否有该表的查询条件。 MySQL 查询不建议使用缓存，因为查询缓存失效在实际业务场景中可能会非常频繁，假如你对一个表更新的话，这个表上的所有的查询缓存都会被清空。对于不经常更新的数据来说，使用缓存还是可以的。 所以，一般在大多数情况下我们都是不推荐去使用查询缓存的。 MySQL 8.0 版本后删除了缓存的功能，官方也是认为该功能在实际的应用场景比较少，所以干脆直接删掉了。 3) 分析器MySQL 没有命中缓存，那么就会进入分析器，分析器主要是用来分析 SQL 语句是来干嘛的，分析器也会分为几步： 第一步，词法分析，一条 SQL 语句有多个字符串组成，首先要提取关键字，比如 select，提出查询的表，提出字段名，提出查询条件等等。做完这些操作后，就会进入第二步。 第二步，语法分析，主要就是判断你输入的 sql 是否正确，是否符合 MySQL 的语法。 完成这 2 步之后，MySQL 就准备开始执行了，但是如何执行，怎么执行是最好的结果呢？这个时候就需要优化器上场了。 4) 优化器优化器的作用就是它认为的最优的执行方案去执行（有时候可能也不是最优，这篇文章涉及对这部分知识的深入讲解），比如多个索引的时候该如何选择索引，多表查询的时候如何选择关联顺序等。 可以说，经过了优化器之后可以说这个语句具体该如何执行就已经定下来。 5) 执行器当选择了执行方案后，MySQL 就准备开始执行了，首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会去调用引擎的接口，返回接口执行的结果。 二 语句分析2.1 查询语句说了以上这么多，那么究竟一条 sql 语句是如何执行的呢？其实我们的 sql 可以分为两种，一种是查询，一种是更新（增加，更新，删除）。我们先分析下查询语句，语句如下： 1select * from tb_student A where A.age='18' and A.name=' 张三 '; 结合上面的说明，我们分析下这个语句的执行流程： 先检查该语句是否有权限，如果没有权限，直接返回错误信息，如果有权限，在 MySQL8.0 版本以前，会先查询缓存，以这条 sql 语句为 key 在内存中查询是否有结果，如果有直接缓存，如果没有，执行下一步。 通过分析器进行词法分析，提取 sql 语句的关键元素，比如提取上面这个语句是查询 select，提取需要查询的表名为 tb_student,需要查询所有的列，查询条件是这个表的 id=’1’。然后判断这个 sql 语句是否有语法错误，比如关键词是否正确等等，如果检查没问题就执行下一步。 接下来就是优化器进行确定执行方案，上面的 sql 语句，可以有两种执行方案： a.先查询学生表中姓名为“张三”的学生，然后判断是否年龄是 18。 b.先找出学生中年龄 18 岁的学生，然后再查询姓名为“张三”的学生。 那么优化器根据自己的优化算法进行选择执行效率最好的一个方案（优化器认为，有时候不一定最好）。那么确认了执行计划后就准备开始执行了。 进行权限校验，如果没有权限就会返回错误信息，如果有权限就会调用数据库引擎接口，返回引擎的执行结果。 2.2 更新语句以上就是一条查询 sql 的执行流程，那么接下来我们看看一条更新语句如何执行的呢？sql 语句如下： 1update tb_student A set A.age=&apos;19&apos; where A.name=&apos; 张三 &apos;; 我们来给张三修改下年龄，在实际数据库肯定不会设置年龄这个字段的，不然要被技术负责人打的。其实条语句也基本上会沿着上一个查询的流程走，只不过执行更新的时候肯定要记录日志啦，这就会引入日志模块了，MySQL 自带的日志模块式 binlog（归档日志） ，所有的存储引擎都可以使用，我们常用的 InnoDB 引擎还自带了一个日志模块 redo log（重做日志），我们就以 InnoDB 模式下来探讨这个语句的执行流程。流程如下： 先查询到张三这一条数据，如果有缓存，也是会用到缓存。 然后拿到查询的语句，把 age 改为 19，然后调用引擎 API 接口，写入这一行数据，InnoDB 引擎把数据保存在内存中，同时记录 redo log，此时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，随时可以提交。 执行器收到通知后记录 binlog，然后调用引擎接口，提交 redo log 为提交状态。 更新完成。 这里肯定有同学会问，为什么要用两个日志模块，用一个日志模块不行吗? 这是因为最开始 MySQL 并没与 InnoDB 引擎( InnoDB 引擎是其他公司以插件形式插入 MySQL 的) ，MySQL 自带的引擎是 MyISAM，但是我们知道 redo log 是 InnoDB 引擎特有的，其他存储引擎都没有，这就导致会没有 crash-safe 的能力(crash-safe 的能力即使数据库发生异常重启，之前提交的记录都不会丢失)，binlog 日志只能用来归档。 并不是说只用一个日志模块不可以，只是 InnoDB 引擎就是通过 redo log 来支持事务的。那么，又会有同学问，我用两个日志模块，但是不要这么复杂行不行，为什么 redo log 要引入 prepare 预提交状态？这里我们用反证法来说明下为什么要这么做？ 先写 redo log 直接提交，然后写 binlog，假设写完 redo log 后，机器挂了，binlog 日志没有被写入，那么机器重启后，这台机器会通过 redo log 恢复数据，但是这个时候 bingog 并没有记录该数据，后续进行机器备份的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。 先写 binlog，然后写 redo log，假设写完了 binlog，机器异常重启了，由于没有 redo log，本机是无法恢复这一条记录的，但是 binlog 又有记录，那么和上面同样的道理，就会产生数据不一致的情况。 如果采用 redo log 两阶段提交的方式就不一样了，写完 binglog 后，然后再提交 redo log 就会防止出现上述的问题，从而保证了数据的一致性。那么问题来了，有没有一个极端的情况呢？假设 redo log 处于预提交状态，binglog 也已经写完了，这个时候发生了异常重启会怎么样呢？这个就要依赖于 MySQL 的处理机制了，MySQL 的处理过程如下： 判断 redo log 是否完整，如果判断是完整的，就立即提交。 如果 redo log 只是预提交但不是 commit 状态，这个时候就会去判断 binlog 是否完整，如果完整就提交 redo log, 不完整就回滚事务。 这样就解决了数据一致性的问题。 三 总结 MySQL 主要分为 Server 层和引擎层，Server 层主要包括连接器、查询缓存、分析器、优化器、执行器，同时还有一个日志模块（binlog），这个日志模块所有执行引擎都可以共用,redolog 只有 InnoDB 有。 引擎层是插件式的，目前主要包括，MyISAM,InnoDB,Memory 等。 查询语句的执行流程如下：权限校验（如果命中缓存）—》查询缓存—》分析器—》优化器—》权限校验—》执行器—》引擎 更新语句执行流程如下：分析器—-》权限校验—-》执行器—》引擎—redo log(prepare 状态—》binlog—》redo log(commit状态) 四 参考 《MySQL 实战45讲》 MySQL 5.6参考手册:https://dev.MySQL.com/doc/refman/5.6/en/]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[索引的工作原理及其种类]]></title>
    <url>%2F2011%2F04%2F05%2Fdb-mysql-question-9-1-4-%E7%B4%A2%E5%BC%95%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E7%A7%8D%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[题目**：索引的工作原理及其种类参考答案：数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。 在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。 为表设置索引要付出代价的：一是增加了数据库的存储空间，二是在插入和修改数据时要花费较多的时间(因为索引也要随之变动)。 图展示了一种可能的索引方式。左边是数据表，一共有两列七条记录，最左边的是数据记录的物理地址（注意逻辑上相邻的记录在磁盘上也并不是一定物理相邻的）。为了加快Col2的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找在O(log2n)的复杂度内获取到相应数据。 创建索引可以大大提高系统的性能。 第一，通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 第二，可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 第三，可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 第四，在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。 第五，通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 也许会有人要问：增加索引有如此多的优点，为什么不对表中的每一个列创建一个索引呢？因为，增加索引也有许多不利的方面。 第一，创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 第二，索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 第三，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 索引是建立在数据库表中的某些列的上面。在创建索引的时候，应该考虑在哪些列上可以创建索引，在哪些列上不能创建索引。一般来说，应该在这些列上创建索引：在经常需要搜索的列上，可以加快搜索的速度；在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构；在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度；在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的；在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间；在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。 同样，对于有些列不应该创建索引。一般来说，不应该创建索引的的这些列具有下列特点： 第一，对于那些在查询中很少使用或者参考的列不应该创建索引。这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 第二，对于那些只有很少数据值的列也不应该增加索引。这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。 第三，对于那些定义为text, image和bit数据类型的列不应该增加索引。这是因为，这些列的数据量要么相当大，要么取值很少。 第四，当修改性能远远大于检索性能时，不应该创建索引。这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。 根据数据库的功能，可以在数据库设计器中创建三种索引：唯一索引、主键索引和聚集索引。 唯一索引 唯一索引是不允许其中任何两行具有相同索引值的索引。 当现有数据中存在重复的键值时，大多数数据库不允许将新创建的唯一索引与表一起保存。数据库还可能防止添加将在表中创建重复键值的新数据。例如，如果在employee表中职员的姓(lname)上创建了唯一索引，则任何两个员工都不能同姓。 主键索引 数据库表经常有一列或列组合，其值唯一标识表中的每一行。该列称为表的主键。 在数据库关系图中为表定义主键将自动创建主键索引，主键索引是唯一索引的特定类型。该索引要求主键中的每个值都唯一。当在查询中使用主键索引时，它还允许对数据的快速访问。 聚集索引 在聚集索引中，表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表只能包含一个聚集索引。 如果某索引不是聚集索引，则表中行的物理顺序与键值的逻辑顺序不匹配。与非聚集索引相比，聚集索引通常提供更快的数据访问速度。 局部性原理与磁盘预读由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 B-/+Tree索引的性能分析到这里终于可以分析B-/+Tree索引的性能了。 上文说过一般使用磁盘I/O次数评价索引结构的优劣。先从B-Tree分析，根据B-Tree的定义，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧： 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。 而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。 综上所述，用B-Tree作为索引结构效率是非常高的。]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一千行MySQL命令]]></title>
    <url>%2F2011%2F03%2F06%2Fdb-mysql-%E4%B8%80%E5%8D%83%E8%A1%8CMySQL%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[基本操作123456789/* Windows服务 */-- 启动MySQL net start mysql-- 创建Windows服务 sc create mysql binPath= mysqld_bin_path(注意：等号与值之间有空格)/* 连接与断开服务器 */mysql -h 地址 -P 端口 -u 用户名 -p 密码SHOW PROCESSLIST -- 显示哪些线程正在运行SHOW VARIABLES -- 显示系统变量信息 数据库操作12345678910111213141516171819/* 数据库操作 */ -------------------- 查看当前数据库 SELECT DATABASE();-- 显示当前时间、用户名、数据库版本 SELECT now(), user(), version();-- 创建库 CREATE DATABASE[ IF NOT EXISTS] 数据库名 数据库选项 数据库选项： CHARACTER SET charset_name COLLATE collation_name-- 查看已有库 SHOW DATABASES[ LIKE &apos;PATTERN&apos;]-- 查看当前库信息 SHOW CREATE DATABASE 数据库名-- 修改库的选项信息 ALTER DATABASE 库名 选项信息-- 删除库 DROP DATABASE[ IF EXISTS] 数据库名 同时删除该数据库相关的目录及其目录内容 表的操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576-- 创建表 CREATE [TEMPORARY] TABLE[ IF NOT EXISTS] [库名.]表名 ( 表的结构定义 )[ 表选项] 每个字段必须有数据类型 最后一个字段后不能有逗号 TEMPORARY 临时表，会话结束时表自动消失 对于字段的定义： 字段名 数据类型 [NOT NULL | NULL] [DEFAULT default_value] [AUTO_INCREMENT] [UNIQUE [KEY] | [PRIMARY] KEY] [COMMENT &apos;string&apos;]-- 表选项 -- 字符集 CHARSET = charset_name 如果表没有设定，则使用数据库字符集 -- 存储引擎 ENGINE = engine_name 表在管理数据时采用的不同的数据结构，结构不同会导致处理方式、提供的特性操作等不同 常见的引擎：InnoDB MyISAM Memory/Heap BDB Merge Example CSV MaxDB Archive 不同的引擎在保存表的结构和数据时采用不同的方式 MyISAM表文件含义：.frm表定义，.MYD表数据，.MYI表索引 InnoDB表文件含义：.frm表定义，表空间数据和日志文件 SHOW ENGINES -- 显示存储引擎的状态信息 SHOW ENGINE 引擎名 &#123;LOGS|STATUS&#125; -- 显示存储引擎的日志或状态信息 -- 自增起始数 AUTO_INCREMENT = 行数 -- 数据文件目录 DATA DIRECTORY = &apos;目录&apos; -- 索引文件目录 INDEX DIRECTORY = &apos;目录&apos; -- 表注释 COMMENT = &apos;string&apos; -- 分区选项 PARTITION BY ... (详细见手册)-- 查看所有表 SHOW TABLES[ LIKE &apos;pattern&apos;] SHOW TABLES FROM 库名-- 查看表机构 SHOW CREATE TABLE 表名 （信息更详细） DESC 表名 / DESCRIBE 表名 / EXPLAIN 表名 / SHOW COLUMNS FROM 表名 [LIKE &apos;PATTERN&apos;] SHOW TABLE STATUS [FROM db_name] [LIKE &apos;pattern&apos;]-- 修改表 -- 修改表本身的选项 ALTER TABLE 表名 表的选项 eg: ALTER TABLE 表名 ENGINE=MYISAM; -- 对表进行重命名 RENAME TABLE 原表名 TO 新表名 RENAME TABLE 原表名 TO 库名.表名 （可将表移动到另一个数据库） -- RENAME可以交换两个表名 -- 修改表的字段机构（13.1.2. ALTER TABLE语法） ALTER TABLE 表名 操作名 -- 操作名 ADD[ COLUMN] 字段定义 -- 增加字段 AFTER 字段名 -- 表示增加在该字段名后面 FIRST -- 表示增加在第一个 ADD PRIMARY KEY(字段名) -- 创建主键 ADD UNIQUE [索引名] (字段名)-- 创建唯一索引 ADD INDEX [索引名] (字段名) -- 创建普通索引 DROP[ COLUMN] 字段名 -- 删除字段 MODIFY[ COLUMN] 字段名 字段属性 -- 支持对字段属性进行修改，不能修改字段名(所有原有属性也需写上) CHANGE[ COLUMN] 原字段名 新字段名 字段属性 -- 支持对字段名修改 DROP PRIMARY KEY -- 删除主键(删除主键前需删除其AUTO_INCREMENT属性) DROP INDEX 索引名 -- 删除索引 DROP FOREIGN KEY 外键 -- 删除外键-- 删除表 DROP TABLE[ IF EXISTS] 表名 ...-- 清空表数据 TRUNCATE [TABLE] 表名-- 复制表结构 CREATE TABLE 表名 LIKE 要复制的表名-- 复制表结构和数据 CREATE TABLE 表名 [AS] SELECT * FROM 要复制的表名-- 检查表是否有错误 CHECK TABLE tbl_name [, tbl_name] ... [option] ...-- 优化表 OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ...-- 修复表 REPAIR [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... [QUICK] [EXTENDED] [USE_FRM]-- 分析表 ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 数据操作1234567891011121314151617/* 数据操作 */ -------------------- 增 INSERT [INTO] 表名 [(字段列表)] VALUES (值列表)[, (值列表), ...] -- 如果要插入的值列表包含所有字段并且顺序一致，则可以省略字段列表。 -- 可同时插入多条数据记录！ REPLACE 与 INSERT 完全一样，可互换。 INSERT [INTO] 表名 SET 字段名=值[, 字段名=值, ...]-- 查 SELECT 字段列表 FROM 表名[ 其他子句] -- 可来自多个表的多个字段 -- 其他子句可以不使用 -- 字段列表可以用*代替，表示所有字段-- 删 DELETE FROM 表名[ 删除条件子句] 没有条件子句，则会删除全部-- 改 UPDATE 表名 SET 字段名=新值[, 字段名=新值] [更新条件] 字符集编码123456789101112131415161718/* 字符集编码 */ -------------------- MySQL、数据库、表、字段均可设置编码-- 数据编码与客户端编码不需一致SHOW VARIABLES LIKE &apos;character_set_%&apos; -- 查看所有字符集编码项 character_set_client 客户端向服务器发送数据时使用的编码 character_set_results 服务器端将结果返回给客户端所使用的编码 character_set_connection 连接层编码SET 变量名 = 变量值 SET character_set_client = gbk; SET character_set_results = gbk; SET character_set_connection = gbk;SET NAMES GBK; -- 相当于完成以上三个设置-- 校对集 校对集用以排序 SHOW CHARACTER SET [LIKE &apos;pattern&apos;]/SHOW CHARSET [LIKE &apos;pattern&apos;] 查看所有字符集 SHOW COLLATION [LIKE &apos;pattern&apos;] 查看所有校对集 CHARSET 字符集编码 设置字符集编码 COLLATE 校对集编码 设置校对集编码 数据类型(列类型)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697/* 数据类型（列类型） */ ------------------1. 数值类型-- a. 整型 ---------- 类型 字节 范围（有符号位） tinyint 1字节 -128 ~ 127 无符号位：0 ~ 255 smallint 2字节 -32768 ~ 32767 mediumint 3字节 -8388608 ~ 8388607 int 4字节 bigint 8字节 int(M) M表示总位数 - 默认存在符号位，unsigned 属性修改 - 显示宽度，如果某个数不够定义字段时设置的位数，则前面以0补填，zerofill 属性修改 例：int(5) 插入一个数&apos;123&apos;，补填后为&apos;00123&apos; - 在满足要求的情况下，越小越好。 - 1表示bool值真，0表示bool值假。MySQL没有布尔类型，通过整型0和1表示。常用tinyint(1)表示布尔型。-- b. 浮点型 ---------- 类型 字节 范围 float(单精度) 4字节 double(双精度) 8字节 浮点型既支持符号位 unsigned 属性，也支持显示宽度 zerofill 属性。 不同于整型，前后均会补填0. 定义浮点型时，需指定总位数和小数位数。 float(M, D) double(M, D) M表示总位数，D表示小数位数。 M和D的大小会决定浮点数的范围。不同于整型的固定范围。 M既表示总位数（不包括小数点和正负号），也表示显示宽度（所有显示符号均包括）。 支持科学计数法表示。 浮点数表示近似值。-- c. 定点数 ---------- decimal -- 可变长度 decimal(M, D) M也表示总位数，D表示小数位数。 保存一个精确的数值，不会发生数据的改变，不同于浮点数的四舍五入。 将浮点数转换为字符串来保存，每9位数字保存为4个字节。2. 字符串类型-- a. char, varchar ---------- char 定长字符串，速度快，但浪费空间 varchar 变长字符串，速度慢，但节省空间 M表示能存储的最大长度，此长度是字符数，非字节数。 不同的编码，所占用的空间不同。 char,最多255个字符，与编码无关。 varchar,最多65535字符，与编码有关。 一条有效记录最大不能超过65535个字节。 utf8 最大为21844个字符，gbk 最大为32766个字符，latin1 最大为65532个字符 varchar 是变长的，需要利用存储空间保存 varchar 的长度，如果数据小于255个字节，则采用一个字节来保存长度，反之需要两个字节来保存。 varchar 的最大有效长度由最大行大小和使用的字符集确定。 最大有效长度是65532字节，因为在varchar存字符串时，第一个字节是空的，不存在任何数据，然后还需两个字节来存放字符串的长度，所以有效长度是64432-1-2=65532字节。 例：若一个表定义为 CREATE TABLE tb(c1 int, c2 char(30), c3 varchar(N)) charset=utf8; 问N的最大值是多少？ 答：(65535-1-2-4-30*3)/3-- b. blob, text ---------- blob 二进制字符串（字节字符串） tinyblob, blob, mediumblob, longblob text 非二进制字符串（字符字符串） tinytext, text, mediumtext, longtext text 在定义时，不需要定义长度，也不会计算总长度。 text 类型在定义时，不可给default值-- c. binary, varbinary ---------- 类似于char和varchar，用于保存二进制字符串，也就是保存字节字符串而非字符字符串。 char, varchar, text 对应 binary, varbinary, blob.3. 日期时间类型 一般用整型保存时间戳，因为PHP可以很方便的将时间戳进行格式化。 datetime 8字节 日期及时间 1000-01-01 00:00:00 到 9999-12-31 23:59:59 date 3字节 日期 1000-01-01 到 9999-12-31 timestamp 4字节 时间戳 19700101000000 到 2038-01-19 03:14:07 time 3字节 时间 -838:59:59 到 838:59:59 year 1字节 年份 1901 - 2155datetime YYYY-MM-DD hh:mm:sstimestamp YY-MM-DD hh:mm:ss YYYYMMDDhhmmss YYMMDDhhmmss YYYYMMDDhhmmss YYMMDDhhmmssdate YYYY-MM-DD YY-MM-DD YYYYMMDD YYMMDD YYYYMMDD YYMMDDtime hh:mm:ss hhmmss hhmmssyear YYYY YY YYYY YY4. 枚举和集合-- 枚举(enum) ----------enum(val1, val2, val3...) 在已知的值中进行单选。最大数量为65535. 枚举值在保存时，以2个字节的整型(smallint)保存。每个枚举值，按保存的位置顺序，从1开始逐一递增。 表现为字符串类型，存储却是整型。 NULL值的索引是NULL。 空字符串错误值的索引值是0。-- 集合（set） ----------set(val1, val2, val3...) create table tab ( gender set(&apos;男&apos;, &apos;女&apos;, &apos;无&apos;) ); insert into tab values (&apos;男, 女&apos;); 最多可以有64个不同的成员。以bigint存储，共8个字节。采取位运算的形式。 当创建表时，SET成员值的尾部空格将自动被删除。 列属性(列约束)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/* 列属性（列约束） */ ------------------1. PRIMARY 主键 - 能唯一标识记录的字段，可以作为主键。 - 一个表只能有一个主键。 - 主键具有唯一性。 - 声明字段时，用 primary key 标识。 也可以在字段列表之后声明 例：create table tab ( id int, stu varchar(10), primary key (id)); - 主键字段的值不能为null。 - 主键可以由多个字段共同组成。此时需要在字段列表后声明的方法。 例：create table tab ( id int, stu varchar(10), age int, primary key (stu, age));2. UNIQUE 唯一索引（唯一约束） 使得某字段的值也不能重复。3. NULL 约束 null不是数据类型，是列的一个属性。 表示当前列是否可以为null，表示什么都没有。 null, 允许为空。默认。 not null, 不允许为空。 insert into tab values (null, &apos;val&apos;); -- 此时表示将第一个字段的值设为null, 取决于该字段是否允许为null4. DEFAULT 默认值属性 当前字段的默认值。 insert into tab values (default, &apos;val&apos;); -- 此时表示强制使用默认值。 create table tab ( add_time timestamp default current_timestamp ); -- 表示将当前时间的时间戳设为默认值。 current_date, current_time5. AUTO_INCREMENT 自动增长约束 自动增长必须为索引（主键或unique） 只能存在一个字段为自动增长。 默认为1开始自动增长。可以通过表属性 auto_increment = x进行设置，或 alter table tbl auto_increment = x;6. COMMENT 注释 例：create table tab ( id int ) comment &apos;注释内容&apos;;7. FOREIGN KEY 外键约束 用于限制主表与从表数据完整性。 alter table t1 add constraint `t1_t2_fk` foreign key (t1_id) references t2(id); -- 将表t1的t1_id外键关联到表t2的id字段。 -- 每个外键都有一个名字，可以通过 constraint 指定 存在外键的表，称之为从表（子表），外键指向的表，称之为主表（父表）。 作用：保持数据一致性，完整性，主要目的是控制存储在外键表（从表）中的数据。 MySQL中，可以对InnoDB引擎使用外键约束： 语法： foreign key (外键字段） references 主表名 (关联字段) [主表记录删除时的动作] [主表记录更新时的动作] 此时需要检测一个从表的外键需要约束为主表的已存在的值。外键在没有关联的情况下，可以设置为null.前提是该外键列，没有not null。 可以不指定主表记录更改或更新时的动作，那么此时主表的操作被拒绝。 如果指定了 on update 或 on delete：在删除或更新时，有如下几个操作可以选择： 1. cascade，级联操作。主表数据被更新（主键值更新），从表也被更新（外键值更新）。主表记录被删除，从表相关记录也被删除。 2. set null，设置为null。主表数据被更新（主键值更新），从表的外键被设置为null。主表记录被删除，从表相关记录外键被设置成null。但注意，要求该外键列，没有not null属性约束。 3. restrict，拒绝父表删除和更新。 注意，外键只被InnoDB存储引擎所支持。其他引擎是不支持的。 建表规范1234567891011121314/* 建表规范 */ ------------------ -- Normal Format, NF - 每个表保存一个实体信息 - 每个具有一个ID字段作为主键 - ID主键 + 原子表 -- 1NF, 第一范式 字段不能再分，就满足第一范式。 -- 2NF, 第二范式 满足第一范式的前提下，不能出现部分依赖。 消除符合主键就可以避免部分依赖。增加单列关键字。 -- 3NF, 第三范式 满足第二范式的前提下，不能出现传递依赖。 某个字段依赖于主键，而有其他字段依赖于该字段。这就是传递依赖。 将一个实体信息的数据放在一个表内实现。 SELECT123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/* SELECT */ ------------------SELECT [ALL|DISTINCT] select_expr FROM -&gt; WHERE -&gt; GROUP BY [合计函数] -&gt; HAVING -&gt; ORDER BY -&gt; LIMITa. select_expr -- 可以用 * 表示所有字段。 select * from tb; -- 可以使用表达式（计算公式、函数调用、字段也是个表达式） select stu, 29+25, now() from tb; -- 可以为每个列使用别名。适用于简化列标识，避免多个列标识符重复。 - 使用 as 关键字，也可省略 as. select stu+10 as add10 from tb;b. FROM 子句 用于标识查询来源。 -- 可以为表起别名。使用as关键字。 SELECT * FROM tb1 AS tt, tb2 AS bb; -- from子句后，可以同时出现多个表。 -- 多个表会横向叠加到一起，而数据会形成一个笛卡尔积。 SELECT * FROM tb1, tb2; -- 向优化符提示如何选择索引 USE INDEX、IGNORE INDEX、FORCE INDEX SELECT * FROM table1 USE INDEX (key1,key2) WHERE key1=1 AND key2=2 AND key3=3; SELECT * FROM table1 IGNORE INDEX (key3) WHERE key1=1 AND key2=2 AND key3=3;c. WHERE 子句 -- 从from获得的数据源中进行筛选。 -- 整型1表示真，0表示假。 -- 表达式由运算符和运算数组成。 -- 运算数：变量（字段）、值、函数返回值 -- 运算符： =, &lt;=&gt;, &lt;&gt;, !=, &lt;=, &lt;, &gt;=, &gt;, !, &amp;&amp;, ||, in (not) null, (not) like, (not) in, (not) between and, is (not), and, or, not, xor is/is not 加上ture/false/unknown，检验某个值的真假 &lt;=&gt;与&lt;&gt;功能相同，&lt;=&gt;可用于null比较d. GROUP BY 子句, 分组子句 GROUP BY 字段/别名 [排序方式] 分组后会进行排序。升序：ASC，降序：DESC 以下[合计函数]需配合 GROUP BY 使用： count 返回不同的非NULL值数目 count(*)、count(字段) sum 求和 max 求最大值 min 求最小值 avg 求平均值 group_concat 返回带有来自一个组的连接的非NULL值的字符串结果。组内字符串连接。e. HAVING 子句，条件子句 与 where 功能、用法相同，执行时机不同。 where 在开始时执行检测数据，对原数据进行过滤。 having 对筛选出的结果再次进行过滤。 having 字段必须是查询出来的，where 字段必须是数据表存在的。 where 不可以使用字段的别名，having 可以。因为执行WHERE代码时，可能尚未确定列值。 where 不可以使用合计函数。一般需用合计函数才会用 having SQL标准要求HAVING必须引用GROUP BY子句中的列或用于合计函数中的列。f. ORDER BY 子句，排序子句 order by 排序字段/别名 排序方式 [,排序字段/别名 排序方式]... 升序：ASC，降序：DESC 支持多个字段的排序。g. LIMIT 子句，限制结果数量子句 仅对处理好的结果进行数量限制。将处理好的结果的看作是一个集合，按照记录出现的顺序，索引从0开始。 limit 起始位置, 获取条数 省略第一个参数，表示从索引0开始。limit 获取条数h. DISTINCT, ALL 选项 distinct 去除重复记录 默认为 all, 全部记录 UNION12345678/* UNION */ ------------------ 将多个select查询的结果组合成一个结果集合。 SELECT ... UNION [ALL|DISTINCT] SELECT ... 默认 DISTINCT 方式，即所有返回的行都是唯一的 建议，对每个SELECT查询加上小括号包裹。 ORDER BY 排序时，需加上 LIMIT 进行结合。 需要各select查询的字段数量一样。 每个select查询的字段列表(数量、类型)应一致，因为结果中的字段名以第一条select语句为准。 子查询1234567891011121314151617181920212223242526272829/* 子查询 */ ------------------ - 子查询需用括号包裹。-- from型 from后要求是一个表，必须给子查询结果取个别名。 - 简化每个查询内的条件。 - from型需将结果生成一个临时表格，可用以原表的锁定的释放。 - 子查询返回一个表，表型子查询。 select * from (select * from tb where id&gt;0) as subfrom where id&gt;1;-- where型 - 子查询返回一个值，标量子查询。 - 不需要给子查询取别名。 - where子查询内的表，不能直接用以更新。 select * from tb where money = (select max(money) from tb); -- 列子查询 如果子查询结果返回的是一列。 使用 in 或 not in 完成查询 exists 和 not exists 条件 如果子查询返回数据，则返回1或0。常用于判断条件。 select column1 from t1 where exists (select * from t2); -- 行子查询 查询条件是一个行。 select * from t1 where (id, gender) in (select id, gender from t2); 行构造符：(col1, col2, ...) 或 ROW(col1, col2, ...) 行构造符通常用于与对能返回两个或两个以上列的子查询进行比较。 -- 特殊运算符 != all() 相当于 not in = some() 相当于 in。any 是 some 的别名 != some() 不等同于 not in，不等于其中某一个。 all, some 可以配合其他运算符一起使用。 连接查询(join)123456789101112131415161718192021222324/* 连接查询(join) */ ------------------ 将多个表的字段进行连接，可以指定连接条件。-- 内连接(inner join) - 默认就是内连接，可省略inner。 - 只有数据存在时才能发送连接。即连接结果不能出现空行。 on 表示连接条件。其条件表达式与where类似。也可以省略条件（表示条件永远为真） 也可用where表示连接条件。 还有 using, 但需字段名相同。 using(字段名) -- 交叉连接 cross join 即，没有条件的内连接。 select * from tb1 cross join tb2;-- 外连接(outer join) - 如果数据不存在，也会出现在连接结果中。 -- 左外连接 left join 如果数据不存在，左表记录会出现，而右表为null填充 -- 右外连接 right join 如果数据不存在，右表记录会出现，而左表为null填充-- 自然连接(natural join) 自动判断连接条件完成连接。 相当于省略了using，会自动查找相同字段名。 natural join natural left join natural right joinselect info.id, info.name, info.stu_num, extra_info.hobby, extra_info.sex from info, extra_info where info.stu_num = extra_info.stu_id; TRUNCATE123456789/* TRUNCATE */ ------------------TRUNCATE [TABLE] tbl_name清空数据删除重建表区别：1，truncate 是删除表再创建，delete 是逐条删除2，truncate 重置auto_increment的值。而delete不会3，truncate 不知道删除了几条，而delete知道。4，当被用于带分区的表时，truncate 会保留分区 备份与还原123456789101112131415161718192021/* 备份与还原 */ ------------------备份，将数据的结构与表内数据保存起来。利用 mysqldump 指令完成。-- 导出mysqldump [options] db_name [tables]mysqldump [options] ---database DB1 [DB2 DB3...]mysqldump [options] --all--database1. 导出一张表 mysqldump -u用户名 -p密码 库名 表名 &gt; 文件名(D:/a.sql)2. 导出多张表 mysqldump -u用户名 -p密码 库名 表1 表2 表3 &gt; 文件名(D:/a.sql)3. 导出所有表 mysqldump -u用户名 -p密码 库名 &gt; 文件名(D:/a.sql)4. 导出一个库 mysqldump -u用户名 -p密码 --lock-all-tables --database 库名 &gt; 文件名(D:/a.sql)可以-w携带WHERE条件-- 导入1. 在登录mysql的情况下： source 备份文件2. 在不登录的情况下 mysql -u用户名 -p密码 库名 &lt; 备份文件 视图1234567891011121314151617181920212223242526272829什么是视图： 视图是一个虚拟表，其内容由查询定义。同真实的表一样，视图包含一系列带有名称的列和行数据。但是，视图并不在数据库中以存储的数据值集形式存在。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。 视图具有表结构文件，但不存在数据文件。 对其中所引用的基础表来说，视图的作用类似于筛选。定义视图的筛选可以来自当前或其它数据库的一个或多个表，或者其它视图。通过视图进行查询没有任何限制，通过它们进行数据修改时的限制也很少。 视图是存储在数据库中的查询的sql语句，它主要出于两种原因：安全原因，视图可以隐藏一些数据，如：社会保险基金表，可以用视图只显示姓名，地址，而不显示社会保险号和工资数等，另一原因是可使复杂的查询易于理解和使用。-- 创建视图CREATE [OR REPLACE] [ALGORITHM = &#123;UNDEFINED | MERGE | TEMPTABLE&#125;] VIEW view_name [(column_list)] AS select_statement - 视图名必须唯一，同时不能与表重名。 - 视图可以使用select语句查询到的列名，也可以自己指定相应的列名。 - 可以指定视图执行的算法，通过ALGORITHM指定。 - column_list如果存在，则数目必须等于SELECT语句检索的列数-- 查看结构 SHOW CREATE VIEW view_name-- 删除视图 - 删除视图后，数据依然存在。 - 可同时删除多个视图。 DROP VIEW [IF EXISTS] view_name ...-- 修改视图结构 - 一般不修改视图，因为不是所有的更新视图都会映射到表上。 ALTER VIEW view_name [(column_list)] AS select_statement-- 视图作用 1. 简化业务逻辑 2. 对客户端隐藏真实的表结构-- 视图算法(ALGORITHM) MERGE 合并 将视图的查询语句，与外部查询需要先合并再执行！ TEMPTABLE 临时表 将视图执行完毕后，形成临时表，再做外层查询！ UNDEFINED 未定义(默认)，指的是MySQL自主去选择相应的算法。 事务(transaction)123456789101112131415161718192021222324252627282930313233343536373839404142434445事务是指逻辑上的一组操作，组成这组操作的各个单元，要不全成功要不全失败。 - 支持连续SQL的集体成功或集体撤销。 - 事务是数据库在数据晚自习方面的一个功能。 - 需要利用 InnoDB 或 BDB 存储引擎，对自动提交的特性支持完成。 - InnoDB被称为事务安全型引擎。-- 事务开启 START TRANSACTION; 或者 BEGIN; 开启事务后，所有被执行的SQL语句均被认作当前事务内的SQL语句。-- 事务提交 COMMIT;-- 事务回滚 ROLLBACK; 如果部分操作发生问题，映射到事务开启前。-- 事务的特性 1. 原子性（Atomicity） 事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 2. 一致性（Consistency） 事务前后数据的完整性必须保持一致。 - 事务开始和结束时，外部数据一致 - 在整个事务过程中，操作是连续的 3. 隔离性（Isolation） 多个用户并发访问数据库时，一个用户的事务不能被其它用户的事物所干扰，多个并发事务之间的数据要相互隔离。 4. 持久性（Durability） 一个事务一旦被提交，它对数据库中的数据改变就是永久性的。-- 事务的实现 1. 要求是事务支持的表类型 2. 执行一组相关的操作前开启事务 3. 整组操作完成后，都成功，则提交；如果存在失败，选择回滚，则会回到事务开始的备份点。-- 事务的原理 利用InnoDB的自动提交(autocommit)特性完成。 普通的MySQL执行语句后，当前的数据提交操作均可被其他客户端可见。 而事务是暂时关闭“自动提交”机制，需要commit提交持久化数据操作。-- 注意 1. 数据定义语言（DDL）语句不能被回滚，比如创建或取消数据库的语句，和创建、取消或更改表或存储的子程序的语句。 2. 事务不能被嵌套-- 保存点 SAVEPOINT 保存点名称 -- 设置一个事务保存点 ROLLBACK TO SAVEPOINT 保存点名称 -- 回滚到保存点 RELEASE SAVEPOINT 保存点名称 -- 删除保存点-- InnoDB自动提交特性设置 SET autocommit = 0|1; 0表示关闭自动提交，1表示开启自动提交。 - 如果关闭了，那普通操作的结果对其他客户端也不可见，需要commit提交后才能持久化数据操作。 - 也可以关闭自动提交来开启事务。但与START TRANSACTION不同的是， SET autocommit是永久改变服务器的设置，直到下次再次修改该设置。(针对当前连接) 而START TRANSACTION记录开启前的状态，而一旦事务提交或回滚后就需要再次开启事务。(针对当前事务) 锁表1234567/* 锁表 */表锁定只用于防止其它客户端进行不正当地读取和写入MyISAM 支持表锁，InnoDB 支持行锁-- 锁定 LOCK TABLES tbl_name [AS alias]-- 解锁 UNLOCK TABLES 触发器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* 触发器 */ ------------------ 触发程序是与表有关的命名数据库对象，当该表出现特定事件时，将激活该对象 监听：记录的增加、修改、删除。-- 创建触发器CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt 参数： trigger_time是触发程序的动作时间。它可以是 before 或 after，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event指明了激活触发程序的语句的类型 INSERT：将新行插入表时激活触发程序 UPDATE：更改某一行时激活触发程序 DELETE：从表中删除某一行时激活触发程序 tbl_name：监听的表，必须是永久性的表，不能将触发程序与TEMPORARY表或视图关联起来。 trigger_stmt：当触发程序激活时执行的语句。执行多个语句，可使用BEGIN...END复合语句结构-- 删除DROP TRIGGER [schema_name.]trigger_name可以使用old和new代替旧的和新的数据 更新操作，更新前是old，更新后是new. 删除操作，只有old. 增加操作，只有new.-- 注意 1. 对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。-- 字符连接函数concat(str1,str2,...])concat_ws(separator,str1,str2,...)-- 分支语句if 条件 then 执行语句elseif 条件 then 执行语句else 执行语句end if;-- 修改最外层语句结束符delimiter 自定义结束符号 SQL语句自定义结束符号delimiter ; -- 修改回原来的分号-- 语句块包裹begin 语句块end-- 特殊的执行1. 只要添加记录，就会触发程序。2. Insert into on duplicate key update 语法会触发： 如果没有重复记录，会触发 before insert, after insert; 如果有重复记录并更新，会触发 before insert, before update, after update; 如果有重复记录但是没有发生更新，则触发 before insert, before update3. Replace 语法 如果有记录，则执行 before insert, before delete, after delete, after insert SQL编程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130/* SQL编程 */ --------------------// 局部变量 ------------ 变量声明 declare var_name[,...] type [default value] 这个语句被用来声明局部变量。要给变量提供一个默认值，请包含一个default子句。值可以被指定为一个表达式，不需要为一个常数。如果没有default子句，初始值为null。-- 赋值 使用 set 和 select into 语句为变量赋值。 - 注意：在函数内是可以使用全局变量（用户自定义的变量）--// 全局变量 ------------ 定义、赋值set 语句可以定义并为变量赋值。set @var = value;也可以使用select into语句为变量初始化并赋值。这样要求select语句只能返回一行，但是可以是多个字段，就意味着同时为多个变量进行赋值，变量的数量需要与查询的列数一致。还可以把赋值语句看作一个表达式，通过select执行完成。此时为了避免=被当作关系运算符看待，使用:=代替。（set语句可以使用= 和 :=）。select @var:=20;select @v1:=id, @v2=name from t1 limit 1;select * from tbl_name where @var:=30;select into 可以将表中查询获得的数据赋给变量。 -| select max(height) into @max_height from tb;-- 自定义变量名为了避免select语句中，用户自定义的变量与系统标识符（通常是字段名）冲突，用户自定义变量在变量名前使用@作为开始符号。@var=10; - 变量被定义后，在整个会话周期都有效（登录到退出）--// 控制结构 ------------ if语句if search_condition then statement_list [elseif search_condition then statement_list]...[else statement_list]end if;-- case语句CASE value WHEN [compare-value] THEN result[WHEN [compare-value] THEN result ...][ELSE result]END-- while循环[begin_label:] while search_condition do statement_listend while [end_label];- 如果需要在循环内提前终止 while循环，则需要使用标签；标签需要成对出现。 -- 退出循环 退出整个循环 leave 退出当前循环 iterate 通过退出的标签决定退出哪个循环--// 内置函数 ------------ 数值函数abs(x) -- 绝对值 abs(-10.9) = 10format(x, d) -- 格式化千分位数值 format(1234567.456, 2) = 1,234,567.46ceil(x) -- 向上取整 ceil(10.1) = 11floor(x) -- 向下取整 floor (10.1) = 10round(x) -- 四舍五入去整mod(m, n) -- m%n m mod n 求余 10%3=1pi() -- 获得圆周率pow(m, n) -- m^nsqrt(x) -- 算术平方根rand() -- 随机数truncate(x, d) -- 截取d位小数-- 时间日期函数now(), current_timestamp(); -- 当前日期时间current_date(); -- 当前日期current_time(); -- 当前时间date(&apos;yyyy-mm-dd hh:ii:ss&apos;); -- 获取日期部分time(&apos;yyyy-mm-dd hh:ii:ss&apos;); -- 获取时间部分date_format(&apos;yyyy-mm-dd hh:ii:ss&apos;, &apos;%d %y %a %d %m %b %j&apos;); -- 格式化时间unix_timestamp(); -- 获得unix时间戳from_unixtime(); -- 从时间戳获得时间-- 字符串函数length(string) -- string长度，字节char_length(string) -- string的字符个数substring(str, position [,length]) -- 从str的position开始,取length个字符replace(str ,search_str ,replace_str) -- 在str中用replace_str替换search_strinstr(string ,substring) -- 返回substring首次在string中出现的位置concat(string [,...]) -- 连接字串charset(str) -- 返回字串字符集lcase(string) -- 转换成小写left(string, length) -- 从string2中的左边起取length个字符load_file(file_name) -- 从文件读取内容locate(substring, string [,start_position]) -- 同instr,但可指定开始位置lpad(string, length, pad) -- 重复用pad加在string开头,直到字串长度为lengthltrim(string) -- 去除前端空格repeat(string, count) -- 重复count次rpad(string, length, pad) --在str后用pad补充,直到长度为lengthrtrim(string) -- 去除后端空格strcmp(string1 ,string2) -- 逐字符比较两字串大小-- 流程函数case when [condition] then result [when [condition] then result ...] [else result] end 多分支if(expr1,expr2,expr3) 双分支。-- 聚合函数count()sum();max();min();avg();group_concat()-- 其他常用函数md5();default();--// 存储函数，自定义函数 ------------ 新建 CREATE FUNCTION function_name (参数列表) RETURNS 返回值类型 函数体 - 函数名，应该合法的标识符，并且不应该与已有的关键字冲突。 - 一个函数应该属于某个数据库，可以使用db_name.funciton_name的形式执行当前函数所属数据库，否则为当前数据库。 - 参数部分，由&quot;参数名&quot;和&quot;参数类型&quot;组成。多个参数用逗号隔开。 - 函数体由多条可用的mysql语句，流程控制，变量声明等语句构成。 - 多条语句应该使用 begin...end 语句块包含。 - 一定要有 return 返回值语句。-- 删除 DROP FUNCTION [IF EXISTS] function_name;-- 查看 SHOW FUNCTION STATUS LIKE &apos;partten&apos; SHOW CREATE FUNCTION function_name;-- 修改 ALTER FUNCTION function_name 函数选项--// 存储过程，自定义功能 ------------ 定义存储存储过程 是一段代码（过程），存储在数据库中的sql组成。一个存储过程通常用于完成一段业务逻辑，例如报名，交班费，订单入库等。而一个函数通常专注与某个功能，视为其他程序服务的，需要在其他语句中调用函数才可以，而存储过程不能被其他调用，是自己执行 通过call执行。-- 创建CREATE PROCEDURE sp_name (参数列表) 过程体参数列表：不同于函数的参数列表，需要指明参数类型IN，表示输入型OUT，表示输出型INOUT，表示混合型注意，没有返回值。 存储过程12345678910111213141516/* 存储过程 */ ------------------存储过程是一段可执行性代码的集合。相比函数，更偏向于业务逻辑。调用：CALL 过程名-- 注意- 没有返回值。- 只能单独调用，不可夹杂在其他语句中-- 参数IN|OUT|INOUT 参数名 数据类型IN 输入：在调用过程中，将数据输入到过程体内部的参数OUT 输出：在调用过程中，将过程体处理完的结果返回到客户端INOUT 输入输出：既可输入，也可输出-- 语法CREATE PROCEDURE 过程名 (参数列表)BEGIN 过程体END 用户和权限管理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/* 用户和权限管理 */ -------------------- root密码重置1. 停止MySQL服务2. [Linux] /usr/local/mysql/bin/safe_mysqld --skip-grant-tables &amp; [Windows] mysqld --skip-grant-tables3. use mysql;4. UPDATE `user` SET PASSWORD=PASSWORD(&quot;密码&quot;) WHERE `user` = &quot;root&quot;;5. FLUSH PRIVILEGES;用户信息表：mysql.user-- 刷新权限FLUSH PRIVILEGES;-- 增加用户CREATE USER 用户名 IDENTIFIED BY [PASSWORD] 密码(字符串) - 必须拥有mysql数据库的全局CREATE USER权限，或拥有INSERT权限。 - 只能创建用户，不能赋予权限。 - 用户名，注意引号：如 &apos;user_name&apos;@&apos;192.168.1.1&apos; - 密码也需引号，纯数字密码也要加引号 - 要在纯文本中指定密码，需忽略PASSWORD关键词。要把密码指定为由PASSWORD()函数返回的混编值，需包含关键字PASSWORD-- 重命名用户RENAME USER old_user TO new_user-- 设置密码SET PASSWORD = PASSWORD(&apos;密码&apos;) -- 为当前用户设置密码SET PASSWORD FOR 用户名 = PASSWORD(&apos;密码&apos;) -- 为指定用户设置密码-- 删除用户DROP USER 用户名-- 分配权限/添加用户GRANT 权限列表 ON 表名 TO 用户名 [IDENTIFIED BY [PASSWORD] &apos;password&apos;] - all privileges 表示所有权限 - *.* 表示所有库的所有表 - 库名.表名 表示某库下面的某表 GRANT ALL PRIVILEGES ON `pms`.* TO &apos;pms&apos;@&apos;%&apos; IDENTIFIED BY &apos;pms0817&apos;;-- 查看权限SHOW GRANTS FOR 用户名 -- 查看当前用户权限 SHOW GRANTS; 或 SHOW GRANTS FOR CURRENT_USER; 或 SHOW GRANTS FOR CURRENT_USER();-- 撤消权限REVOKE 权限列表 ON 表名 FROM 用户名REVOKE ALL PRIVILEGES, GRANT OPTION FROM 用户名 -- 撤销所有权限-- 权限层级-- 要使用GRANT或REVOKE，您必须拥有GRANT OPTION权限，并且您必须用于您正在授予或撤销的权限。全局层级：全局权限适用于一个给定服务器中的所有数据库，mysql.user GRANT ALL ON *.*和 REVOKE ALL ON *.*只授予和撤销全局权限。数据库层级：数据库权限适用于一个给定数据库中的所有目标，mysql.db, mysql.host GRANT ALL ON db_name.*和REVOKE ALL ON db_name.*只授予和撤销数据库权限。表层级：表权限适用于一个给定表中的所有列，mysql.talbes_priv GRANT ALL ON db_name.tbl_name和REVOKE ALL ON db_name.tbl_name只授予和撤销表权限。列层级：列权限适用于一个给定表中的单一列，mysql.columns_priv 当使用REVOKE时，您必须指定与被授权列相同的列。-- 权限列表ALL [PRIVILEGES] -- 设置除GRANT OPTION之外的所有简单权限ALTER -- 允许使用ALTER TABLEALTER ROUTINE -- 更改或取消已存储的子程序CREATE -- 允许使用CREATE TABLECREATE ROUTINE -- 创建已存储的子程序CREATE TEMPORARY TABLES -- 允许使用CREATE TEMPORARY TABLECREATE USER -- 允许使用CREATE USER, DROP USER, RENAME USER和REVOKE ALL PRIVILEGES。CREATE VIEW -- 允许使用CREATE VIEWDELETE -- 允许使用DELETEDROP -- 允许使用DROP TABLEEXECUTE -- 允许用户运行已存储的子程序FILE -- 允许使用SELECT...INTO OUTFILE和LOAD DATA INFILEINDEX -- 允许使用CREATE INDEX和DROP INDEXINSERT -- 允许使用INSERTLOCK TABLES -- 允许对您拥有SELECT权限的表使用LOCK TABLESPROCESS -- 允许使用SHOW FULL PROCESSLISTREFERENCES -- 未被实施RELOAD -- 允许使用FLUSHREPLICATION CLIENT -- 允许用户询问从属服务器或主服务器的地址REPLICATION SLAVE -- 用于复制型从属服务器（从主服务器中读取二进制日志事件）SELECT -- 允许使用SELECTSHOW DATABASES -- 显示所有数据库SHOW VIEW -- 允许使用SHOW CREATE VIEWSHUTDOWN -- 允许使用mysqladmin shutdownSUPER -- 允许使用CHANGE MASTER, KILL, PURGE MASTER LOGS和SET GLOBAL语句，mysqladmin debug命令；允许您连接（一次），即使已达到max_connections。UPDATE -- 允许使用UPDATEUSAGE -- “无权限”的同义词GRANT OPTION -- 允许授予权限 表维护12345678/* 表维护 */-- 分析和存储表的关键字分布ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE 表名 ...-- 检查一个或多个表是否有错误CHECK TABLE tbl_name [, tbl_name] ... [option] ...option = &#123;QUICK | FAST | MEDIUM | EXTENDED | CHANGED&#125;-- 整理数据文件的碎片OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 杂项1234567891011121314/* 杂项 */ ------------------1. 可用反引号（`）为标识符（库名、表名、字段名、索引、别名）包裹，以避免与关键字重名！中文也可以作为标识符！2. 每个库目录存在一个保存当前数据库的选项文件db.opt。3. 注释： 单行注释 # 注释内容 多行注释 /* 注释内容 */ 单行注释 -- 注释内容 (标准SQL注释风格，要求双破折号后加一空格符（空格、TAB、换行等）)4. 模式通配符： _ 任意单个字符 % 任意多个字符，甚至包括零字符 单引号需要进行转义 \&apos;5. CMD命令行内的语句结束符可以为 &quot;;&quot;, &quot;\G&quot;, &quot;\g&quot;，仅影响显示结果。其他地方还是用分号结束。delimiter 可修改当前对话的语句结束符。6. SQL对大小写不敏感7. 清除已有语句：\c]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql入门]]></title>
    <url>%2F2011%2F02%2F05%2Fdb-mysql-MySQL%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[一、索引B+ Tree 原理1. 数据结构B Tree 指的是 Balance Tree，也就是平衡树。平衡树是一颗查找树，并且所有叶子节点位于同一层。 B+ Tree 是基于 B Tree 和叶子节点顺序访问指针进行实现，它具有 B Tree 的平衡性，并且通过顺序访问指针来提高区间查询的性能。 在 B+ Tree 中，一个节点中的 key 从左到右非递减排列，如果某个指针的左右相邻 key 分别是 keyi 和 keyi+1，且不为 null，则该指针指向节点的所有 key 大于等于 keyi 且小于等于 keyi+1。 2. 操作进行查找操作时，首先在根节点进行二分查找，找到一个 key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出 key 所对应的 data。 插入删除操作会破坏平衡树的平衡性，因此在插入删除操作之后，需要对树进行一个分裂、合并、旋转等操作来维护平衡性。 3. 与红黑树的比较红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用 B+ Tree 作为索引结构，主要有以下两个原因： （一）更少的查找次数 平衡树查找操作的时间复杂度和树高 h 相关，O(h)=O(logdN)，其中 d 为每个节点的出度。 红黑树的出度为 2，而 B+ Tree 的出度一般都非常大，所以红黑树的树高 h 很明显比 B+ Tree 大非常多，查找的次数也就更多。 （二）利用磁盘预读特性 为了减少磁盘 I/O 操作，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的旋转时间，速度会非常快。 操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点。并且可以利用预读特性，相邻的节点也能够被预先载入。 MySQL 索引索引是在存储引擎层实现的，而不是在服务器层实现的，所以不同存储引擎具有不同的索引类型和实现。 1. B+Tree 索引是大多数 MySQL 存储引擎的默认索引类型。 因为不再需要进行全表扫描，只需要对树进行搜索即可，所以查找速度快很多。 除了用于查找，还可以用于排序和分组。 可以指定多个列作为索引列，多个索引列共同组成键。 适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。如果不是按照索引列的顺序进行查找，则无法使用索引。 InnoDB 的 B+Tree 索引分为主索引和辅助索引。主索引的叶子节点 data 域记录着完整的数据记录，这种索引方式被称为聚簇索引。因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。 辅助索引的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找。 ### 2. 哈希索引 哈希索引能以 O(1) 时间进行查找，但是失去了有序性： 无法用于排序与分组； 只支持精确查找，无法用于部分查找和范围查找。 InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。 3. 全文索引MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。 查找条件使用 MATCH AGAINST，而不是普通的 WHERE。 全文索引使用倒排索引实现，它记录着关键词到其所在文档的映射。 InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。 4. 空间数据索引MyISAM 存储引擎支持空间数据索引（R-Tree），可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。 必须使用 GIS 相关的函数来维护数据。 索引优化1. 独立的列在进行查询时，索引列不能是表达式的一部分，也不能是函数的参数，否则无法使用索引。 例如下面的查询不能使用 actor_id 列的索引： 1SELECT actor_id FROM sakila.actor WHERE actor_id + 1 = 5; 2. 多列索引在需要使用多个列作为条件进行查询时，使用多列索引比使用多个单列索引性能更好。例如下面的语句中，最好把 actor_id 和 film_id 设置为多列索引。 12SELECT film_id, actor_ id FROM sakila.film_actorWHERE actor_id = 1 AND film_id = 1; 3. 索引列的顺序让选择性最强的索引列放在前面。 索引的选择性是指：不重复的索引值和记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，查询效率也越高。 例如下面显示的结果中 customer_id 的选择性比 staff_id 更高，因此最好把 customer_id 列放在多列索引的前面。 1234SELECT COUNT(DISTINCT staff_id)/COUNT(*) AS staff_id_selectivity,COUNT(DISTINCT customer_id)/COUNT(*) AS customer_id_selectivity,COUNT(*)FROM payment; 123 staff_id_selectivity: 0.0001customer_id_selectivity: 0.0373 COUNT(*): 16049 4. 前缀索引对于 BLOB、TEXT 和 VARCHAR 类型的列，必须使用前缀索引，只索引开始的部分字符。 对于前缀长度的选取需要根据索引选择性来确定。 5. 覆盖索引索引包含所有需要查询的字段的值。 具有以下优点： 索引通常远小于数据行的大小，只读取索引能大大减少数据访问量。 一些存储引擎（例如 MyISAM）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）。 对于 InnoDB 引擎，若辅助索引能够覆盖查询，则无需访问主索引。 索引的优点 大大减少了服务器需要扫描的数据行数。 帮助服务器避免进行排序和分组，以及避免创建临时表（B+Tree 索引是有序的，可以用于 ORDER BY 和 GROUP BY 操作。临时表主要是在排序和分组过程中创建，因为不需要排序和分组，也就不需要创建临时表）。 将随机 I/O 变为顺序 I/O（B+Tree 索引是有序的，会将相邻的数据都存储在一起）。 索引的使用条件 对于非常小的表、大部分情况下简单的全表扫描比建立索引更高效； 对于中到大型的表，索引就非常有效； 但是对于特大型的表，建立和维护索引的代价将会随之增长。这种情况下，需要用到一种技术可以直接区分出需要查询的一组数据，而不是一条记录一条记录地匹配，例如可以使用分区技术。 二、查询性能优化使用 Explain 进行分析Explain 用来分析 SELECT 查询语句，开发人员可以通过分析 Explain 结果来优化查询语句。 比较重要的字段有： select_type : 查询类型，有简单查询、联合查询、子查询等 key : 使用的索引 rows : 扫描的行数 优化数据访问1. 减少请求的数据量 只返回必要的列：最好不要使用 SELECT * 语句。 只返回必要的行：使用 LIMIT 语句来限制返回的数据。 缓存重复查询的数据：使用缓存可以避免在数据库中进行查询，特别在要查询的数据经常被重复查询时，缓存带来的查询性能提升将会是非常明显的。 2. 减少服务器端扫描的行数最有效的方式是使用索引来覆盖查询。 重构查询方式1. 切分大查询一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。 1DELETE FROM messages WHERE create &lt; DATE_SUB(NOW(), INTERVAL 3 MONTH); 12345rows_affected = 0do &#123; rows_affected = do_query( "DELETE FROM messages WHERE create &lt; DATE_SUB(NOW(), INTERVAL 3 MONTH) LIMIT 10000")&#125; while rows_affected &gt; 0 2. 分解大连接查询将一个大连接查询分解成对每一个表进行一次单表查询，然后在应用程序中进行关联，这样做的好处有： 让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。 分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。 减少锁竞争； 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可伸缩。 查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。 1234SELECT * FROM tabJOIN tag_post ON tag_post.tag_id=tag.idJOIN post ON tag_post.post_id=post.idWHERE tag.tag='mysql'; 123SELECT * FROM tag WHERE tag='mysql';SELECT * FROM tag_post WHERE tag_id=1234;SELECT * FROM post WHERE post.id IN (123,456,567,9098,8904); 三、存储引擎InnoDB是 MySQL 默认的事务型存储引擎，只有在需要它不支持的特性时，才考虑使用其它存储引擎。 实现了四个标准的隔离级别，默认级别是可重复读（REPEATABLE READ）。在可重复读隔离级别下，通过多版本并发控制（MVCC）+ 间隙锁（Next-Key Locking）防止幻影读。 主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。 内部做了很多优化，包括从磁盘读取数据时采用的可预测性读、能够加快读操作并且自动创建的自适应哈希索引、能够加速插入操作的插入缓冲区等。 支持真正的在线热备份。其它存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。 MyISAM设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用它。 提供了大量的特性，包括压缩表、空间数据索引等。 不支持事务。 不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入（CONCURRENT INSERT）。 可以手工或者自动执行检查和修复操作，但是和事务恢复以及崩溃恢复不同，可能导致一些数据丢失，而且修复操作是非常慢的。 如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时，不会立即将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入磁盘。这种方式可以极大的提升写入性能，但是在数据库或者主机崩溃时会造成索引损坏，需要执行修复操作。 比较 事务：InnoDB 是事务型的，可以使用 Commit 和 Rollback 语句。 并发：MyISAM 只支持表级锁，而 InnoDB 还支持行级锁。 外键：InnoDB 支持外键。 备份：InnoDB 支持在线热备份。 崩溃恢复：MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。 其它特性：MyISAM 支持压缩表和空间数据索引。 四、数据类型整型TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT 分别使用 8, 16, 24, 32, 64 位存储空间，一般情况下越小的列越好。 INT(11) 中的数字只是规定了交互工具显示字符的个数，对于存储和计算来说是没有意义的。 浮点数FLOAT 和 DOUBLE 为浮点类型，DECIMAL 为高精度小数类型。CPU 原生支持浮点运算，但是不支持 DECIMAl 类型的计算，因此 DECIMAL 的计算比浮点类型需要更高的代价。 FLOAT、DOUBLE 和 DECIMAL 都可以指定列宽，例如 DECIMAL(18, 9) 表示总共 18 位，取 9 位存储小数部分，剩下 9 位存储整数部分。 字符串主要有 CHAR 和 VARCHAR 两种类型，一种是定长的，一种是变长的。 VARCHAR 这种变长类型能够节省空间，因为只需要存储必要的内容。但是在执行 UPDATE 时可能会使行变得比原来长，当超出一个页所能容纳的大小时，就要执行额外的操作。MyISAM 会将行拆成不同的片段存储，而 InnoDB 则需要分裂页来使行放进页内。 在进行存储和检索时，会保留 VARCHAR 末尾的空格，而会删除 CHAR 末尾的空格。 时间和日期MySQL 提供了两种相似的日期时间类型：DATETIME 和 TIMESTAMP。 1. DATETIME能够保存从 1001 年到 9999 年的日期和时间，精度为秒，使用 8 字节的存储空间。 它与时区无关。 默认情况下，MySQL 以一种可排序的、无歧义的格式显示 DATETIME 值，例如“2008-01-16 22:37:08”，这是 ANSI 标准定义的日期和时间表示方法。 2. TIMESTAMP和 UNIX 时间戳相同，保存从 1970 年 1 月 1 日午夜（格林威治时间）以来的秒数，使用 4 个字节，只能表示从 1970 年到 2038 年。 它和时区有关，也就是说一个时间戳在不同的时区所代表的具体时间是不同的。 MySQL 提供了 FROM_UNIXTIME() 函数把 UNIX 时间戳转换为日期，并提供了 UNIX_TIMESTAMP() 函数把日期转换为 UNIX 时间戳。 默认情况下，如果插入时没有指定 TIMESTAMP 列的值，会将这个值设置为当前时间。 应该尽量使用 TIMESTAMP，因为它比 DATETIME 空间效率更高。 五、切分水平切分水平切分又称为 Sharding，它是将同一个表中的记录拆分到多个结构相同的表中。 当一个表的数据不断增多时，Sharding 是必然的选择，它可以将数据分布到集群的不同节点上，从而缓存单个数据库的压力。 ## 垂直切分 垂直切分是将一张表按列切分成多个表，通常是按照列的关系密集程度进行切分，也可以利用垂直切分将经常被使用的列和不经常被使用的列切分到不同的表中。 在数据库的层面使用垂直切分将按数据库中表的密集程度部署到不同的库中，例如将原来的电商数据库垂直切分成商品数据库、用户数据库等。 ## Sharding 策略 哈希取模：hash(key) % N； 范围：可以是 ID 范围也可以是时间范围； 映射表：使用单独的一个数据库来存储映射关系。 Sharding 存在的问题1. 事务问题使用分布式事务来解决，比如 XA 接口。 2. 连接可以将原来的连接分解成多个单表查询，然后在用户程序中进行连接。 3. ID 唯一性 使用全局唯一 ID（GUID） 为每个分片指定一个 ID 范围 分布式 ID 生成器 (如 Twitter 的 Snowflake 算法) 六、复制主从复制主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。 binlog 线程 ：负责将主服务器上的数据更改写入二进制日志（Binary log）中。 I/O 线程 ：负责从主服务器上读取二进制日志，并写入从服务器的中继日志（Relay log）。 SQL 线程 ：负责读取中继日志，解析出主服务器已经执行的数据更改并在从服务器中执行。 ## 读写分离 主服务器处理写操作以及实时性要求比较高的读操作，而从服务器处理读操作。 读写分离能提高性能的原因在于： 主从服务器负责各自的读和写，极大程度缓解了锁的争用； 从服务器可以使用 MyISAM，提升查询性能以及节约系统开销； 增加冗余，提高可用性。 读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql刚开始]]></title>
    <url>%2F2011%2F02%2F05%2Fdb-mysql-MySQL%2F</url>
    <content type="text"><![CDATA[文字教程推荐MySQL 教程（菜鸟教程） MySQL教程（易百教程） 视频教程推荐基础入门： 与MySQL的零距离接触-慕课网 Mysql开发技巧： MySQL开发技巧（一） MySQL开发技巧（二） MySQL开发技巧（三） Mysql5.7新特性及相关优化技巧： MySQL5.7版本新特性 性能优化之MySQL优化 MySQL集群（PXC）入门 MyCAT入门及应用 常见问题总结 ①存储引擎MySQL常见的两种存储引擎：MyISAM与InnoDB的爱恨情仇 ②字符集及校对规则 字符集指的是一种从二进制编码到某类字符符号的映射。校对规则则是指某种字符集下的排序规则。Mysql中每一种字符集都会对应一系列的校对规则。 Mysql采用的是类似继承的方式指定字符集的默认值，每个数据库以及每张数据表都有自己的默认值，他们逐层继承。比如：某个库中所有表的默认字符集将是该数据库所指定的字符集（这些表在没有指定字符集的情况下，才会采用默认字符集） PS：整理自《Java工程师修炼之道》 详细内容可以参考： MySQL字符集及校对规则的理解 ③索引相关的内容（数据库使用中非常关键的技术，合理正确的使用索引可以大大提高数据库的查询性能） Mysql索引使用的数据结构主要有BTree索引 和 哈希索引 。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。 Mysql的BTree索引使用的是B数中的B+Tree，但对于主要的两种存储引擎的实现方式是不同的。 MyISAM: B+Tree叶节点的data域存放的是数据记录的地址。在索引检索的时候，首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录。这被称为“非聚簇索引”。 InnoDB: 其数据文件本身就是索引文件。相比MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按B+Tree组织的一个索引结构，树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。这被称为“聚簇索引（或聚集索引）”。而其余的索引都作为辅助索引，辅助索引的data域存储相应记录主键的值而不是地址，这也是和MyISAM不同的地方。在根据主索引搜索时，直接找到key所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，再走一遍主索引。 因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。 PS：整理自《Java工程师修炼之道》 详细内容可以参考： 干货：mysql索引的数据结构 MySQL优化系列（三）–索引的使用、原理和设计优化 数据库两大神器【索引和锁】 ④查询缓存的使用 my.cnf加入以下配置，重启Mysql开启查询缓存 12query_cache_type=1query_cache_size=600000 Mysql执行以下命令也可以开启查询缓存 12set global query_cache_type=1;set global query_cache_size=600000; 如上，开启查询缓存后在同样的查询条件以及数据情况下，会直接在缓存中返回结果。这里的查询条件包括查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息。因此任何两个查询在任何字符上的不同都会导致缓存不命中。此外，如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、Mysql库中的系统表，其查询结果也不会被缓存。 缓存建立之后，Mysql的查询缓存系统会跟踪查询中涉及的每张表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。 缓存虽然能够提升数据库的查询性能，但是缓存同时也带来了额外的开销，每次查询后都要做一次缓存操作，失效后还要销毁。 因此，开启缓存查询要谨慎，尤其对于写密集的应用来说更是如此。如果开启，要注意合理控制缓存空间大小，一般来说其大小设置为几十MB比较合适。此外，还可以通过sql_cache和sql_no_cache来控制某个查询语句是否需要缓存： 1select sql_no_cache count(*) from usr; ⑤事务机制 关系性数据库需要遵循ACID规则，具体内容如下： 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性： 执行事务前后，数据库从一个一致性状态转换到另一个一致性状态。 隔离性： 并发访问数据库时，一个用户的事物不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库 发生故障也不应该对其有任何影响。 为了达到上述事务特性，数据库定义了几种不同的事务隔离级别： READ_UNCOMMITTED（未提交读）: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 READ_COMMITTED（提交读）: 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 REPEATABLE_READ（可重复读）: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE（串行）: 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 这里需要注意的是：Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别. 事务隔离机制的实现基于锁机制和并发调度。其中并发调度使用的是MVCC（多版本并发控制），通过行的创建时间和行的过期时间来支持并发一致性读和回滚等特性。 详细内容可以参考： 可能是最漂亮的Spring事务管理详解 ⑥锁机制与InnoDB锁算法 MyISAM和InnoDB存储引擎使用的锁： MyISAM采用表级锁(table-level locking)。 InnoDB支持行级锁(row-level locking)和表级锁,默认为行级锁 表级锁和行级锁对比： 表级锁： Mysql中锁定 粒度最大 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM和 InnoDB引擎都支持表级锁。 行级锁： Mysql中锁定 粒度最小 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。 详细内容可以参考：Mysql锁机制简单了解一下 InnoDB存储引擎的锁的算法有三种： Record lock：单个行记录上的锁 Gap lock：间隙锁，锁定一个范围，不包括记录本身 Next-key lock：record+gap 锁定一个范围，包含记录本身 相关知识点： innodb对于行的查询使用next-key lock Next-locking keying为了解决Phantom Problem幻读问题 当查询的索引含有唯一属性时，将next-key lock降级为record key Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1 ⑦大表优化当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下： 限定数据的范围： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内； 读/写分离： 经典的数据库拆分方案，主库负责写，从库负责读；3 . 垂直分区： 根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。 简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。 如下图所示，这样来说大家应该就更容易理解了。 垂直拆分的优点： 可以使得列数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。 垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂； 水平分区： **保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。** 水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。 ![数据库水平拆分](https://user-gold-cdn.xitu.io/2018/6/16/164084b7e9e423e3?w=690&amp;h=271&amp;f=jpeg&amp;s=23119) 水平拆分可以支持非常大的数据量。需要注意的一点是：分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 **水平拆分最好分库** 。 水平拆分能够 **支持非常大的数据量存储，应用端改造也少**，但 **分片事务难以解决** ，跨节点Join性能较差，逻辑复杂。《Java工程师修炼之道》的作者推荐 **尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度** ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。 **下面补充一下数据库分片的两种常见方案：** - **客户端代理：** **分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。** 当当网的 **Sharding-JDBC** 、阿里的TDDL是两种比较常用的实现。 - **中间件代理：** **在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。** 我们现在谈的 **Mycat** 、360的Atlas、网易的DDB等等都是这种架构的实现。 详细内容可以参考： MySQL大表优化方案]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[drop,delete与truncate的区别]]></title>
    <url>%2F2011%2F01%2F10%2Fdb-mysql-question-9-1-3-drop-delete%E4%B8%8Etruncate%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[题目**：drop,delete与truncate的区别参考答案：drop直接删掉表 truncate删除表中数据，再插入时自增长id又从1开始 delete删除表中数据，可以加where字句。 （1） DELETE语句执行删除的过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。 （2） 表和索引所占空间。当表被TRUNCATE 后，这个表和索引所占用的空间会恢复到初始大小，而DELETE操作不会减少表或索引所占用的空间。drop语句将表所占用的空间全释放掉。 （3） 一般而言，drop &gt; truncate &gt; delete （4） 应用范围。TRUNCATE 只能对TABLE；DELETE可以是table和view （5） TRUNCATE 和DELETE只删除数据，而DROP则删除整个表（结构和数据）。 （6） truncate与不带where的delete ：只删除数据，而不删除表的结构（定义）drop语句将删除表的结构被依赖的约束（constrain),触发器（trigger)索引（index);依赖于该表的存储过程/函数将被保留，但其状态会变为：invalid。 （7） delete语句为DML（data maintain Language),这个操作会被放到 rollback segment中,事务提交后才生效。如果有相应的 tigger,执行的时候将被触发。 （8） truncate、drop是DLL（data define language),操作立即生效，原数据不放到 rollback segment中，不能回滚 （9） 在没有备份情况下，谨慎使用 drop 与 truncate。要删除部分数据行采用delete且注意结合where来约束影响范围。回滚段要足够大。要删除表用drop;若想保留表而将表中数据删除，如果于事务无关，用truncate即可实现。如果和事务有关，或老师想触发trigger,还是用delete。 （10） Truncate table 表名 速度快,而且效率高,因为: truncate table 在功能上与不带 WHERE 子句的 DELETE 语句相同：二者均删除表中的全部行。但 TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少。DELETE 语句每次删除一行，并在事务日志中为所删除的每行记录一项。TRUNCATE TABLE 通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。 （11） TRUNCATE TABLE 删除表中的所有行，但表结构及其列、约束、索引等保持不变。新行标识所用的计数值重置为该列的种子。如果想保留标识计数值，请改用 DELETE。如果要删除表定义及其数据，请使用 DROP TABLE 语句。 （12） 对于由 FOREIGN KEY 约束引用的表，不能使用 TRUNCATE TABLE，而应使用不带 WHERE 子句的 DELETE 语句。由于 TRUNCATE TABLE 不记录在日志中，所以它不能激活触发器。]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据类型]]></title>
    <url>%2F2011%2F01%2F08%2Fdb-mysql-MySQL%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[数字类型 整数: tinyint、smallint、mediumint、int、bigint 浮点数: float、double、real、decimal 日期和时间: date、time、datetime、timestamp、year 字符串类型 字符串: char、varchar 文本: tinytext、text、mediumtext、longtext 二进制(可用来存储图片、音乐等): tinyblob、blob、mediumblob、longblob 数字类型整型 type Storage Minumun Value Maximum Value (Bytes) (Signed/Unsigned) (Signed/Unsigned) TINYINT 1 -128 127 0 255 SMALLINT 2 -32768 32767 0 65535 MEDIUMINT 3 -8388608 8388607 0 16777215 INT 4 -2147483648 2147483647 0 4294967295 BIGINT 8 -9223372036854775808 9223372036854775807 0 18446744073709551615 浮点型 属性 存储空间 精度 精确性 说明 FLOAT(M, D) 4 bytes 单精度 非精确 单精度浮点型，m总个数，d小数位 DOUBLE(M, D) 8 bytes 双精度 比Float精度高 双精度浮点型，m总个数，d小数位 FLOAT容易造成精度丢失 定点数DECIMAL 高精度的数据类型，常用来存储交易相关的数据 DECIMAL(M,N).M代表总精度，N代表小数点右侧的位数（标度） 1 &lt; M &lt; 254, 0 &lt; N &lt; 60; 存储空间变长 时间类型 类型 字节 例 精确性 DATE 三字节 2015-05-01 精确到年月日 TIME 三字节 11:12:00 精确到时分秒 DATETIME 八字节 2015-05-01 11::12:00 精确到年月日时分秒 TIMESTAMP 2015-05-01 11::12:00 精确到年月日时分秒 MySQL在5.6.4版本之后，TIMESTAMP和DATETIME支持到微秒。 TIMESTAMP会根据系统时区进行转换，DATETIME则不会 存储范围的区别 TIMESTAMP存储范围：1970-01-01 00::00:01 to 2038-01-19 03:14:07 DATETIME的存储范围：1000-01-01 00:00:00 to 9999-12-31 23:59:59 一般使用TIMESTAMP国际化 如存时间戳使用数字类型BIGINT 字符串类型 类型 单位 最大 特性 CHAR 字符 最大为255字符 存储定长，容易造成空间的浪费 VARCHAR 字符 可以超过255个字符 存储变长，节省存储空间 TEXT 字节 总大小为65535字节，约为64KB - TEXT在MySQL内部大多存储格式为溢出页，效率不如CHAR Mysql默认为utf-8，那么在英文模式下1个字符=1个字节，在中文模式下1个字符=3个字节。]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库范式]]></title>
    <url>%2F2011%2F01%2F05%2Fdb-mysql-question-9-1-6-%E6%95%B0%E6%8D%AE%E5%BA%93%E8%8C%83%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[题目**：数据库范式参考答案：1 第一范式（1NF） 在任何一个关系数据库中，第一范式（1NF）是对关系模式的基本要求，不满足第一范式（1NF）的数据库就不是关系数据库。所谓第一范式（1NF）是指数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性。如果出现重复的属性，就可能需要定义一个新的实体，新的实体由重复的属性构成，新实体与原实体之间为一对多关系。在第一范式（1NF）中表的每一行只包含一个实例的信息。简而言之，第一范式就是无重复的列。 2 第二范式（2NF） 第二范式（2NF）是在第一范式（1NF）的基础上建立起来的，即满足第二范式（2NF）必须先满足第一范式（1NF）。第二范式（2NF）要求数据库表中的每个实例或行必须可以被惟一地区分。为实现区分通常需要为表加上一个列，以存储各个实例的惟一标识。这个惟一属性列被称为主关键字或主键、主码。第二范式（2NF）要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性，如果存在，那么这个属性和主关键字的这一部分应该分离出来形成一个新的实体，新实体与原实体之间是一对多的关系。为实现区分通常需要为表加上一个列，以存储各个实例的惟一标识。简而言之，第二范式就是非主属性非部分依赖于主关键字。 3 第三范式（3NF） 满足第三范式（3NF）必须先满足第二范式（2NF）。简而言之，第三范式（3NF）要求一个数据库表中不包含已在其它表中已包含的非主关键字信息。例如，存在一个部门信息表，其中每个部门有部门编号（dept_id）、部门名称、部门简介等信息。那么在员工信息表中列出部门编号后就不能再将部门名称、部门简介等与部门有关的信息再加入员工信息表中。如果不存在部门信息表，则根据第三范式（3NF）也应该构建它，否则就会有大量的数据冗余。简而言之，第三范式就是属性不依赖于其它非主属性。（我的理解是消除冗余）]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础知识]]></title>
    <url>%2F2010%2F05%2F19%2Flang-java-basic-Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[1. 面向对象和面向过程的区别面向过程优点： 性能比面向对象高。因为类调用时需要实例化，开销比较大，比较消耗资源，所以当性能是最重要的考量因素的时候，比如单片机、嵌入式开发、Linux/Unix等一般采用面向过程开发 缺点： 没有面向对象易维护、易复用、易扩展 面向对象优点： 易维护、易复用、易扩展，由于面向对象有封装、继承、多态性的特性，可以设计出低耦合的系统，使系统更加灵活、更加易于维护 缺点： 性能比面向过程低 2. Java 语言有哪些特点? 简单易学； 面向对象（封装，继承，多态）； 平台无关性（ Java 虚拟机实现平台无关性）； 可靠性； 安全性； 支持多线程（ C++ 语言没有内置的多线程机制，因此必须调用操作系统的多线程功能来进行多线程程序设计，而 Java 语言却提供了多线程支持）； 支持网络编程并且很方便（ Java 语言诞生本身就是为简化网络编程设计的，因此 Java 语言不仅支持网络编程而且很方便）； 编译与解释并存； 3. 关于 JVM JDK 和 JRE 最详细通俗的解答JVMJava虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。 什么是字节码?采用字节码的好处是什么? 在 Java 中，JVM可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，因此，Java程序无须重新编译便可在多种不同操作系统的计算机上运行。 Java 程序从源代码到运行一般有下面3步： 我们需要格外注意的是 .class-&gt;机器码 这一步。在这一步 JVM 类加载器首先加载字节码文件，然后通过解释器逐行解释执行，这种方式的执行速度会相对比较慢。而且，有些方法和代码块是经常需要被调用的(也就是所谓的热点代码)，所以后面引进了 JIT 编译器，而JIT 属于运行时编译。当 JIT 编译器完成第一次编译后，其会将字节码对应的机器码保存下来，下次可以直接使用。而我们知道，机器码的运行效率肯定是高于 Java 解释器的。这也解释了我们为什么经常会说 Java 是编译与解释共存的语言。 HotSpot采用了惰性评估(Lazy Evaluation)的做法，根据二八定律，消耗大部分系统资源的只有那一小部分的代码（热点代码），而这也就是JIT所需要编译的部分。JVM会根据代码每次被执行的情况收集信息并相应地做出一些优化，因此执行的次数越多，它的速度就越快。JDK 9引入了一种新的编译模式AOT(Ahead of Time Compilation)，它是直接将字节码编译成机器码，这样就避免了JIT预热等各方面的开销。JDK支持分层编译和AOT协作使用。但是 ，AOT 编译器的编译质量是肯定比不上 JIT 编译器的。 总结：Java虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。字节码和不同系统的 JVM 实现是 Java 语言“一次编译，随处可以运行”的关键所在。 JDK 和 JREJDK是Java Development Kit，它是功能齐全的Java SDK。它拥有JRE所拥有的一切，还有编译器（javac）和工具（如javadoc和jdb）。它能够创建和编译程序。 JRE 是 Java运行时环境。它是运行已编译 Java 程序所需的所有内容的集合，包括 Java虚拟机（JVM），Java类库，java命令和其他的一些基础构件。但是，它不能用于创建新程序。 如果你只是为了运行一下 Java 程序的话，那么你只需要安装 JRE 就可以了。如果你需要进行一些 Java 编程方面的工作，那么你就需要安装JDK了。但是，这不是绝对的。有时，即使您不打算在计算机上进行任何Java开发，仍然需要安装JDK。例如，如果要使用JSP部署Web应用程序，那么从技术上讲，您只是在应用程序服务器中运行Java程序。那你为什么需要JDK呢？因为应用程序服务器会将 JSP 转换为 Java servlet，并且需要使用 JDK 来编译 servlet。 4. Oracle JDK 和 OpenJDK 的对比可能在看这个问题之前很多人和我一样并没有接触和使用过 OpenJDK 。那么Oracle和OpenJDK之间是否存在重大差异？下面我通过收集到的一些资料，为你解答这个被很多人忽视的问题。 对于Java 7，没什么关键的地方。OpenJDK项目主要基于Sun捐赠的HotSpot源代码。此外，OpenJDK被选为Java 7的参考实现，由Oracle工程师维护。关于JVM，JDK，JRE和OpenJDK之间的区别，Oracle博客帖子在2012年有一个更详细的答案： 问：OpenJDK存储库中的源代码与用于构建Oracle JDK的代码之间有什么区别？ 答：非常接近 - 我们的Oracle JDK版本构建过程基于OpenJDK 7构建，只添加了几个部分，例如部署代码，其中包括Oracle的Java插件和Java WebStart的实现，以及一些封闭的源代码派对组件，如图形光栅化器，一些开源的第三方组件，如Rhino，以及一些零碎的东西，如附加文档或第三方字体。展望未来，我们的目的是开源Oracle JDK的所有部分，除了我们考虑商业功能的部分。 总结： Oracle JDK版本将每三年发布一次，而OpenJDK版本每三个月发布一次； OpenJDK 是一个参考模型并且是完全开源的，而Oracle JDK是OpenJDK的一个实现，并不是完全开源的； Oracle JDK 比 OpenJDK 更稳定。OpenJDK和Oracle JDK的代码几乎相同，但Oracle JDK有更多的类和一些错误修复。因此，如果您想开发企业/商业软件，我建议您选择Oracle JDK，因为它经过了彻底的测试和稳定。某些情况下，有些人提到在使用OpenJDK 可能会遇到了许多应用程序崩溃的问题，但是，只需切换到Oracle JDK就可以解决问题； 在响应性和JVM性能方面，Oracle JDK与OpenJDK相比提供了更好的性能； Oracle JDK不会为即将发布的版本提供长期支持，用户每次都必须通过更新到最新版本获得支持来获取最新版本； Oracle JDK根据二进制代码许可协议获得许可，而OpenJDK根据GPL v2许可获得许可。 5. Java和C++的区别?我知道很多人没学过 C++，但是面试官就是没事喜欢拿咱们 Java 和 C++ 比呀！没办法！！！就算没学过C++，也要记下来！ 都是面向对象的语言，都支持封装、继承和多态 Java 不提供指针来直接访问内存，程序内存更加安全 Java 的类是单继承的，C++ 支持多重继承；虽然 Java 的类不可以多继承，但是接口可以多继承。 Java 有自动内存管理机制，不需要程序员手动释放无用内存 6. 什么是 Java 程序的主类 应用程序和小程序的主类有何不同?一个程序中可以有多个类，但只能有一个类是主类。在 Java 应用程序中，这个主类是指包含 main（）方法的类。而在 Java 小程序中，这个主类是一个继承自系统类 JApplet 或 Applet 的子类。应用程序的主类不一定要求是 public 类，但小程序的主类要求必须是 public 类。主类是 Java 程序执行的入口点。 7. Java 应用程序与小程序之间有那些差别?简单说应用程序是从主线程启动(也就是 main() 方法)。applet 小程序没有main方法，主要是嵌在浏览器页面上运行(调用init()线程或者run()来启动)，嵌入浏览器这点跟 flash 的小游戏类似。 8. 字符型常量和字符串常量的区别? 形式上: 字符常量是单引号引起的一个字符; 字符串常量是双引号引起的若干个字符 含义上: 字符常量相当于一个整形值( ASCII 值),可以参加表达式运算; 字符串常量代表一个地址值(该字符串在内存中存放位置) 占内存大小 字符常量只占2个字节; 字符串常量占若干个字节(至少一个字符结束标志) (注意： char在Java中占两个字节) java编程思想第四版：2.2.2节 9. 构造器 Constructor 是否可被 override?在讲继承的时候我们就知道父类的私有属性和构造方法并不能被继承，所以 Constructor 也就不能被 override（重写）,但是可以 overload（重载）,所以你可以看到一个类中有多个构造函数的情况。 10. 重载和重写的区别重载： 发生在同一个类中，方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同，发生在编译时。 重写： 发生在父子类中，方法名、参数列表必须相同，返回值范围小于等于父类，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类；如果父类方法访问修饰符为 private 则子类就不能重写该方法。 11. Java 面向对象编程三大特性: 封装 继承 多态封装封装把一个对象的属性私有化，同时提供一些可以被外界访问的属性的方法，如果属性不想被外界访问，我们大可不必提供方法给外界访问。但是如果一个类没有提供给外界访问的方法，那么这个类也没有什么意义了。 继承继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承我们能够非常方便地复用以前的代码。 关于继承如下 3 点请记住： 子类拥有父类对象所有的属性和方法（包括私有属性和私有方法），但是父类中的私有属性和方法子类是无法访问，只是拥有。 子类可以拥有自己属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。（以后介绍）。 多态所谓多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定，而是在程序运行期间才确定，即一个引用变量到底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。 在Java中有两种形式可以实现多态：继承（多个子类对同一方法的重写）和接口（实现接口并覆盖接口中同一方法）。 12. String StringBuffer 和 StringBuilder 的区别是什么? String 为什么是不可变的?可变性 简单的来说：String 类中使用 final 关键字修饰字符数组来保存字符串，private final char value[]，所以 String 对象是不可变的。而StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串char[]value 但是没有用 final 关键字修饰，所以这两种对象都是可变的。 StringBuilder 与 StringBuffer 的构造方法都是调用父类构造方法也就是 AbstractStringBuilder 实现的，大家可以自行查阅源码。 AbstractStringBuilder.java 12345678abstract class AbstractStringBuilder implements Appendable, CharSequence &#123; char[] value; int count; AbstractStringBuilder() &#123; &#125; AbstractStringBuilder(int capacity) &#123; value = new char[capacity]; &#125; 线程安全性 String 中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。 性能 每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 对于三者使用的总结： 操作少量的数据: 适用String 单线程操作字符串缓冲区下操作大量数据: 适用StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用StringBuffer 13. 自动装箱与拆箱装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型； 14. 在一个静态方法内调用一个非静态成员为什么是非法的?由于静态方法可以不通过对象进行调用，因此在静态方法里，不能调用其他非静态变量，也不可以访问非静态变量成员。 15. 在 Java 中定义一个不做事且没有参数的构造方法的作用Java 程序在执行子类的构造方法之前，如果没有用 super() 来调用父类特定的构造方法，则会调用父类中“没有参数的构造方法”。因此，如果父类中只定义了有参数的构造方法，而在子类的构造方法中又没有用 super() 来调用父类中特定的构造方法，则编译时将发生错误，因为 Java 程序在父类中找不到没有参数的构造方法可供执行。解决办法是在父类里加上一个不做事且没有参数的构造方法。 16. import java和javax有什么区别？刚开始的时候 JavaAPI 所必需的包是 java 开头的包，javax 当时只是扩展 API 包来使用。然而随着时间的推移，javax 逐渐地扩展成为 Java API 的组成部分。但是，但是，将扩展从 javax 包移动到 java 包确实太麻烦了，最终会破坏一堆现有的代码。因此，最终决定 javax 包将成为标准API的一部分。 所以，实际上java和javax没有区别。这都是一个名字。 17. 接口和抽象类的区别是什么？ 接口的方法默认是 public，所有方法在接口中不能有实现(Java 8 开始接口方法可以有默认实现），而抽象类可以有非抽象的方法。 接口中除了static、final变量，不能有其他变量，而抽象类中则不一定。 一个类可以实现多个接口，但只能实现一个抽象类。接口自己本身可以通过extends关键字扩展多个接口。 接口方法默认修饰符是public，抽象方法可以有public、protected和default这些修饰符（抽象方法就是为了被重写所以不能使用private关键字修饰！）。 从设计层面来说，抽象是对类的抽象，是一种模板设计，而接口是对行为的抽象，是一种行为的规范。 备注：在JDK8中，接口也可以定义静态方法，可以直接用接口名调用。实现类和实现是不可以调用的。如果同时实现两个接口，接口中定义了一样的默认方法，则必须重写，不然会报错。(详见issue:https://github.com/Snailclimb/JavaGuide/issues/146) 18. 成员变量与局部变量的区别有那些？ 从语法形式上看:成员变量是属于类的，而局部变量是在方法中定义的变量或是方法的参数；成员变量可以被 public,private,static 等修饰符所修饰，而局部变量不能被访问控制修饰符及 static 所修饰；但是，成员变量和局部变量都能被 final 所修饰。 从变量在内存中的存储方式来看:如果成员变量是使用static修饰的，那么这个成员变量是属于类的，如果没有使用使用static修饰，这个成员变量是属于实例的。而对象存在于堆内存，局部变量则存在于栈内存。 从变量在内存中的生存时间上看:成员变量是对象的一部分，它随着对象的创建而存在，而局部变量随着方法的调用而自动消失。 成员变量如果没有被赋初值:则会自动以类型的默认值而赋值（一种情况例外被 final 修饰的成员变量也必须显示地赋值），而局部变量则不会自动赋值。 19. 创建一个对象用什么运算符?对象实体与对象引用有何不同?new运算符，new创建对象实例（对象实例在堆内存中），对象引用指向对象实例（对象引用存放在栈内存中）。一个对象引用可以指向0个或1个对象（一根绳子可以不系气球，也可以系一个气球）;一个对象可以有n个引用指向它（可以用n条绳子系住一个气球）。 20. 什么是方法的返回值?返回值在类的方法里的作用是什么?方法的返回值是指我们获取到的某个方法体中的代码执行后产生的结果！（前提是该方法可能产生结果）。返回值的作用:接收出结果，使得它可以用于其他的操作！ 21. 一个类的构造方法的作用是什么? 若一个类没有声明构造方法，该程序能正确执行吗? 为什么?主要作用是完成对类对象的初始化工作。可以执行。因为一个类即使没有声明构造方法也会有默认的不带参数的构造方法。 22. 构造方法有哪些特性？ 名字与类名相同。 没有返回值，但不能用void声明构造函数。 生成类的对象时自动执行，无需调用。 23. 静态方法和实例方法有何不同 在外部调用静态方法时，可以使用”类名.方法名”的方式，也可以使用”对象名.方法名”的方式。而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象。 静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），而不允许访问实例成员变量和实例方法；实例方法则无此限制。 24. 对象的相等与指向他们的引用相等,两者有什么不同?对象的相等，比的是内存中存放的内容是否相等。而引用相等，比较的是他们指向的内存地址是否相等。 25. 在调用子类构造方法之前会先调用父类没有参数的构造方法,其目的是?帮助子类做初始化工作。 26. == 与 equals(重要)== : 它的作用是判断两个对象的地址是不是相等。即，判断两个对象是不是同一个对象(基本数据类型==比较的是值，引用数据类型==比较的是内存地址)。 equals() : 它的作用也是判断两个对象是否相等。但它一般有两种使用情况： 情况1：类没有覆盖 equals() 方法。则通过 equals() 比较该类的两个对象时，等价于通过“==”比较这两个对象。 情况2：类覆盖了 equals() 方法。一般，我们都覆盖 equals() 方法来两个对象的内容相等；若它们的内容相等，则返回 true (即，认为这两个对象相等)。 举个例子： 1234567891011121314151617public class test1 &#123; public static void main(String[] args) &#123; String a = new String("ab"); // a 为一个引用 String b = new String("ab"); // b为另一个引用,对象的内容一样 String aa = "ab"; // 放在常量池中 String bb = "ab"; // 从常量池中查找 if (aa == bb) // true System.out.println("aa==bb"); if (a == b) // false，非同一对象 System.out.println("a==b"); if (a.equals(b)) // true System.out.println("aEQb"); if (42 == 42.0) &#123; // true System.out.println("true"); &#125; &#125;&#125; 说明： String 中的 equals 方法是被重写过的，因为 object 的 equals 方法是比较的对象的内存地址，而 String 的 equals 方法比较的是对象的值。 当创建 String 类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个 String 对象。 27. hashCode 与 equals (重要)面试官可能会问你：“你重写过 hashcode 和 equals 么，为什么重写equals时必须重写hashCode方法？” hashCode（）介绍hashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode() 定义在JDK的Object.java中，这就意味着Java中的任何类都包含有hashCode() 函数。 散列表存储的是键值对(key-value)，它的特点是：能根据“键”快速的检索出对应的“值”。这其中就利用到了散列码！（可以快速找到所需要的对象） 为什么要有 hashCode我们先以“HashSet 如何检查重复”为例子来说明为什么要有 hashCode： 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashcode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashcode 值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用 equals（）方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。（摘自我的Java启蒙书《Head first java》第二版）。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。 通过我们可以看出：hashCode() 的作用就是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode()在散列表中才有用，在其它情况下没用。在散列表中hashCode() 的作用是获取对象的散列码，进而确定该对象在散列表中的位置。 hashCode（）与equals（）的相关规定 如果两个对象相等，则hashcode一定也是相同的 两个对象相等,对两个对象分别调用equals方法都返回true 两个对象有相同的hashcode值，它们也不一定是相等的 因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖 hashCode() 的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） 推荐阅读：Java hashCode() 和 equals()的若干问题解答 28. 为什么Java中只有值传递？ 为什么Java中只有值传递？ 29. 简述线程、程序、进程的基本概念。以及他们之间关系是什么?线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 程序是含有指令和数据的文件，被存储在磁盘或其他的数据存储设备中，也就是说程序是静态的代码。 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。简单来说，一个进程就是一个执行中的程序，它在计算机中一个指令接着一个指令地执行着，同时，每个进程还占有某些系统资源如CPU时间，内存空间，文件，文件，输入输出设备的使用权等等。换句话说，当程序在执行时，将会被操作系统载入内存中。线程是进程划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。从另一角度来说，进程属于操作系统的范畴，主要是同一段时间内，可以同时执行一个以上的程序，而线程则是在同一程序内几乎同时执行一个以上的程序段。 30. 线程有哪些基本状态?Java 线程在运行的生命周期中的指定时刻只可能处于下面6种不同状态的其中一个状态（图源《Java 并发编程艺术》4.1.4节）。 线程在生命周期中并不是固定处于某一个状态而是随着代码的执行在不同状态之间切换。Java 线程状态变迁如下图所示（图源《Java 并发编程艺术》4.1.4节）： 由上图可以看出： 线程创建之后它将处于 NEW（新建） 状态，调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 cpu 时间片（timeslice）后就处于 RUNNING（运行） 状态。 操作系统隐藏 Java虚拟机（JVM）中的 RUNNABLE 和 RUNNING 状态，它只能看到 RUNNABLE 状态（图源：HowToDoInJava：Java Thread Life Cycle and Thread States），所以 Java 系统一般将这两个状态统称为 RUNNABLE（运行中） 状态 。 当线程执行 wait()方法之后，线程进入 WAITING（等待）状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 TIME_WAITING(超时等待) 状态相当于在等待状态的基础上增加了超时限制，比如通过 sleep（long millis）方法或 wait（long millis）方法可以将 Java 线程置于 TIMED WAITING 状态。当超时时间到达后 Java 线程将会返回到 RUNNABLE 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 BLOCKED（阻塞） 状态。线程在执行 Runnable 的run()方法之后将会进入到 TERMINATED（终止） 状态。 31 关于 final 关键字的一些总结final关键字主要用在三个地方：变量、方法、类。 对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。 当用final修饰一个类时，表明这个类不能被继承。final类中的所有成员方法都会被隐式地指定为final方法。 使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的Java实现版本中，会将final方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的Java版本已经不需要使用final方法进行这些优化了）。类中所有的private方法都隐式地指定为final。 32 Java 中的异常处理Java异常类层次结构图 在 Java 中，所有的异常都有一个共同的祖先java.lang包中的 Throwable类。Throwable： 有两个重要的子类：Exception（异常） 和 Error（错误） ，二者都是 Java 异常处理的重要子类，各自都包含大量子类。 Error（错误）:是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。 这些错误表示故障发生于虚拟机自身、或者发生在虚拟机试图执行应用时，如Java虚拟机运行错误（Virtual MachineError）、类定义错误（NoClassDefFoundError）等。这些错误是不可查的，因为它们在应用程序的控制和处理能力之 外，而且绝大多数是程序运行时不允许出现的状况。对于设计合理的应用程序来说，即使确实发生了错误，本质上也不应该试图去处理它所引起的异常状况。在 Java中，错误通过Error的子类描述。 Exception（异常）:是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 异常由Java虚拟机抛出。NullPointerException（要访问的变量没有引用任何对象时，抛出该异常）、ArithmeticException（算术运算异常，一个整数除以0时，抛出该异常）和 ArrayIndexOutOfBoundsException （下标越界异常）。 注意：异常和错误的区别：异常能被程序本身可以处理，错误是无法处理。 Throwable类常用方法 public string getMessage():返回异常发生时的详细信息 public string toString():返回异常发生时的简要描述 public string getLocalizedMessage():返回异常对象的本地化信息。使用Throwable的子类覆盖这个方法，可以声称本地化信息。如果子类没有覆盖该方法，则该方法返回的信息与getMessage（）返回的结果相同 public void printStackTrace():在控制台上打印Throwable对象封装的异常信息 异常处理总结 try 块：用于捕获异常。其后可接零个或多个catch块，如果没有catch块，则必须跟一个finally块。 catch 块：用于处理try捕获到的异常。 finally 块：无论是否捕获或处理异常，finally块里的语句都会被执行。当在try块或catch块中遇到return语句时，finally语句块将在方法返回之前被执行。 在以下4种特殊情况下，finally块不会被执行： 在finally语句块第一行发生了异常。 因为在其他行，finally块还是会得到执行 在前面的代码中用了System.exit(int)已退出程序。 exit是带参函数 ；若该语句在异常语句之后，finally会执行 程序所在的线程死亡。 关闭CPU。 下面这部分内容来自issue:https://github.com/Snailclimb/JavaGuide/issues/190。 关于返回值： 如果try语句里有return，返回的是try语句块中变量值。详细执行过程如下： 如果有返回值，就把返回值保存到局部变量中； 执行jsr指令跳到finally语句里执行； 执行完finally语句后，返回之前保存在局部变量表里的值。 如果try，finally语句里均有return，忽略try的return，而使用finally的return. 33 Java序列化中如果有些字段不想进行序列化，怎么办？对于不想进行序列化的变量，使用transient关键字修饰。 transient关键字的作用是：阻止实例中那些用此关键字修饰的的变量序列化；当对象被反序列化时，被transient修饰的变量值不会被持久化和恢复。transient只能修饰变量，不能修饰类和方法。 34 获取用键盘输入常用的的两种方法方法1：通过 Scanner 123Scanner input = new Scanner(System.in);String s = input.nextLine();input.close(); 方法2：通过 BufferedReader 12BufferedReader input = new BufferedReader(new InputStreamReader(System.in)); String s = input.readLine();]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Collections 工具类和 Arrays 工具类常见方法]]></title>
    <url>%2F2010%2F05%2F15%2Flang-java-basic-Arrays-CollectionsCommonMethods%2F</url>
    <content type="text"><![CDATA[Collections 工具类和 Arrays 工具类常见方法CollectionsCollections 工具类常用方法: 排序 查找,替换操作 同步控制(不推荐，需要线程安全的集合类型时请考虑使用 JUC 包下的并发集合) 排序操作123456void reverse(List list)//反转void shuffle(List list)//随机排序void sort(List list)//按自然排序的升序排序void sort(List list, Comparator c)//定制排序，由Comparator控制排序逻辑void swap(List list, int i , int j)//交换两个索引位置的元素void rotate(List list, int distance)//旋转。当distance为正数时，将list后distance个元素整体移到前面。当distance为负数时，将 list的前distance个元素整体移到后面。 示例代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;();arrayList.add(-1);arrayList.add(3);arrayList.add(3);arrayList.add(-5);arrayList.add(7);arrayList.add(4);arrayList.add(-9);arrayList.add(-7);System.out.println("原始数组:");System.out.println(arrayList);// void reverse(List list)：反转Collections.reverse(arrayList);System.out.println("Collections.reverse(arrayList):");System.out.println(arrayList);Collections.rotate(arrayList, 4);System.out.println("Collections.rotate(arrayList, 4):");System.out.println(arrayList);// void sort(List list),按自然排序的升序排序Collections.sort(arrayList);System.out.println("Collections.sort(arrayList):");System.out.println(arrayList);// void shuffle(List list),随机排序Collections.shuffle(arrayList);System.out.println("Collections.shuffle(arrayList):");System.out.println(arrayList);// void swap(List list, int i , int j),交换两个索引位置的元素Collections.swap(arrayList, 2, 5);System.out.println("Collections.swap(arrayList, 2, 5):");System.out.println(arrayList);// 定制排序的用法Collections.sort(arrayList, new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; return o2.compareTo(o1); &#125;&#125;);System.out.println("定制排序后：");System.out.println(arrayList); 查找,替换操作1234567int binarySearch(List list, Object key)//对List进行二分查找，返回索引，注意List必须是有序的int max(Collection coll)//根据元素的自然顺序，返回最大的元素。 类比int min(Collection coll)int max(Collection coll, Comparator c)//根据定制排序，返回最大元素，排序规则由Comparatator类控制。类比int min(Collection coll, Comparator c)void fill(List list, Object obj)//用指定的元素代替指定list中的所有元素。int frequency(Collection c, Object o)//统计元素出现次数int indexOfSubList(List list, List target)//统计target在list中第一次出现的索引，找不到则返回-1，类比int lastIndexOfSubList(List source, list target).boolean replaceAll(List list, Object oldVal, Object newVal), 用新元素替换旧元素 示例代码： 123456789101112131415161718192021222324252627282930313233343536ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;();arrayList.add(-1);arrayList.add(3);arrayList.add(3);arrayList.add(-5);arrayList.add(7);arrayList.add(4);arrayList.add(-9);arrayList.add(-7);ArrayList&lt;Integer&gt; arrayList2 = new ArrayList&lt;Integer&gt;();arrayList2.add(-3);arrayList2.add(-5);arrayList2.add(7);System.out.println("原始数组:");System.out.println(arrayList);System.out.println("Collections.max(arrayList):");System.out.println(Collections.max(arrayList));System.out.println("Collections.min(arrayList):");System.out.println(Collections.min(arrayList));System.out.println("Collections.replaceAll(arrayList, 3, -3):");Collections.replaceAll(arrayList, 3, -3);System.out.println(arrayList);System.out.println("Collections.frequency(arrayList, -3):");System.out.println(Collections.frequency(arrayList, -3));System.out.println("Collections.indexOfSubList(arrayList, arrayList2):");System.out.println(Collections.indexOfSubList(arrayList, arrayList2));System.out.println("Collections.binarySearch(arrayList, 7):");// 对List进行二分查找，返回索引，List必须是有序的Collections.sort(arrayList);System.out.println(Collections.binarySearch(arrayList, 7)); 同步控制Collections提供了多个synchronizedXxx()方法·，该方法可以将指定集合包装成线程同步的集合，从而解决多线程并发访问集合时的线程安全问题。 我们知道 HashSet，TreeSet，ArrayList,LinkedList,HashMap,TreeMap 都是线程不安全的。Collections提供了多个静态方法可以把他们包装成线程同步的集合。 最好不要用下面这些方法，效率非常低，需要线程安全的集合类型时请考虑使用 JUC 包下的并发集合。 方法如下： 1234synchronizedCollection(Collection&lt;T&gt; c) //返回指定 collection 支持的同步（线程安全的）collection。synchronizedList(List&lt;T&gt; list)//返回指定列表支持的同步（线程安全的）List。synchronizedMap(Map&lt;K,V&gt; m) //返回由指定映射支持的同步（线程安全的）Map。synchronizedSet(Set&lt;T&gt; s) //返回指定 set 支持的同步（线程安全的）set。 Collections还可以设置不可变集合，提供了如下三类方法：1234emptyXxx(): 返回一个空的、不可变的集合对象，此处的集合既可以是List，也可以是Set，还可以是Map。singletonXxx(): 返回一个只包含指定对象（只有一个或一个元素）的不可变的集合对象，此处的集合可以是：List，Set，Map。unmodifiableXxx(): 返回指定集合对象的不可变视图，此处的集合可以是：List，Set，Map。上面三类方法的参数是原有的集合对象，返回值是该集合的”只读“版本。 示例代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); arrayList.add(-1); arrayList.add(3); arrayList.add(3); arrayList.add(-5); arrayList.add(7); arrayList.add(4); arrayList.add(-9); arrayList.add(-7); HashSet&lt;Integer&gt; integers1 = new HashSet&lt;&gt;(); integers1.add(1); integers1.add(3); integers1.add(2); Map scores = new HashMap(); scores.put("语文" , 80); scores.put("Java" , 82); //Collections.emptyXXX();创建一个空的、不可改变的XXX对象 List&lt;Object&gt; list = Collections.emptyList(); System.out.println(list);//[] Set&lt;Object&gt; objects = Collections.emptySet(); System.out.println(objects);//[] Map&lt;Object, Object&gt; objectObjectMap = Collections.emptyMap(); System.out.println(objectObjectMap);//&#123;&#125; //Collections.singletonXXX(); List&lt;ArrayList&lt;Integer&gt;&gt; arrayLists = Collections.singletonList(arrayList); System.out.println(arrayLists);//[[-1, 3, 3, -5, 7, 4, -9, -7]] //创建一个只有一个元素，且不可改变的Set对象 Set&lt;ArrayList&lt;Integer&gt;&gt; singleton = Collections.singleton(arrayList); System.out.println(singleton);//[[-1, 3, 3, -5, 7, 4, -9, -7]] Map&lt;String, String&gt; nihao = Collections.singletonMap("1", "nihao"); System.out.println(nihao);//&#123;1=nihao&#125; //unmodifiableXXX();创建普通XXX对象对应的不可变版本 List&lt;Integer&gt; integers = Collections.unmodifiableList(arrayList); System.out.println(integers);//[-1, 3, 3, -5, 7, 4, -9, -7] Set&lt;Integer&gt; integers2 = Collections.unmodifiableSet(integers1); System.out.println(integers2);//[1, 2, 3] Map&lt;Object, Object&gt; objectObjectMap2 = Collections.unmodifiableMap(scores); System.out.println(objectObjectMap2);//&#123;Java=82, 语文=80&#125; //添加出现异常：java.lang.UnsupportedOperationException// list.add(1);// arrayLists.add(arrayList);// integers.add(1); Arrays类的常见操作 排序 : sort() 查找 : binarySearch() 比较: equals() 填充 : fill() 转列表: asList() 转字符串 : toString() 复制: copyOf() 排序 : sort()12345678910111213141516171819202122232425262728293031323334353637383940// *************排序 sort****************int a[] = &#123; 1, 3, 2, 7, 6, 5, 4, 9 &#125;;// sort(int[] a)方法按照数字顺序排列指定的数组。Arrays.sort(a);System.out.println("Arrays.sort(a):");for (int i : a) &#123; System.out.print(i);&#125;// 换行System.out.println();// sort(int[] a,int fromIndex,int toIndex)按升序排列数组的指定范围int b[] = &#123; 1, 3, 2, 7, 6, 5, 4, 9 &#125;;Arrays.sort(b, 2, 6);System.out.println("Arrays.sort(b, 2, 6):");for (int i : b) &#123; System.out.print(i);&#125;// 换行System.out.println();int c[] = &#123; 1, 3, 2, 7, 6, 5, 4, 9 &#125;;// parallelSort(int[] a) 按照数字顺序排列指定的数组(并行的)。同sort方法一样也有按范围的排序Arrays.parallelSort(c);System.out.println("Arrays.parallelSort(c)：");for (int i : c) &#123; System.out.print(i);&#125;// 换行System.out.println();// parallelSort给字符数组排序，sort也可以char d[] = &#123; 'a', 'f', 'b', 'c', 'e', 'A', 'C', 'B' &#125;;Arrays.parallelSort(d);System.out.println("Arrays.parallelSort(d)：");for (char d2 : d) &#123; System.out.print(d2);&#125;// 换行System.out.println(); 在做算法面试题的时候，我们还可能会经常遇到对字符串排序的情况,Arrays.sort() 对每个字符串的特定位置进行比较，然后按照升序排序。 123String[] strs = &#123; "abcdehg", "abcdefg", "abcdeag" &#125;;Arrays.sort(strs);System.out.println(Arrays.toString(strs));//[abcdeag, abcdefg, abcdehg] 查找 : binarySearch()12345678// *************查找 binarySearch()****************char[] e = &#123; 'a', 'f', 'b', 'c', 'e', 'A', 'C', 'B' &#125;;// 排序后再进行二分查找，否则找不到Arrays.sort(e);System.out.println("Arrays.sort(e)" + Arrays.toString(e));System.out.println("Arrays.binarySearch(e, 'c')：");int s = Arrays.binarySearch(e, 'c');System.out.println("字符c在数组的位置：" + s); 比较: equals()12345678// *************比较 equals****************char[] e = &#123; 'a', 'f', 'b', 'c', 'e', 'A', 'C', 'B' &#125;;char[] f = &#123; 'a', 'f', 'b', 'c', 'e', 'A', 'C', 'B' &#125;;/** 元素数量相同，并且相同位置的元素相同。 另外，如果两个数组引用都是null，则它们被认为是相等的 。*/// 输出trueSystem.out.println("Arrays.equals(e, f):" + Arrays.equals(e, f)); 填充 : fill()1234567891011121314151617181920// *************填充fill(批量初始化)****************int[] g = &#123; 1, 2, 3, 3, 3, 3, 6, 6, 6 &#125;;// 数组中所有元素重新分配值Arrays.fill(g, 3);System.out.println("Arrays.fill(g, 3)：");// 输出结果：333333333for (int i : g) &#123; System.out.print(i);&#125;// 换行System.out.println();int[] h = &#123; 1, 2, 3, 3, 3, 3, 6, 6, 6, &#125;;// 数组中指定范围元素重新分配值Arrays.fill(h, 0, 2, 9);System.out.println("Arrays.fill(h, 0, 2, 9);：");// 输出结果：993333666for (int i : h) &#123; System.out.print(i);&#125; 转列表 asList()123456789// *************转列表 asList()****************/* * 返回由指定数组支持的固定大小的列表。 * （将返回的列表更改为“写入数组”。）该方法作为基于数组和基于集合的API之间的桥梁，与Collection.toArray()相结合 。 * 返回的列表是可序列化的，并实现RandomAccess 。 * 此方法还提供了一种方便的方式来创建一个初始化为包含几个元素的固定大小的列表如下： */List&lt;String&gt; stooges = Arrays.asList("Larry", "Moe", "Curly");System.out.println(stooges); 转字符串 toString()123456// *************转字符串 toString()****************/** 返回指定数组的内容的字符串表示形式。*/char[] k = &#123; 'a', 'f', 'b', 'c', 'e', 'A', 'C', 'B' &#125;;System.out.println(Arrays.toString(k));// [a, f, b, c, e, A, C, B] 复制 copyOf()1234567891011121314151617181920// *************复制 copy****************// copyOf 方法实现数组复制,h为数组，6为复制的长度int[] h = &#123; 1, 2, 3, 3, 3, 3, 6, 6, 6, &#125;;int i[] = Arrays.copyOf(h, 6);System.out.println("Arrays.copyOf(h, 6);：");// 输出结果：123333for (int j : i) &#123; System.out.print(j);&#125;// 换行System.out.println();// copyOfRange将指定数组的指定范围复制到新数组中int j[] = Arrays.copyOfRange(h, 6, 11);System.out.println("Arrays.copyOfRange(h, 6, 11)：");// 输出结果66600(h数组只有9个元素这里是从索引6到索引11复制所以不足的就为0)for (int j2 : j) &#123; System.out.print(j2);&#125;// 换行System.out.println();]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[J2EE基础知识]]></title>
    <url>%2F2010%2F04%2F17%2Flang-java-basic-J2EE%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Servlet总结在Java Web程序中，Servlet主要负责接收用户请求HttpServletRequest,在doGet(),doPost()中做相应的处理，并将回应HttpServletResponse反馈给用户。Servlet可以设置初始化参数，供Servlet内部使用。一个Servlet类只会有一个实例，在它初始化时调用init()方法，销毁时调用destroy()方法。Servlet需要在web.xml中配置（MyEclipse中创建Servlet会自动配置），一个Servlet可以设置多个URL访问。Servlet不是线程安全，因此要谨慎使用类变量。 阐述Servlet和CGI的区别?CGI的不足之处:1，需要为每个请求启动一个操作CGI程序的系统进程。如果请求频繁，这将会带来很大的开销。 2，需要为每个请求加载和运行一个CGI程序，这将带来很大的开销 3，需要重复编写处理网络协议的代码以及编码，这些工作都是非常耗时的。 Servlet的优点:1，只需要启动一个操作系统进程以及加载一个JVM，大大降低了系统的开销 2，如果多个请求需要做同样处理的时候，这时候只需要加载一个类，这也大大降低了开销 3，所有动态加载的类可以实现对网络协议以及请求解码的共享，大大降低了工作量。 4，Servlet能直接和Web服务器交互，而普通的CGI程序不能。Servlet还能在各个程序之间共享数据，使数据库连接池之类的功能很容易实现。 补充：Sun Microsystems公司在1996年发布Servlet技术就是为了和CGI进行竞争，Servlet是一个特殊的Java程序，一个基于Java的Web应用通常包含一个或多个Servlet类。Servlet不能够自行创建并执行，它是在Servlet容器中运行的，容器将用户的请求传递给Servlet程序，并将Servlet的响应回传给用户。通常一个Servlet会关联一个或多个JSP页面。以前CGI经常因为性能开销上的问题被诟病，然而Fast CGI早就已经解决了CGI效率上的问题，所以面试的时候大可不必信口开河的诟病CGI，事实上有很多你熟悉的网站都使用了CGI技术。 参考：《javaweb整合开发王者归来》P7 Servlet接口中有哪些方法及Servlet生命周期探秘Servlet接口定义了5个方法，其中前三个方法与Servlet生命周期相关： void init(ServletConfig config) throws ServletException void service(ServletRequest req, ServletResponse resp) throws ServletException, java.io.IOException void destory() java.lang.String getServletInfo() ServletConfig getServletConfig() 生命周期： Web容器加载Servlet并将其实例化后，Servlet生命周期开始，容器运行其init()方法进行Servlet的初始化；请求到达时调用Servlet的service()方法，service()方法会根据需要调用与请求对应的doGet或doPost等方法；当服务器关闭或项目被卸载时服务器会将Servlet实例销毁，此时会调用Servlet的destroy()方法。init方法和destroy方法只会执行一次，service方法客户端每次请求Servlet都会执行。Servlet中有时会用到一些需要初始化与销毁的资源，因此可以把初始化资源的代码放入init方法中，销毁资源的代码放入destroy方法中，这样就不需要每次处理客户端的请求都要初始化与销毁资源。 参考：《javaweb整合开发王者归来》P81 get和post请求的区别 网上也有文章说：get和post请求实际上是没有区别，大家可以自行查询相关文章（参考文章：https://www.cnblogs.com/logsharing/p/8448446.html，知乎对应的问题链接：get和post区别？）！我下面给出的只是一种常见的答案。 ①get请求用来从服务器上获得资源，而post是用来向服务器提交数据； ②get将表单中数据按照name=value的形式，添加到action 所指向的URL 后面，并且两者使用”?”连接，而各个变量之间使用”&amp;”连接；post是将表单中的数据放在HTTP协议的请求头或消息体中，传递到action所指向URL； ③get传输的数据要受到URL长度限制（最大长度是 2048 个字符）；而post可以传输大量的数据，上传文件通常要使用post方式； ④使用get时参数会显示在地址栏上，如果这些数据不是敏感数据，那么可以使用get；对于敏感数据还是应用使用post； ⑤get使用MIME类型application/x-www-form-urlencoded的URL编码（也叫百分号编码）文本的格式传递参数，保证被传送的参数由遵循规范的文本组成，例如一个空格的编码是”%20”。 补充：GET方式提交表单的典型应用是搜索引擎。GET方式就是被设计为查询用的。 还有另外一种回答。推荐大家看一下： https://www.zhihu.com/question/28586791 https://mp.weixin.qq.com/s?__biz=MzI3NzIzMzg3Mw==&amp;mid=100000054&amp;idx=1&amp;sn=71f6c214f3833d9ca20b9f7dcd9d33e4#rd 什么情况下调用doGet()和doPost()Form标签里的method的属性为get时调用doGet()，为post时调用doPost()。 转发(Forward)和重定向(Redirect)的区别转发是服务器行为，重定向是客户端行为。 转发（Forword）通过RequestDispatcher对象的forward（HttpServletRequest request,HttpServletResponse response）方法实现的。RequestDispatcher可以通过HttpServletRequest 的getRequestDispatcher()方法获得。例如下面的代码就是跳转到login_success.jsp页面。 1request.getRequestDispatcher("login_success.jsp").forward(request, response); 重定向（Redirect） 是利用服务器返回的状态码来实现的。客户端浏览器请求服务器的时候，服务器会返回一个状态码。服务器通过 HttpServletResponse 的 setStatus(int status) 方法设置状态码。如果服务器返回301或者302，则浏览器会到新的网址重新请求该资源。 从地址栏显示来说 forward是服务器请求资源,服务器直接访问目标地址的URL,把那个URL的响应内容读取过来,然后把这些内容再发给浏览器.浏览器根本不知道服务器发送的内容从哪里来的,所以它的地址栏还是原来的地址.redirect是服务端根据逻辑,发送一个状态码,告诉浏览器重新去请求那个地址.所以地址栏显示的是新的URL. 从数据共享来说 forward:转发页面和转发到的页面可以共享request里面的数据.redirect:不能共享数据. 从运用地方来说 forward:一般用于用户登陆的时候,根据角色转发到相应的模块.redirect:一般用于用户注销登陆时返回主页面和跳转到其它的网站等 从效率来说 forward:高.redirect:低. 自动刷新(Refresh)自动刷新不仅可以实现一段时间之后自动跳转到另一个页面，还可以实现一段时间之后自动刷新本页面。Servlet中通过HttpServletResponse对象设置Header属性实现自动刷新例如： 1Response.setHeader("Refresh","5;URL=http://localhost:8080/servlet/example.htm"); 其中5为时间，单位为秒。URL指定就是要跳转的页面（如果设置自己的路径，就会实现每过5秒自动刷新本页面一次） Servlet与线程安全Servlet不是线程安全的，多线程并发的读写会导致数据不同步的问题。 解决的办法是尽量不要定义name属性，而是要把name变量分别定义在doGet()和doPost()方法内。虽然使用synchronized(name){}语句块可以解决问题，但是会造成线程的等待，不是很科学的办法。注意：多线程的并发的读写Servlet类属性会导致数据不同步。但是如果只是并发地读取属性而不写入，则不存在数据不同步的问题。因此Servlet里的只读属性最好定义为final类型的。 参考：《javaweb整合开发王者归来》P92 JSP和Servlet是什么关系其实这个问题在上面已经阐述过了，Servlet是一个特殊的Java程序，它运行于服务器的JVM中，能够依靠服务器的支持向浏览器提供显示内容。JSP本质上是Servlet的一种简易形式，JSP会被服务器处理成一个类似于Servlet的Java程序，可以简化页面内容的生成。Servlet和JSP最主要的不同点在于，Servlet的应用逻辑是在Java文件中，并且完全从表示层中的HTML分离开来。而JSP的情况是Java和HTML可以组合成一个扩展名为.jsp的文件。有人说，Servlet就是在Java中写HTML，而JSP就是在HTML中写Java代码，当然这个说法是很片面且不够准确的。JSP侧重于视图，Servlet更侧重于控制逻辑，在MVC架构模式中，JSP适合充当视图（view）而Servlet适合充当控制器（controller）。 JSP工作原理JSP是一种Servlet，但是与HttpServlet的工作方式不太一样。HttpServlet是先由源代码编译为class文件后部署到服务器下，为先编译后部署。而JSP则是先部署后编译。JSP会在客户端第一次请求JSP文件时被编译为HttpJspPage类（接口Servlet的一个子类）。该类会被服务器临时存放在服务器工作目录里面。下面通过实例给大家介绍。工程JspLoginDemo下有一个名为login.jsp的Jsp文件，把工程第一次部署到服务器上后访问这个Jsp文件，我们发现这个目录下多了下图这两个东东。.class文件便是JSP对应的Servlet。编译完毕后再运行class文件来响应客户端请求。以后客户端访问login.jsp的时候，Tomcat将不再重新编译JSP文件，而是直接调用class文件来响应客户端请求。由于JSP只会在客户端第一次请求的时候被编译 ，因此第一次请求JSP时会感觉比较慢，之后就会感觉快很多。如果把服务器保存的class文件删除，服务器也会重新编译JSP。 开发Web程序时经常需要修改JSP。Tomcat能够自动检测到JSP程序的改动。如果检测到JSP源代码发生了改动。Tomcat会在下次客户端请求JSP时重新编译JSP，而不需要重启Tomcat。这种自动检测功能是默认开启的，检测改动会消耗少量的时间，在部署Web应用的时候可以在web.xml中将它关掉。 参考：《javaweb整合开发王者归来》P97 JSP有哪些内置对象、作用分别是什么JSP内置对象 - CSDN博客 JSP有9个内置对象： request：封装客户端的请求，其中包含来自GET或POST请求的参数； response：封装服务器对客户端的响应； pageContext：通过该对象可以获取其他对象； session：封装用户会话的对象； application：封装服务器运行环境的对象； out：输出服务器响应的输出流对象； config：Web应用的配置对象； page：JSP页面本身（相当于Java程序中的this）； exception：封装页面抛出异常的对象。 Request对象的主要方法有哪些 setAttribute(String name,Object)：设置名字为name的request 的参数值 getAttribute(String name)：返回由name指定的属性值 getAttributeNames()：返回request 对象所有属性的名字集合，结果是一个枚举的实例 getCookies()：返回客户端的所有 Cookie 对象，结果是一个Cookie 数组 getCharacterEncoding() ：返回请求中的字符编码方式 = getContentLength() ：返回请求的 Body的长度 getHeader(String name) ：获得HTTP协议定义的文件头信息 getHeaders(String name) ：返回指定名字的request Header 的所有值，结果是一个枚举的实例 getHeaderNames() ：返回所以request Header 的名字，结果是一个枚举的实例 getInputStream() ：返回请求的输入流，用于获得请求中的数据 getMethod() ：获得客户端向服务器端传送数据的方法 getParameter(String name) ：获得客户端传送给服务器端的有 name指定的参数值 getParameterNames() ：获得客户端传送给服务器端的所有参数的名字，结果是一个枚举的实例 getParameterValues(String name)：获得有name指定的参数的所有值 getProtocol()：获取客户端向服务器端传送数据所依据的协议名称 getQueryString() ：获得查询字符串 getRequestURI() ：获取发出请求字符串的客户端地址 getRemoteAddr()：获取客户端的 IP 地址 getRemoteHost() ：获取客户端的名字 getSession([Boolean create]) ：返回和请求相关 Session getServerName() ：获取服务器的名字 getServletPath()：获取客户端所请求的脚本文件的路径 getServerPort()：获取服务器的端口号 removeAttribute(String name)：删除请求中的一个属性 request.getAttribute()和 request.getParameter()有何区别从获取方向来看： getParameter()是获取 POST/GET 传递的参数值； getAttribute()是获取对象容器中的数据值； 从用途来看： getParameter用于客户端重定向时，即点击了链接或提交按扭时传值用，即用于在用表单或url重定向传值时接收数据用。 getAttribute用于服务器端重定向时，即在 sevlet 中使用了 forward 函数,或 struts 中使用了mapping.findForward。 getAttribute 只能收到程序用 setAttribute 传过来的值。 另外，可以用 setAttribute,getAttribute 发送接收对象.而 getParameter 显然只能传字符串。setAttribute 是应用服务器把这个对象放在该页面所对应的一块内存中去，当你的页面服务器重定向到另一个页面时，应用服务器会把这块内存拷贝另一个页面所对应的内存中。这样getAttribute就能取得你所设下的值，当然这种方法可以传对象。session也一样，只是对象在内存中的生命周期不一样而已。getParameter只是应用服务器在分析你送上来的 request页面的文本时，取得你设在表单或 url 重定向时的值。 总结： getParameter 返回的是String,用于读取提交的表单中的值;（获取之后会根据实际需要转换为自己需要的相应类型，比如整型，日期类型啊等等） getAttribute 返回的是Object，需进行转换,可用setAttribute 设置成任意对象，使用很灵活，可随时用 include指令include的行为的区别include指令： JSP可以通过include指令来包含其他文件。被包含的文件可以是JSP文件、HTML文件或文本文件。包含的文件就好像是该JSP文件的一部分，会被同时编译执行。 语法格式如下：&lt;%@ include file=”文件相对 url 地址” %&gt; include动作： jsp:include动作元素用来包含静态和动态的文件。该动作把指定文件插入正在生成的页面。语法格式如下：&lt;jsp:include page=”相对 URL 地址” flush=”true” /&gt; JSP九大内置对象，七大动作，三大指令JSP九大内置对象，七大动作，三大指令总结 讲解JSP中的四种作用域JSP中的四种作用域包括page、request、session和application，具体来说： page代表与一个页面相关的对象和属性。 request代表与Web客户机发出的一个请求相关的对象和属性。一个请求可能跨越多个页面，涉及多个Web组件；需要在页面显示的临时数据可以置于此作用域。 session代表与某个用户与服务器建立的一次会话相关的对象和属性。跟某个用户相关的数据应该放在用户自己的session中。 application代表与整个Web应用程序相关的对象和属性，它实质上是跨越整个Web应用程序，包括多个页面、请求和会话的一个全局作用域。 如何实现JSP或Servlet的单线程模式对于JSP页面，可以通过page指令进行设置。&lt;%@page isThreadSafe=”false”%&gt; 对于Servlet，可以让自定义的Servlet实现SingleThreadModel标识接口。 说明：如果将JSP或Servlet设置成单线程工作模式，会导致每个请求创建一个Servlet实例，这种实践将导致严重的性能问题（服务器的内存压力很大，还会导致频繁的垃圾回收），所以通常情况下并不会这么做。 实现会话跟踪的技术有哪些 使用Cookie 向客户端发送Cookie 123Cookie c =new Cookie("name","value"); //创建Cookie c.setMaxAge(60*60*24); //设置最大时效，此处设置的最大时效为一天response.addCookie(c); //把Cookie放入到HTTP响应中 从客户端读取Cookie 123456789101112String name ="name"; Cookie[]cookies =request.getCookies(); if(cookies !=null)&#123; for(int i= 0;i&lt;cookies.length;i++)&#123; Cookie cookie =cookies[i]; if(name.equals(cookis.getName())) //something is here. //you can get the value cookie.getValue(); &#125; &#125; 优点: 数据可以持久保存，不需要服务器资源，简单，基于文本的Key-Value 缺点: 大小受到限制，用户可以禁用Cookie功能，由于保存在本地，有一定的安全风险。 URL 重写 在URL中添加用户会话的信息作为请求的参数，或者将唯一的会话ID添加到URL结尾以标识一个会话。 优点： 在Cookie被禁用的时候依然可以使用 缺点： 必须对网站的URL进行编码，所有页面必须动态生成，不能用预先记录下来的URL进行访问。 3.隐藏的表单域 1&lt;input type="hidden" name ="session" value="..."/&gt; 优点： Cookie被禁时可以使用 缺点： 所有页面必须是表单提交之后的结果。 HttpSession 在所有会话跟踪技术中，HttpSession对象是最强大也是功能最多的。当一个用户第一次访问某个网站时会自动创建 HttpSession，每个用户可以访问他自己的HttpSession。可以通过HttpServletRequest对象的getSession方 法获得HttpSession，通过HttpSession的setAttribute方法可以将一个值放在HttpSession中，通过调用 HttpSession对象的getAttribute方法，同时传入属性名就可以获取保存在HttpSession中的对象。与上面三种方式不同的 是，HttpSession放在服务器的内存中，因此不要将过大的对象放在里面，即使目前的Servlet容器可以在内存将满时将HttpSession 中的对象移到其他存储设备中，但是这样势必影响性能。添加到HttpSession中的值可以是任意Java对象，这个对象最好实现了 Serializable接口，这样Servlet容器在必要的时候可以将其序列化到文件中，否则在序列化时就会出现异常。 Cookie和Session的的区别 由于HTTP协议是无状态的协议，所以服务端需要记录用户的状态时，就需要用某种机制来识具体的用户，这个机制就是Session.典型的场景比如购物车，当你点击下单按钮时，由于HTTP协议无状态，所以并不知道是哪个用户操作的，所以服务端要为特定的用户创建了特定的Session，用用于标识这个用户，并且跟踪用户，这样才知道购物车里面有几本书。这个Session是保存在服务端的，有一个唯一标识。在服务端保存Session的方法很多，内存、数据库、文件都有。集群的时候也要考虑Session的转移，在大型的网站，一般会有专门的Session服务器集群，用来保存用户会话，这个时候 Session 信息都是放在内存的，使用一些缓存服务比如Memcached之类的来放 Session。 思考一下服务端如何识别特定的客户？这个时候Cookie就登场了。每次HTTP请求的时候，客户端都会发送相应的Cookie信息到服务端。实际上大多数的应用都是用 Cookie 来实现Session跟踪的，第一次创建Session的时候，服务端会在HTTP协议中告诉客户端，需要在 Cookie 里面记录一个Session ID，以后每次请求把这个会话ID发送到服务器，我就知道你是谁了。有人问，如果客户端的浏览器禁用了 Cookie 怎么办？一般这种情况下，会使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务端据此来识别用户。 Cookie其实还可以用在一些方便用户的场景下，设想你某次登陆过一个网站，下次登录的时候不想再次输入账号了，怎么办？这个信息可以写到Cookie里面，访问网站的时候，网站页面的脚本可以读取这个信息，就自动帮你把用户名给填了，能够方便一下用户。这也是Cookie名称的由来，给用户的一点甜头。所以，总结一下：Session是在服务端保存的一个数据结构，用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中；Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArrayList-Grow]]></title>
    <url>%2F2010%2F04%2F15%2Flang-java-basic-ArrayList-Grow%2F</url>
    <content type="text"><![CDATA[一 先从 ArrayList 的构造函数说起ArrayList有三种方式来初始化，构造方法源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;; /** *默认构造函数，使用初始容量10构造一个空列表(无参数构造) */ public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; &#125; /** * 带初始容量参数的构造函数。（用户自己指定容量） */ public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123;//初始容量大于0 //创建initialCapacity大小的数组 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123;//初始容量等于0 //创建空数组 this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123;//初始容量小于0，抛出异常 throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); &#125; &#125;/** *构造包含指定collection元素的列表，这些元素利用该集合的迭代器按顺序返回 *如果指定的集合为null，throws NullPointerException。 */ public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; &#125; &#125; 细心的同学一定会发现 ：以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为10。 下面在我们分析 ArrayList 扩容时会讲到这一点内容！ 二 一步一步分析 ArrayList 扩容机制这里以无参构造函数创建的 ArrayList 为例分析 1. 先来看 add 方法12345678910 /** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) &#123;//添加元素之前，先调用ensureCapacityInternal方法 ensureCapacityInternal(size + 1); // Increments modCount!! //这里看到ArrayList添加元素的实质就相当于为数组赋值 elementData[size++] = e; return true; &#125; 2. 再来看看 ensureCapacityInternal() 方法可以看到 add 方法 首先调用了ensureCapacityInternal(size + 1) 123456789//得到最小扩容量 private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 获取默认的容量和传入参数的较大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; 当 要 add 进第1个元素时，minCapacity为1，在Math.max()方法比较后，minCapacity 为10。 3. ensureExplicitCapacity() 方法如果调用 ensureCapacityInternal() 方法就一定会进过（执行）这个方法，下面我们来研究一下这个方法的源码！ 123456789//判断是否需要扩容 private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); &#125; 我们来仔细分析一下： 当我们要 add 进第1个元素到 ArrayList 时，elementData.length 为0 （因为还是一个空的 list），因为执行了 ensureCapacityInternal() 方法 ，所以 minCapacity 此时为10。此时，minCapacity - elementData.length &gt; 0成立，所以会进入 grow(minCapacity) 方法。 当add第2个元素时，minCapacity 为2，此时e lementData.length(容量)在添加第一个元素后扩容成 10 了。此时，minCapacity - elementData.length &gt; 0 不成立，所以不会进入 （执行）grow(minCapacity) 方法。 添加第3、4···到第10个元素时，依然不会执行grow方法，数组容量都为10。 直到添加第11个元素，minCapacity(为11)比elementData.length（为10）要大。进入grow方法进行扩容。 4. grow() 方法123456789101112131415161718192021222324/** * 要分配的最大数组大小 */private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;/** * ArrayList扩容的核心方法。 */private void grow(int minCapacity) &#123; // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; // 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) `hugeCapacity()` 方法来比较 minCapacity 和 MAX_ARRAY_SIZE， //如果minCapacity大于最大容量，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 `Integer.MAX_VALUE - 8`。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1),所以 ArrayList 每次扩容之后容量都会变为原来的 1.5 倍！（JDK1.6版本以后） JDk1.6版本时，扩容之后容量为 1.5 倍+1！详情请参考源码 “&gt;&gt;”（移位运算符）：&gt;&gt;1 右移一位相当于除2，右移n位相当于除以 2 的 n 次方。这里 oldCapacity 明显右移了1位所以相当于oldCapacity /2。对于大数据的2进制运算,位移运算符比那些普通运算符的运算要快很多,因为程序仅仅移动一下而已,不去计算,这样提高了效率,节省了资源 我们再来通过例子探究一下grow() 方法 ： 当add第1个元素时，oldCapacity 为0，经比较后第一个if判断成立，newCapacity = minCapacity(为10)。但是第二个if判断不会成立，即newCapacity 不比 MAX_ARRAY_SIZE大，则不会进入 hugeCapacity 方法。数组容量为10，add方法中 return true,size增为1。 当add第11个元素进入grow方法时，newCapacity为15，比minCapacity（为11）大，第一个if判断不成立。新容量没有大于数组最大size，不会进入hugeCapacity方法。数组容量扩为15，add方法中return true,size增为11。 以此类推······ 这里补充一点比较重要，但是容易被忽视掉的知识点： java 中的 length属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了 length 这个属性. java 中的 length() 方法是针对字符串说的,如果想看这个字符串的长度则用到 length() 这个方法. java 中的 size() 方法是针对泛型集合说的,如果想看这个泛型有多少个元素,就调用此方法来查看! 5. hugeCapacity() 方法。从上面 grow() 方法源码我们知道： 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) hugeCapacity() 方法来比较 minCapacity 和 MAX_ARRAY_SIZE，如果minCapacity大于最大容量，则新容量则为Integer.MAX_VALUE，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 Integer.MAX_VALUE - 8。 1234567891011private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); //对minCapacity和MAX_ARRAY_SIZE进行比较 //若minCapacity大，将Integer.MAX_VALUE作为新数组的大小 //若MAX_ARRAY_SIZE大，将MAX_ARRAY_SIZE作为新数组的大小 //MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;&#125; 三 System.arraycopy() 和 Arrays.copyOf()方法阅读源码的话，我们就会发现 ArrayList 中大量调用了这两个方法。比如：我们上面讲的扩容操作以及add(int index, E element)、toArray() 等方法中都用到了该方法！ 3.1 System.arraycopy() 方法123456789101112131415/** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()方法实现数组自己复制自己 //elementData:源数组;index:源数组中的起始位置;elementData：目标数组；index + 1：目标数组中的起始位置； size - index：要复制的数组元素的数量； System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 我们写一个简单的方法测试以下： 1234567891011121314151617public class ArraycopyTest &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub int[] a = new int[10]; a[0] = 0; a[1] = 1; a[2] = 2; a[3] = 3; System.arraycopy(a, 2, a, 3, 3); a[2]=99; for (int i = 0; i &lt; a.length; i++) &#123; System.out.println(a[i]); &#125; &#125;&#125; 结果： 10 1 99 2 3 0 0 0 0 0 3.2 Arrays.copyOf()方法1234567/** 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; 返回的数组的运行时类型是指定数组的运行时类型。 */ public Object[] toArray() &#123; //elementData：要复制的数组；size：要复制的长度 return Arrays.copyOf(elementData, size); &#125; 个人觉得使用 Arrays.copyOf()方法主要是为了给原有数组扩容，测试代码如下： 1234567891011public class ArrayscopyOfTest &#123; public static void main(String[] args) &#123; int[] a = new int[3]; a[0] = 0; a[1] = 1; a[2] = 2; int[] b = Arrays.copyOf(a, 10); System.out.println("b.length"+b.length); &#125;&#125; 结果： 110 3.3 两者联系和区别联系： 看两者源代码可以发现 copyOf() 内部实际调用了 System.arraycopy() 方法 区别： arraycopy() 需要目标数组，将原数组拷贝到你自己定义的数组里或者原数组，而且可以选择拷贝的起点和长度以及放入新数组中的位置 copyOf() 是系统自动在内部新建一个数组，并返回该数组。 四 ensureCapacity方法ArrayList 源码中有一个 ensureCapacity 方法不知道大家注意到没有，这个方法 ArrayList 内部没有被调用过，所以很显然是提供给用户调用的，那么这个方法有什么作用呢？ 1234567891011121314151617/**如有必要，增加此 ArrayList 实例的容量，以确保它至少可以容纳由minimum capacity参数指定的元素数。 * * @param minCapacity 所需的最小容量 */public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125;&#125; 最好在 add 大量元素之前用 ensureCapacity 方法，以减少增量重新分配的次数 我们通过下面的代码实际测试以下这个方法的效果： 123456789101112131415161718192021public class EnsureCapacityTest &#123; public static void main(String[] args) &#123; ArrayList&lt;Object&gt; list = new ArrayList&lt;Object&gt;(); final int N = 10000000; long startTime = System.currentTimeMillis(); for (int i = 0; i &lt; N; i++) &#123; list.add(i); &#125; long endTime = System.currentTimeMillis(); System.out.println("使用ensureCapacity方法前："+(endTime - startTime)); list = new ArrayList&lt;Object&gt;(); long startTime1 = System.currentTimeMillis(); list.ensureCapacity(N); for (int i = 0; i &lt; N; i++) &#123; list.add(i); &#125; long endTime1 = System.currentTimeMillis(); System.out.println("使用ensureCapacity方法后："+(endTime1 - startTime1)); &#125;&#125; 运行结果： 12使用ensureCapacity方法前：4637使用ensureCapacity方法后：241 通过运行结果，我们可以很明显的看出向 ArrayList 添加大量元素之前最好先使用ensureCapacity 方法，以减少增量重新分配的次数]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArrayList]]></title>
    <url>%2F2010%2F04%2F15%2Flang-java-basic-ArrayList%2F</url>
    <content type="text"><![CDATA[ArrayList简介 ArrayList 的底层是数组队列，相当于动态数组。与 Java 中的数组相比，它的容量能动态增长。在添加大量元素前，应用程序可以使用ensureCapacity操作来增加 ArrayList 实例的容量。这可以减少递增式再分配的数量。 它继承于 AbstractList，实现了 List, RandomAccess, Cloneable, java.io.Serializable 这些接口。 在我们学数据结构的时候就知道了线性表的顺序存储，插入删除元素的时间复杂度为O（n）,求表长以及增加元素，取第 i 元素的时间复杂度为O（1） ArrayList 继承了AbstractList，实现了List。它是一个数组队列，提供了相关的添加、删除、修改、遍历等功能。 ArrayList 实现了RandomAccess 接口， RandomAccess 是一个标志接口，表明实现这个这个接口的 List 集合是支持快速随机访问的。在 ArrayList 中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问。 ArrayList 实现了Cloneable 接口，即覆盖了函数 clone()，能被克隆。 ArrayList 实现java.io.Serializable 接口，这意味着ArrayList支持序列化，能通过序列化去传输。 和 Vector 不同，ArrayList 中的操作不是线程安全的！所以，建议在单线程中才使用 ArrayList，而在多线程中可以选择 Vector 或者 CopyOnWriteArrayList。 ArrayList核心源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498package java.util;import java.util.function.Consumer;import java.util.function.Predicate;import java.util.function.UnaryOperator;public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123; private static final long serialVersionUID = 8683452581122892189L; /** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; /** * 空数组（用于空实例）。 */ private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;; //用于默认大小空实例的共享空数组实例。 //我们把它从EMPTY_ELEMENTDATA数组中区分出来，以知道在添加第一个元素时容量需要增加多少。 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;; /** * 保存ArrayList数据的数组 */ transient Object[] elementData; // non-private to simplify nested class access /** * ArrayList 所包含的元素个数 */ private int size; /** * 带初始容量参数的构造函数。（用户自己指定容量） */ public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; //创建initialCapacity大小的数组 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; //创建空数组 this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); &#125; &#125; /** *默认构造函数，DEFAULTCAPACITY_EMPTY_ELEMENTDATA 为0.初始化为10，也就是说初始其实是空数组 当添加第一个元素的时候数组容量才变成10 */ public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; &#125; /** * 构造一个包含指定集合的元素的列表，按照它们由集合的迭代器返回的顺序。 */ public ArrayList(Collection&lt;? extends E&gt; c) &#123; // elementData = c.toArray(); //如果指定集合元素个数不为0 if ((size = elementData.length) != 0) &#123; // c.toArray 可能返回的不是Object类型的数组所以加上下面的语句用于判断， //这里用到了反射里面的getClass()方法 if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // 用空数组代替 this.elementData = EMPTY_ELEMENTDATA; &#125; &#125; /** * 修改这个ArrayList实例的容量是列表的当前大小。 应用程序可以使用此操作来最小化ArrayList实例的存储。 */ public void trimToSize() &#123; modCount++; if (size &lt; elementData.length) &#123; elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); &#125; &#125;//下面是ArrayList的扩容机制//ArrayList的扩容机制提高了性能，如果每次只扩充一个，//那么频繁的插入会导致频繁的拷贝，降低性能，而ArrayList的扩容机制避免了这种情况。 /** * 如有必要，增加此ArrayList实例的容量，以确保它至少能容纳元素的数量 * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125; &#125; //得到最小扩容量 private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 获取默认的容量和传入参数的较大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; //判断是否需要扩容 private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); &#125; /** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) &#123; // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //再检查新容量是否超出了ArrayList所定义的最大容量， //若超出了，则调用hugeCapacity()来比较minCapacity和 MAX_ARRAY_SIZE， //如果minCapacity大于MAX_ARRAY_SIZE，则新容量则为Interger.MAX_VALUE，否则，新容量大小则为 MAX_ARRAY_SIZE。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; //比较minCapacity和 MAX_ARRAY_SIZE private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; /** *返回此列表中的元素数。 */ public int size() &#123; return size; &#125; /** * 如果此列表不包含元素，则返回 true 。 */ public boolean isEmpty() &#123; //注意=和==的区别 return size == 0; &#125; /** * 如果此列表包含指定的元素，则返回true 。 */ public boolean contains(Object o) &#123; //indexOf()方法：返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1 return indexOf(o) &gt;= 0; &#125; /** *返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1 */ public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) //equals()方法比较 if (o.equals(elementData[i])) return i; &#125; return -1; &#125; /** * 返回此列表中指定元素的最后一次出现的索引，如果此列表不包含元素，则返回-1。. */ public int lastIndexOf(Object o) &#123; if (o == null) &#123; for (int i = size-1; i &gt;= 0; i--) if (elementData[i]==null) return i; &#125; else &#123; for (int i = size-1; i &gt;= 0; i--) if (o.equals(elementData[i])) return i; &#125; return -1; &#125; /** * 返回此ArrayList实例的浅拷贝。 （元素本身不被复制。） */ public Object clone() &#123; try &#123; ArrayList&lt;?&gt; v = (ArrayList&lt;?&gt;) super.clone(); //Arrays.copyOf功能是实现数组的复制，返回复制后的数组。参数是被复制的数组和复制的长度 v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; &#125; catch (CloneNotSupportedException e) &#123; // 这不应该发生，因为我们是可以克隆的 throw new InternalError(e); &#125; &#125; /** *以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。 *返回的数组将是“安全的”，因为该列表不保留对它的引用。 （换句话说，这个方法必须分配一个新的数组）。 *因此，调用者可以自由地修改返回的数组。 此方法充当基于阵列和基于集合的API之间的桥梁。 */ public Object[] toArray() &#123; return Arrays.copyOf(elementData, size); &#125; /** * 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; *返回的数组的运行时类型是指定数组的运行时类型。 如果列表适合指定的数组，则返回其中。 *否则，将为指定数组的运行时类型和此列表的大小分配一个新数组。 *如果列表适用于指定的数组，其余空间（即数组的列表数量多于此元素），则紧跟在集合结束后的数组中的元素设置为null 。 *（这仅在调用者知道列表不包含任何空元素的情况下才能确定列表的长度。） */ @SuppressWarnings("unchecked") public &lt;T&gt; T[] toArray(T[] a) &#123; if (a.length &lt; size) // 新建一个运行时类型的数组，但是ArrayList数组的内容 return (T[]) Arrays.copyOf(elementData, size, a.getClass()); //调用System提供的arraycopy()方法实现数组之间的复制 System.arraycopy(elementData, 0, a, 0, size); if (a.length &gt; size) a[size] = null; return a; &#125; // Positional Access Operations @SuppressWarnings("unchecked") E elementData(int index) &#123; return (E) elementData[index]; &#125; /** * 返回此列表中指定位置的元素。 */ public E get(int index) &#123; rangeCheck(index); return elementData(index); &#125; /** * 用指定的元素替换此列表中指定位置的元素。 */ public E set(int index, E element) &#123; //对index进行界限检查 rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; //返回原来在这个位置的元素 return oldValue; &#125; /** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! //这里看到ArrayList添加元素的实质就相当于为数组赋值 elementData[size++] = e; return true; &#125; /** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */ public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()这个实现数组之间复制的方法一定要看一下，下面就用到了arraycopy()方法实现数组自己复制自己 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; &#125; /** * 删除该列表中指定位置的元素。 将任何后续元素移动到左侧（从其索引中减去一个元素）。 */ public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work //从列表中删除的元素 return oldValue; &#125; /** * 从列表中删除指定元素的第一个出现（如果存在）。 如果列表不包含该元素，则它不会更改。 *返回true，如果此列表包含指定的元素 */ public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false; &#125; /* * Private remove method that skips bounds checking and does not * return the value removed. */ private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work &#125; /** * 从列表中删除所有元素。 */ public void clear() &#123; modCount++; // 把数组中所有的元素的值设为null for (int i = 0; i &lt; size; i++) elementData[i] = null; size = 0; &#125; /** * 按指定集合的Iterator返回的顺序将指定集合中的所有元素追加到此列表的末尾。 */ public boolean addAll(Collection&lt;? extends E&gt; c) &#123; Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0; &#125; /** * 将指定集合中的所有元素插入到此列表中，从指定的位置开始。 */ public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; rangeCheckForAdd(index); Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount int numMoved = size - index; if (numMoved &gt; 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0; &#125; /** * 从此列表中删除所有索引为fromIndex （含）和toIndex之间的元素。 *将任何后续元素移动到左侧（减少其索引）。 */ protected void removeRange(int fromIndex, int toIndex) &#123; modCount++; int numMoved = size - toIndex; System.arraycopy(elementData, toIndex, elementData, fromIndex, numMoved); // clear to let GC do its work int newSize = size - (toIndex-fromIndex); for (int i = newSize; i &lt; size; i++) &#123; elementData[i] = null; &#125; size = newSize; &#125; /** * 检查给定的索引是否在范围内。 */ private void rangeCheck(int index) &#123; if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); &#125; /** * add和addAll使用的rangeCheck的一个版本 */ private void rangeCheckForAdd(int index) &#123; if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); &#125; /** * 返回IndexOutOfBoundsException细节信息 */ private String outOfBoundsMsg(int index) &#123; return "Index: "+index+", Size: "+size; &#125; /** * 从此列表中删除指定集合中包含的所有元素。 */ public boolean removeAll(Collection&lt;?&gt; c) &#123; Objects.requireNonNull(c); //如果此列表被修改则返回true return batchRemove(c, false); &#125; /** * 仅保留此列表中包含在指定集合中的元素。 *换句话说，从此列表中删除其中不包含在指定集合中的所有元素。 */ public boolean retainAll(Collection&lt;?&gt; c) &#123; Objects.requireNonNull(c); return batchRemove(c, true); &#125; /** * 从列表中的指定位置开始，返回列表中的元素（按正确顺序）的列表迭代器。 *指定的索引表示初始调用将返回的第一个元素为next 。 初始调用previous将返回指定索引减1的元素。 *返回的列表迭代器是fail-fast 。 */ public ListIterator&lt;E&gt; listIterator(int index) &#123; if (index &lt; 0 || index &gt; size) throw new IndexOutOfBoundsException("Index: "+index); return new ListItr(index); &#125; /** *返回列表中的列表迭代器（按适当的顺序）。 *返回的列表迭代器是fail-fast 。 */ public ListIterator&lt;E&gt; listIterator() &#123; return new ListItr(0); &#125; /** *以正确的顺序返回该列表中的元素的迭代器。 *返回的迭代器是fail-fast 。 */ public Iterator&lt;E&gt; iterator() &#123; return new Itr(); &#125; ArrayList源码分析System.arraycopy()和Arrays.copyOf()方法 通过上面源码我们发现这两个实现数组复制的方法被广泛使用而且很多地方都特别巧妙。比如下面add(int index, E element)方法就很巧妙的用到了arraycopy()方法让数组自己复制自己实现让index开始之后的所有成员后移一个位置: 123456789101112131415/** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()方法实现数组自己复制自己 //elementData:源数组;index:源数组中的起始位置;elementData：目标数组；index + 1：目标数组中的起始位置； size - index：要复制的数组元素的数量； System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 又如toArray()方法中用到了copyOf()方法 12345678910/** *以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。 *返回的数组将是“安全的”，因为该列表不保留对它的引用。 （换句话说，这个方法必须分配一个新的数组）。 *因此，调用者可以自由地修改返回的数组。 此方法充当基于阵列和基于集合的API之间的桥梁。 */public Object[] toArray() &#123;//elementData：要复制的数组；size：要复制的长度 return Arrays.copyOf(elementData, size);&#125; 两者联系与区别联系：看两者源代码可以发现copyOf()内部调用了System.arraycopy()方法区别： arraycopy()需要目标数组，将原数组拷贝到你自己定义的数组里，而且可以选择拷贝的起点和长度以及放入新数组中的位置 copyOf()是系统自动在内部新建一个数组，并返回该数组。ArrayList 核心扩容技术1234567891011121314151617181920212223242526272829303132333435363738//下面是ArrayList的扩容机制//ArrayList的扩容机制提高了性能，如果每次只扩充一个，//那么频繁的插入会导致频繁的拷贝，降低性能，而ArrayList的扩容机制避免了这种情况。 /** * 如有必要，增加此ArrayList实例的容量，以确保它至少能容纳元素的数量 * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125; &#125; //得到最小扩容量 private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 获取默认的容量和传入参数的较大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; //判断是否需要扩容,上面两个方法都要调用 private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // 如果说minCapacity也就是所需的最小容量大于保存ArrayList数据的数组的长度的话，就需要调用grow(minCapacity)方法扩容。 //这个minCapacity到底为多少呢？举个例子在添加元素(add)方法中这个minCapacity的大小就为现在数组的长度加1 if (minCapacity - elementData.length &gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); &#125; 12345678910111213141516171819202122/** * ArrayList扩容的核心方法。 */private void grow(int minCapacity) &#123; //elementData为保存ArrayList数据的数组 ///elementData.length求数组长度elementData.size是求数组中的元素个数 // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //再检查新容量是否超出了ArrayList所定义的最大容量， //若超出了，则调用hugeCapacity()来比较minCapacity和 MAX_ARRAY_SIZE， //如果minCapacity大于MAX_ARRAY_SIZE，则新容量则为Interger.MAX_VALUE，否则，新容量大小则为 MAX_ARRAY_SIZE。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 扩容机制代码已经做了详细的解释。另外值得注意的是大家很容易忽略的一个运算符：移位运算符 简介：移位运算符就是在二进制的基础上对数字进行平移。按照平移的方向和填充数字的规则分为三种:&lt;&lt;(左移)、&gt;&gt;(带符号右移)和&gt;&gt;&gt;(无符号右移)。 作用：对于大数据的2进制运算,位移运算符比那些普通运算符的运算要快很多,因为程序仅仅移动一下而已,不去计算,这样提高了效率,节省了资源 比如这里：int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1);右移一位相当于除2，右移n位相当于除以 2 的 n 次方。这里 oldCapacity 明显右移了1位所以相当于oldCapacity /2。 另外需要注意的是： java 中的length 属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了 length 这个属性. java 中的length()方法是针对字 符串String说的,如果想看这个字符串的长度则用到 length()这个方法. .java 中的size()方法是针对泛型集合说的,如果想看这个泛型有多少个元素,就调用此方法来查看! 内部类1234(1)private class Itr implements Iterator&lt;E&gt; (2)private class ListItr extends Itr implements ListIterator&lt;E&gt; (3)private class SubList extends AbstractList&lt;E&gt; implements RandomAccess (4)static final class ArrayListSpliterator&lt;E&gt; implements Spliterator&lt;E&gt; ArrayList有四个内部类，其中的Itr是实现了Iterator接口，同时重写了里面的hasNext()，next()，remove()等方法；其中的ListItr继承Itr，实现了ListIterator接口，同时重写了hasPrevious()，nextIndex()，previousIndex()，previous()，set(E e)，add(E e)等方法，所以这也可以看出了 Iterator和ListIterator的区别:ListIterator在Iterator的基础上增加了添加对象，修改对象，逆向遍历等方法，这些是Iterator不能实现的。 ArrayList经典Demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package list;import java.util.ArrayList;import java.util.Iterator;public class ArrayListDemo &#123; public static void main(String[] srgs)&#123; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); System.out.printf("Before add:arrayList.size() = %d\n",arrayList.size()); arrayList.add(1); arrayList.add(3); arrayList.add(5); arrayList.add(7); arrayList.add(9); System.out.printf("After add:arrayList.size() = %d\n",arrayList.size()); System.out.println("Printing elements of arrayList"); // 三种遍历方式打印元素 // 第一种：通过迭代器遍历 System.out.print("通过迭代器遍历:"); Iterator&lt;Integer&gt; it = arrayList.iterator(); while(it.hasNext())&#123; System.out.print(it.next() + " "); &#125; System.out.println(); // 第二种：通过索引值遍历 System.out.print("通过索引值遍历:"); for(int i = 0; i &lt; arrayList.size(); i++)&#123; System.out.print(arrayList.get(i) + " "); &#125; System.out.println(); // 第三种：for循环遍历 System.out.print("for循环遍历:"); for(Integer number : arrayList)&#123; System.out.print(number + " "); &#125; // toArray用法 // 第一种方式(最常用) Integer[] integer = arrayList.toArray(new Integer[0]); // 第二种方式(容易理解) Integer[] integer1 = new Integer[arrayList.size()]; arrayList.toArray(integer1); // 抛出异常，java不支持向下转型 //Integer[] integer2 = new Integer[arrayList.size()]; //integer2 = arrayList.toArray(); System.out.println(); // 在指定位置添加元素 arrayList.add(2,2); // 删除指定位置上的元素 arrayList.remove(2); // 删除指定元素 arrayList.remove((Object)3); // 判断arrayList是否包含5 System.out.println("ArrayList contains 5 is: " + arrayList.contains(5)); // 清空ArrayList arrayList.clear(); // 判断ArrayList是否为空 System.out.println("ArrayList is empty: " + arrayList.isEmpty()); &#125;&#125;]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[final,static,this,super 关键字总结]]></title>
    <url>%2F2010%2F02%2F15%2Flang-java-basic-final%E3%80%81static%E3%80%81this%E3%80%81super%2F</url>
    <content type="text"><![CDATA[final,static,this,super 关键字总结final 关键字final关键字主要用在三个地方：变量、方法、类。 对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。 当用final修饰一个类时，表明这个类不能被继承。final类中的所有成员方法都会被隐式地指定为final方法。 使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的Java实现版本中，会将final方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的Java版本已经不需要使用final方法进行这些优化了）。类中所有的private方法都隐式地指定为final。 static 关键字static 关键字主要有以下四种使用场景： 修饰成员变量和成员方法: 被 static 修饰的成员属于类，不属于单个这个类的某个对象，被类中所有对象共享，可以并且建议通过类名调用。被static 声明的成员变量属于静态成员变量，静态变量 存放在 Java 内存区域的方法区。调用格式：类名.静态变量名 类名.静态方法名() 静态代码块: 静态代码块定义在类中方法外, 静态代码块在非静态代码块之前执行(静态代码块—&gt;非静态代码块—&gt;构造方法)。 该类不管创建多少对象，静态代码块只执行一次. 静态内部类（static修饰类的话只能修饰内部类）： 静态内部类与非静态内部类之间存在一个最大的区别: 非静态内部类在编译完成之后会隐含地保存着一个引用，该引用是指向创建它的外围类，但是静态内部类却没有。没有这个引用就意味着：1. 它的创建是不需要依赖外围类的创建。2. 它不能使用任何外围类的非static成员变量和方法。 静态导包(用来导入类中的静态资源，1.5之后的新特性): 格式为：import static 这两个关键字连用可以指定导入某个类中的指定静态资源，并且不需要使用类名调用类中静态成员，可以直接使用类中静态成员变量和成员方法。 this 关键字this关键字用于引用类的当前实例。 例如： 1234567891011class Manager &#123; Employees[] employees; void manageEmployees() &#123; int totalEmp = this.employees.length; System.out.println("Total employees: " + totalEmp); this.report(); &#125; void report() &#123; &#125;&#125; 在上面的示例中，this关键字用于两个地方： this.employees.length：访问类Manager的当前实例的变量。 this.report（）：调用类Manager的当前实例的方法。 此关键字是可选的，这意味着如果上面的示例在不使用此关键字的情况下表现相同。 但是，使用此关键字可能会使代码更易读或易懂。 super 关键字super关键字用于从子类访问父类的变量和方法。 例如： 1234567891011121314public class Super &#123; protected int number; protected showNumber() &#123; System.out.println("number = " + number); &#125;&#125; public class Sub extends Super &#123; void bar() &#123; super.number = 10; super.showNumber(); &#125;&#125; 在上面的例子中，Sub 类访问父类成员变量 number 并调用其其父类 Super 的 showNumber（） 方法。 使用 this 和 super 要注意的问题： 在构造器中使用 super（） 调用父类中的其他构造方法时，该语句必须处于构造器的首行，否则编译器会报错。另外，this 调用本类中的其他构造方法时，也要放在首行。 this、super不能用在static方法中。 简单解释一下： 被 static 修饰的成员属于类，不属于单个这个类的某个对象，被类中所有对象共享。而 this 代表对本类对象的引用，指向本类对象；而 super 代表对父类对象的引用，指向父类对象；所以， this和super是属于对象范畴的东西，而静态方法是属于类范畴的东西。 参考 https://www.codejava.net/java-core/the-java-language/java-keywords https://blog.csdn.net/u013393958/article/details/79881037 static 关键字详解static 关键字主要有以下四种使用场景 修饰成员变量和成员方法 静态代码块 修饰类(只能修饰内部类) 静态导包(用来导入类中的静态资源，1.5之后的新特性) 修饰成员变量和成员方法(常用)被 static 修饰的成员属于类，不属于单个这个类的某个对象，被类中所有对象共享，可以并且建议通过类名调用。被static 声明的成员变量属于静态成员变量，静态变量 存放在 Java 内存区域的方法区。 方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 HotSpot 虚拟机中方法区也常被称为 “永久代”，本质上两者并不等价。仅仅是因为 HotSpot 虚拟机设计团队用永久代来实现方法区而已，这样 HotSpot 虚拟机的垃圾收集器就可以像管理 Java 堆一样管理这部分内存了。但是这并不是一个好主意，因为这样更容易遇到内存溢出问题。 调用格式： 类名.静态变量名 类名.静态方法名() 如果变量或者方法被 private 则代表该属性或者该方法只能在类的内部被访问而不能在类的外部被访问。 测试方法： 1234567891011121314151617181920public class StaticBean &#123; String name; 静态变量 static int age; public StaticBean(String name) &#123; this.name = name; &#125; 静态方法 static void SayHello() &#123; System.out.println(Hello i am java); &#125; @Override public String toString() &#123; return StaticBean&#123; + name=' + name + ''' + age + age + '&#125;'; &#125;&#125; 1234567891011121314public class StaticDemo &#123; public static void main(String[] args) &#123; StaticBean staticBean = new StaticBean(1); StaticBean staticBean2 = new StaticBean(2); StaticBean staticBean3 = new StaticBean(3); StaticBean staticBean4 = new StaticBean(4); StaticBean.age = 33; StaticBean&#123;name='1'age33&#125; StaticBean&#123;name='2'age33&#125; StaticBean&#123;name='3'age33&#125; StaticBean&#123;name='4'age33&#125; System.out.println(staticBean+ +staticBean2+ +staticBean3+ +staticBean4); StaticBean.SayHello();Hello i am java &#125;&#125; 静态代码块静态代码块定义在类中方法外, 静态代码块在非静态代码块之前执行(静态代码块—非静态代码块—构造方法)。 该类不管创建多少对象，静态代码块只执行一次. 静态代码块的格式是 123static &#123; 语句体; &#125; 一个类中的静态代码块可以有多个，位置可以随便放，它不在任何的方法体内，JVM加载类时会执行这些静态的代码块，如果静态代码块有多个，JVM将按照它们在类中出现的先后顺序依次执行它们，每个代码块只会被执行一次。 静态代码块对于定义在它之后的静态变量，可以赋值，但是不能访问. 静态内部类静态内部类与非静态内部类之间存在一个最大的区别，我们知道非静态内部类在编译完成之后会隐含地保存着一个引用，该引用是指向创建它的外围类，但是静态内部类却没有。没有这个引用就意味着： 它的创建是不需要依赖外围类的创建。 它不能使用任何外围类的非static成员变量和方法。 Example（静态内部类实现单例模式） 123456789101112131415public class Singleton &#123; 声明为 private 避免调用默认构造方法创建对象 private Singleton() &#123; &#125; 声明为 private 表明静态内部该类只能在该 Singleton 类中被访问 private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; public static Singleton getUniqueInstance() &#123; return SingletonHolder.INSTANCE; &#125;&#125; 当 Singleton 类加载时，静态内部类 SingletonHolder 没有被加载进内存。只有当调用 getUniqueInstance()方法从而触发 SingletonHolder.INSTANCE 时 SingletonHolder 才会被加载，此时初始化 INSTANCE 实例，并且 JVM 能确保 INSTANCE 只被实例化一次。 这种方式不仅具有延迟初始化的好处，而且由 JVM 提供了对线程安全的支持。 静态导包格式为：import static 这两个关键字连用可以指定导入某个类中的指定静态资源，并且不需要使用类名调用类中静态成员，可以直接使用类中静态成员变量和成员方法 12345678910111213141516 Math. --- 将Math中的所有静态资源导入，这时候可以直接使用里面的静态方法，而不用通过类名进行调用 如果只想导入单一某个静态方法，只需要将换成对应的方法名即可 import static java.lang.Math.; 换成import static java.lang.Math.max;具有一样的效果 public class Demo &#123; public static void main(String[] args) &#123; int max = max(1,2); System.out.println(max); &#125;&#125; 补充内容静态方法与非静态方法静态方法属于类本身，非静态方法属于从该类生成的每个对象。 如果您的方法执行的操作不依赖于其类的各个变量和方法，请将其设置为静态（这将使程序的占用空间更小）。 否则，它应该是非静态的。 Example 12345678910111213141516class Foo &#123; int i; public Foo(int i) &#123; this.i = i; &#125; public static String method1() &#123; return An example string that doesn't depend on i (an instance variable); &#125; public int method2() &#123; return this.i + 1; Depends on i &#125;&#125; 你可以像这样调用静态方法：Foo.method1（）。 如果您尝试使用这种方法调用 method2 将失败。 但这样可行：Foo bar = new Foo（1）;bar.method2（）; 总结： 在外部调用静态方法时，可以使用”类名.方法名”的方式，也可以使用”对象名.方法名”的方式。而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象。 静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），而不允许访问实例成员变量和实例方法；实例方法则无此限制 static{}静态代码块与{}非静态代码块(构造代码块)相同点： 都是在JVM加载类时且在构造方法执行之前执行，在类中都可以定义多个，定义多个时按定义的顺序执行，一般在代码块中对一些static变量进行赋值。 不同点： 静态代码块在非静态代码块之前执行(静态代码块—非静态代码块—构造方法)。静态代码块只在第一次new执行一次，之后不再执行，而非静态代码块在每new一次就执行一次。 非静态代码块可在普通方法中定义(不过作用不大)；而静态代码块不行。 一般情况下,如果有些代码比如一些项目最常用的变量或对象必须在项目启动的时候就执行的时候,需要使用静态代码块,这种代码是主动执行的。如果我们想要设计不需要创建对象就可以调用类中的方法，例如：Arrays类，Character类，String类等，就需要使用静态方法, 两者的区别是 静态代码块是自动执行的而静态方法是被调用的时候才执行的. Example 1234567891011121314151617181920212223242526public class Test &#123; public Test() &#123; System.out.print(默认构造方法！--); &#125; 非静态代码块 &#123; System.out.print(非静态代码块！--); &#125; 静态代码块 static &#123; System.out.print(静态代码块！--); &#125; public static void test() &#123; System.out.print(静态方法中的内容! --); &#123; System.out.print(静态方法中的代码块！--); &#125; &#125; public static void main(String[] args) &#123; Test test = new Test(); Test.test();静态代码块！--静态方法中的内容! --静态方法中的代码块！-- &#125; 当执行 Test.test(); 时输出： 1静态代码块！--静态方法中的内容! --静态方法中的代码块！-- 当执行 Test test = new Test(); 时输出： 1静态代码块！--非静态代码块！--默认构造方法！-- 非静态代码块与构造函数的区别是： 非静态代码块是给所有对象进行统一初始化，而构造函数是给对应的对象初始化，因为构造函数是可以多个的，运行哪个构造函数就会建立什么样的对象，但无论建立哪个对象，都会先执行相同的构造代码块。也就是说，构造代码块中定义的是不同对象共性的初始化内容。 参考 httpsblog.csdn.netchen13579867831articledetails78995480 httpwww.cnblogs.comchenssyp3388487.html httpwww.cnblogs.comQian123p5713440.html]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap 简介]]></title>
    <url>%2F2010%2F02%2F15%2Flang-java-basic-HashMap%2F</url>
    <content type="text"><![CDATA[HashMap 简介HashMap 主要用来存放键值对，它基于哈希表的Map接口实现，是常用的Java集合之一。 JDK1.8 之前 HashMap 由 数组+链表 组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）.JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树，以减少搜索时间。 底层数据结构分析JDK1.8之前JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) &amp; hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。 所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。 JDK 1.8 HashMap 的 hash 方法源码: JDK 1.8 的 hash方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。 1234567 static final int hash(Object key) &#123; int h; // key.hashCode()：返回散列值也就是hashcode // ^ ：按位异或 // &gt;&gt;&gt;:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 对比一下 JDK1.7的 HashMap 的 hash 方法源码. 12345678static int hash(int h) &#123; // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。 所谓 “拉链法” 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。 JDK1.8之后相比于之前的版本，jdk1.8在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。 类的属性： 12345678910111213141516171819202122232425262728public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; // 序列号 private static final long serialVersionUID = 362498820763181265L; // 默认的初始容量是16 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // 最大容量 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 默认的填充因子 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 当桶(bucket)上的结点数大于这个值时会转成红黑树 static final int TREEIFY_THRESHOLD = 8; // 当桶(bucket)上的结点数小于这个值时树转链表 static final int UNTREEIFY_THRESHOLD = 6; // 桶中结构转化为红黑树对应的table的最小大小 static final int MIN_TREEIFY_CAPACITY = 64; // 存储元素的数组，总是2的幂次倍 transient Node&lt;k,v&gt;[] table; // 存放具体元素的集 transient Set&lt;map.entry&lt;k,v&gt;&gt; entrySet; // 存放元素的个数，注意这个不等于数组的长度。 transient int size; // 每次扩容和更改map结构的计数器 transient int modCount; // 临界值 当实际大小(容量*填充因子)超过临界值时，会进行扩容 int threshold; // 加载因子 final float loadFactor;&#125; loadFactor加载因子 loadFactor加载因子是控制数组存放数据的疏密程度，loadFactor越趋近于1，那么 数组中存放的数据(entry)也就越多，也就越密，也就是会让链表的长度增加，loadFactor越小，也就是趋近于0，数组中存放的数据(entry)也就越少，也就越稀疏。 loadFactor太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor的默认值为0.75f是官方给出的一个比较好的临界值。 给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 16 * 0.75 = 12 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。 threshold threshold = capacity * loadFactor，当Size&gt;=threshold的时候，那么就要考虑对数组的扩增了，也就是说，这个的意思就是 衡量数组是否需要扩增的一个标准。 Node节点类源码: 123456789101112131415161718192021222324252627282930313233343536373839// 继承自 Map.Entry&lt;K,V&gt;static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash;// 哈希值，存放元素到hashmap中时用来与其他元素hash值比较 final K key;//键 V value;//值 // 指向下一个节点 Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; // 重写hashCode()方法 public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; // 重写 equals() 方法 public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; 树节点类源码: 12345678910111213141516static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // 父 TreeNode&lt;K,V&gt; left; // 左 TreeNode&lt;K,V&gt; right; // 右 TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; // 判断颜色 TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125; // 返回根节点 final TreeNode&lt;K,V&gt; root() &#123; for (TreeNode&lt;K,V&gt; r = this, p;;) &#123; if ((p = r.parent) == null) return r; r = p; &#125; HashMap源码分析构造方法 123456789101112131415161718192021222324252627// 默认构造函数。public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted &#125; // 包含另一个“Map”的构造函数 public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);//下面会分析到这个方法 &#125; // 指定“容量大小”的构造函数 public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; // 指定“容量大小”和“加载因子”的构造函数 public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); &#125; putMapEntries方法： 123456789101112131415161718192021222324final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; int s = m.size(); if (s &gt; 0) &#123; // 判断table是否已经初始化 if (table == null) &#123; // pre-size // 未初始化，s为m的实际元素个数 float ft = ((float)s / loadFactor) + 1.0F; int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); // 计算得到的t大于阈值，则初始化阈值 if (t &gt; threshold) threshold = tableSizeFor(t); &#125; // 已初始化，并且m元素个数大于阈值，进行扩容处理 else if (s &gt; threshold) resize(); // 将m中的所有元素添加至HashMap中 for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123; K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); &#125; &#125;&#125; put方法HashMap只提供了put用于添加元素，putVal方法只是给put方法调用的一个方法，并没有提供给用户使用。 对putVal方法添加元素的分析如下： ①如果定位到的数组位置没有元素 就直接插入。 ②如果定位到的数组位置有元素就和要插入的key比较，如果key相同就直接覆盖，如果key不相同，就判断p是否是一个树节点，如果是就调用e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value)将元素添加进入。如果不是就遍历链表插入(插入的是链表尾部)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // table未初始化或者长度为0，进行扩容 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // (n - 1) &amp; hash 确定元素存放在哪个桶中，桶为空，新生成结点放入桶中(此时，这个结点是放在数组中) if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); // 桶中已经存在元素 else &#123; Node&lt;K,V&gt; e; K k; // 比较桶中第一个元素(数组中的结点)的hash值相等，key相等 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) // 将第一个元素赋值给e，用e来记录 e = p; // hash值不相等，即key不相等；为红黑树结点 else if (p instanceof TreeNode) // 放入树中 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 为链表结点 else &#123; // 在链表最末插入结点 for (int binCount = 0; ; ++binCount) &#123; // 到达链表的尾部 if ((e = p.next) == null) &#123; // 在尾部插入新结点 p.next = newNode(hash, key, value, null); // 结点数量达到阈值，转化为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); // 跳出循环 break; &#125; // 判断链表中结点的key值与插入的元素的key值是否相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) // 相等，跳出循环 break; // 用于遍历桶中的链表，与前面的e = p.next组合，可以遍历链表 p = e; &#125; &#125; // 表示在桶中找到key值、hash值与插入元素相等的结点 if (e != null) &#123; // 记录e的value V oldValue = e.value; // onlyIfAbsent为false或者旧值为null if (!onlyIfAbsent || oldValue == null) //用新值替换旧值 e.value = value; // 访问后回调 afterNodeAccess(e); // 返回旧值 return oldValue; &#125; &#125; // 结构性修改 ++modCount; // 实际大小大于阈值则扩容 if (++size &gt; threshold) resize(); // 插入后回调 afterNodeInsertion(evict); return null;&#125; 我们再来对比一下 JDK1.7 put方法的代码 对于put方法的分析如下： ①如果定位到的数组位置没有元素 就直接插入。 ②如果定位到的数组位置有元素，遍历以这个元素为头结点的链表，依次和插入的key比较，如果key相同就直接覆盖，不同就采用头插法插入元素。 12345678910111213141516171819202122public V put(K key, V value) if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; // 先遍历 Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); // 再插入 return null;&#125; get方法12345678910111213141516171819202122232425262728public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 数组元素相等 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 桶中不止一个节点 if ((e = first.next) != null) &#123; // 在树中get if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 在链表中get do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; resize方法进行扩容，会伴随着一次重新hash分配，并且会遍历hash表中所有的元素，是非常耗时的。在编写程序中，要尽量避免resize。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; HashMap常用方法测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package map;import java.util.Collection;import java.util.HashMap;import java.util.Set;public class HashMapDemo &#123; public static void main(String[] args) &#123; HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); // 键不能重复，值可以重复 map.put("san", "张三"); map.put("si", "李四"); map.put("wu", "王五"); map.put("wang", "老王"); map.put("wang", "老王2");// 老王被覆盖 map.put("lao", "老王"); System.out.println("-------直接输出hashmap:-------"); System.out.println(map); /** * 遍历HashMap */ // 1.获取Map中的所有键 System.out.println("-------foreach获取Map中所有的键:------"); Set&lt;String&gt; keys = map.keySet(); for (String key : keys) &#123; System.out.print(key+" "); &#125; System.out.println();//换行 // 2.获取Map中所有值 System.out.println("-------foreach获取Map中所有的值:------"); Collection&lt;String&gt; values = map.values(); for (String value : values) &#123; System.out.print(value+" "); &#125; System.out.println();//换行 // 3.得到key的值的同时得到key所对应的值 System.out.println("-------得到key的值的同时得到key所对应的值:-------"); Set&lt;String&gt; keys2 = map.keySet(); for (String key : keys2) &#123; System.out.print(key + "：" + map.get(key)+" "); &#125; /** * 另外一种不常用的遍历方式 */ // 当我调用put(key,value)方法的时候，首先会把key和value封装到 // Entry这个静态内部类对象中，把Entry对象再添加到数组中，所以我们想获取 // map中的所有键值对，我们只要获取数组中的所有Entry对象，接下来 // 调用Entry对象中的getKey()和getValue()方法就能获取键值对了 Set&lt;java.util.Map.Entry&lt;String, String&gt;&gt; entrys = map.entrySet(); for (java.util.Map.Entry&lt;String, String&gt; entry : entrys) &#123; System.out.println(entry.getKey() + "--" + entry.getValue()); &#125; /** * HashMap其他常用方法 */ System.out.println("after map.size()："+map.size()); System.out.println("after map.isEmpty()："+map.isEmpty()); System.out.println(map.remove("san")); System.out.println("after map.remove()："+map); System.out.println("after map.get(si)："+map.get("si")); System.out.println("after map.containsKey(si)："+map.containsKey("si")); System.out.println("after containsValue(李四)："+map.containsValue("李四")); System.out.println(map.replace("si", "李四2")); System.out.println("after map.replace(si, 李四2):"+map); &#125;&#125;]]></content>
      <categories>
        <category>lang</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
</search>
